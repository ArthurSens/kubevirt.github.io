<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://kubevirt.io//feed/news.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2020-02-24T09:16:51+00:00</updated><id>https://kubevirt.io//feed/news.xml</id><title type="html">KubeVirt.io | News</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">KubeVirt: installing Microsoft Windows from an ISO</title><link href="https://kubevirt.io//2020/KubeVirt-installing_Microsoft_Windows_from_an_iso.html" rel="alternate" type="text/html" title="KubeVirt: installing Microsoft Windows from an ISO" /><published>2020-02-14T00:00:00+00:00</published><updated>2020-02-14T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt-installing_Microsoft_Windows_from_an_iso</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt-installing_Microsoft_Windows_from_an_iso.html">&lt;p&gt;Hello! nowadays each operating system vendor has its cloud image available to download ready to import and deploy a new Virtual Machine (VM) inside Kubernetes with KubeVirt, 
but what if you want to follow the traditional way of installing a VM using an existing iso attached as a CDROM?&lt;/p&gt;

&lt;p&gt;In this blogpost, we are going to explain how to prepare that VM with the ISO file and the needed drivers to proceed with the installation of Microsoft Windows.&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A Kubernetes cluster is already up and running&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;KubeVirt&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer/blob/master/README.md&quot;&gt;CDI&lt;/a&gt; are already installed&lt;/li&gt;
  &lt;li&gt;There is enough free CPU, Memory and disk space in the cluster to deploy a Microsoft Windows VM, in this example, the version 2012 R2 VM is going to be used&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preparation&quot;&gt;Preparation&lt;/h2&gt;

&lt;p&gt;To proceed with the Installation steps the different elements involved are listed:&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;NOTE&lt;/p&gt;&lt;p&gt;No need for executing any command until the Installation section.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;An empty KubeVirt Virtual Machine
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachine&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;win2k12-iso&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;running&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;win2k12-iso&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;cores&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
         &lt;span class=&quot;s&quot;&gt;machine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;q35&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
             &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;8G&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A PVC with the Microsoft Windows ISO file attached as CDROM to the VM, would be automatically created with the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; command when uploading the file&lt;/p&gt;

    &lt;p&gt;First thing here is to download the ISO file of the Microsoft Windows, for that the &lt;a href=&quot;https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2012-r2&quot;&gt;Microsoft Evaluation Center&lt;/a&gt; offers
 the ISO files to download for evaluation purposes:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/2020-02-14-KubeVirt-installing_Microsoft_Windows_from_an_iso/win2k12_download_iso.png&quot; alt=&quot;win2k12_download_iso.png&quot; title=&quot;KubeVirt Microsoft Windows iso download&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;To be able to start the evaluation some personal data has to be filled in. Afterwards, the architecture to be checked is “64 bit” and the language selected as shown in 
 the following picture:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/2020-02-14-KubeVirt-installing_Microsoft_Windows_from_an_iso/win2k12_download_iso_64.png&quot; alt=&quot;win2k12_download_iso_64.png&quot; title=&quot;KubeVirt Microsoft Windows iso download&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Once the ISO file is downloaded it has to be uploaded with &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt;, the parameters used in this example are the following:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;image-upload&lt;/code&gt;: Upload a VM image to a PersistentVolumeClaim&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--image-path&lt;/code&gt;: The path of the ISO file&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--pvc-name&lt;/code&gt;: The name of the PVC to store the ISO file, in this example is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;iso-win2k12&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--access-mode&lt;/code&gt;: the access mode for the PVC, in the example &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ReadOnlyMany&lt;/code&gt; has been used.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--pvc-size&lt;/code&gt;: The size of the PVC, is where the ISO will be stored, in this case, the ISO is 4.3G so a PVC os 5G should be enough&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--uploadproxy-url&lt;/code&gt;: The URL of the cdi-upload proxy service, in the following example the CLUSTER-IP is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;10.96.164.35&lt;/code&gt; and the PORT is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;443&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;premonition info&quot;&gt;
 &lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;
 &lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;
 &lt;p&gt;
 To upload data to the cluster, the cdi-uploadproxy service must be accessible from outside the cluster. In a production environment, this probably involves setting up an Ingress or a LoadBalancer Service. 
 &lt;/p&gt;
 &lt;/div&gt;
 &lt;/div&gt;

    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get services &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; cdi
 NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   AGE
 cdi-api           ClusterIP   10.96.117.29   &amp;lt;none&amp;gt;        443/TCP   6d18h
 cdi-uploadproxy   ClusterIP   10.96.164.35   &amp;lt;none&amp;gt;        443/TCP   6d18h
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;In this example the ISO file was copied to the Kubernetes node, to allow the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; to find it and to simplify the operation.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--insecure&lt;/code&gt;: Allow insecure server connections when using HTTPS&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--wait-secs&lt;/code&gt;: The time in seconds to wait for upload pod to start. (default 60)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;The final command with the parameters and the values would look like:&lt;/p&gt;
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtctl image-upload &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--image-path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/root/9600.17050.WINBLUE_REFRESH.140317-1640_X64FRE_SERVER_EVAL_EN-US-IR3_SSS_X64FREE_EN-US_DV9.ISO &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--pvc-name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iso-win2k12 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--access-mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ReadOnlyMany &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--pvc-size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5G &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--uploadproxy-url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://10.96.164.35:443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--insecure&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--wait-secs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;240
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;A PVC for the hard drive where the Operating System is going to be installed, in this example it is called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;winhd&lt;/code&gt; and the space requested is 15Gi:
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PersistentVolumeClaim&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;winhd&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;15Gi&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;storageClassName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;hostpath&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/creating-virtual-machines/virtio-win.html#how-to-obtain-virtio-drivers&quot;&gt;container with the virtio drivers&lt;/a&gt; attached as a CDROM to the VM. 
 The container image has to be pulled to have it available in the local registry.
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker pull kubevirt/virtio-container-disk
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;And also it has to be referenced in the VM YAML, in this example the name for the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;containerDisk&lt;/code&gt; is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtiocontainerdisk&lt;/code&gt;.&lt;/p&gt;
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;sata&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtiocontainerdisk&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerDisk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt/virtio-container-disk&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtiocontainerdisk&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;If the pre-requisites are fullfilled, the final YAML (&lt;a href=&quot;/assets/2020-02-14-KubeVirt-installing_Microsoft_Windows_from_an_iso/win2k12.yml&quot;&gt;win2k12.yml&lt;/a&gt;), will look like:&lt;/p&gt;
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PersistentVolumeClaim&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;winhd&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;15Gi&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;storageClassName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;hostpath&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachine&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;win2k12-iso&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;running&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;win2k12-iso&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;cores&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;disks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;bootOrder&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
             &lt;span class=&quot;na&quot;&gt;cdrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;sata&lt;/span&gt;
             &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cdromiso&lt;/span&gt;
           &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
             &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;harddrive&lt;/span&gt;
           &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;cdrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;sata&lt;/span&gt;
             &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtiocontainerdisk&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;machine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;q35&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
             &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;8G&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cdromiso&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;persistentVolumeClaim&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;claimName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;iso-win2k12&lt;/span&gt;
       &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;harddrive&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;persistentVolumeClaim&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;claimName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;winhd&lt;/span&gt;
       &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerDisk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt/virtio-container-disk&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtiocontainerdisk&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;Special attention to the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;bootOrder: 1&lt;/code&gt; parameter in the first disk as it is the volume containing the ISO and it has to be marked as the first device to boot from.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;
&lt;p&gt;To proceed with the installation the commands commented above are going to be executed:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Uploading the ISO file to the PVC:&lt;/p&gt;

    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtctl image-upload &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--image-path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/root/9600.17050.WINBLUE_REFRESH.140317-1640_X64FRE_SERVER_EVAL_EN-US-IR3_SSS_X64FREE_EN-US_DV9.ISO &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--pvc-name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iso-win2k12 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--access-mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ReadOnlyMany &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--pvc-size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5G &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--uploadproxy-url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://10.96.164.35:443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--insecure&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--wait-secs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;240

 DataVolume default/iso-win2k12 created
 Waiting &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;PVC iso-win2k12 upload pod to be ready...
 Pod now ready
 Uploading data to https://10.96.164.35:443

 4.23 GiB / 4.23 GiB &lt;span class=&quot;o&quot;&gt;[=======================================================================================================================================================================]&lt;/span&gt; 100.00% 1m21s

 Uploading /root/9600.17050.WINBLUE_REFRESH.140317-1640_X64FRE_SERVER_EVAL_EN-US-IR3_SSS_X64FREE_EN-US_DV9.ISO completed successfully
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Pulling the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtio&lt;/code&gt; container image to the locally:
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker pull kubevirt/virtio-container-disk
 Using default tag: latest
 Trying to pull repository docker.io/kubevirt/virtio-container-disk ... 
 latest: Pulling from docker.io/kubevirt/virtio-container-disk
 Digest: sha256:7e5449cb6a4a9586a3cd79433eeaafd980cb516119c03e499492e1e37965fe82
 Status: Image is up to &lt;span class=&quot;nb&quot;&gt;date &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;docker.io/kubevirt/virtio-container-disk:latest
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Creating the PVC and Virtual Machine definitions:
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; win2k12.yml
 virtualmachine.kubevirt.io/win2k12-iso configured
 persistentvolumeclaim/winhd created
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Starting the Virtual Machine Instance:
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtctl start win2k12-iso
 VM win2k12-iso was scheduled to start

 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get vmi
 NAME          AGE   PHASE     IP            NODENAME
 win2k12-iso   82s   Running   10.244.0.53   master-00.kubevirt-io
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Once the status of the VMI is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;RUNNING&lt;/code&gt; it’s time to connect using VNC:
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtctl vnc win2k12-iso
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;img src=&quot;/assets/2020-02-14-KubeVirt-installing_Microsoft_Windows_from_an_iso/windows2k12_install.png&quot; alt=&quot;windows2k12_install.png&quot; title=&quot;KubeVirt Microsoft Windows installation&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Here is important to comment that to be able to connect through VNC using &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; it’s necessary to reach the Kubernetes API. 
 The following video shows how to go through the Microsoft Windows installation process:&lt;/p&gt;
    &lt;figure class=&quot;video_container&quot;&gt;
 &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot; poster=&quot;/assets/2020-02-14-KubeVirt-installing_Microsoft_Windows_from_an_iso/kubevirt_install_windows.mp4&quot; width=&quot;800&quot; height=&quot;600&quot;&gt;
     &lt;source src=&quot;/assets/2020-02-14-KubeVirt-installing_Microsoft_Windows_from_an_iso/kubevirt_install_windows.mp4&quot; type=&quot;video/mp4&quot; /&gt;
 &lt;/video&gt;
 &lt;/figure&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the Virtual Machine is created, the PVC with the ISO and the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtio&lt;/code&gt; drivers can be unattached from the Virtual Machine.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/creating-virtual-machines/virtio-win.html&quot;&gt;KubeVirt user-guide: Virtio Windows Driver disk usage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer/blob/master/doc/image-from-registry.md&quot;&gt;Creating a registry image with a VM disk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer/blob/master/doc/upload.md&quot;&gt;CDI Upload User Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/creating-virtual-machines/virtio-win.html#how-to-obtain-virtio-drivers&quot;&gt;KubeVirt user-guide: How to obtain virtio drivers?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="microsoft windows kubernetes" /><category term="microsoft windows container" /><summary type="html">Hello! nowadays each operating system vendor has its cloud image available to download ready to import and deploy a new Virtual Machine (VM) inside Kubernetes with KubeVirt, but what if you want to follow the traditional way of installing a VM using an existing iso attached as a CDROM?</summary></entry><entry><title type="html">NA KubeCon 2019 - KubeVirt Deep Dive: Virtualized GPU Workloads on KubeVirt - David Vossel, Red Hat &amp;amp; Vishesh Tanksale, NVIDIA</title><link href="https://kubevirt.io//2020/KubeVirt_deep_dive-virtualized_gpu_workloads.html" rel="alternate" type="text/html" title="NA KubeCon 2019 - KubeVirt Deep Dive: Virtualized GPU Workloads on KubeVirt - David Vossel, Red Hat &amp; Vishesh Tanksale, NVIDIA" /><published>2020-02-06T00:00:00+00:00</published><updated>2020-02-06T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt_deep_dive-virtualized_gpu_workloads</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt_deep_dive-virtualized_gpu_workloads.html">&lt;p&gt;In this &lt;a href=&quot;https://www.youtube.com/watch?v=Qejlyny0G58&quot;&gt;video&lt;/a&gt;, David and Vishesh explore the architecture behind KubeVirt and how NVIDIA is leveraging that architecture to power GPU workloads on Kubernetes. 
Using NVIDIA’s GPU workloads as a case of study, they provide a focused view on how host device passthrough is accomplished with KubeVirt as well as providing some 
performance metrics comparing KubeVirt to standalone KVM.&lt;/p&gt;

&lt;h2 id=&quot;kubevirt-intro&quot;&gt;KubeVirt Intro&lt;/h2&gt;

&lt;p&gt;David introduces the talk showing what KubeVirt is and what is not:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;KubeVirt is not involved with managing AWS or GCP instances&lt;/li&gt;
  &lt;li&gt;KubeVirt is not a competitor to Firecracker or Kata containers&lt;/li&gt;
  &lt;li&gt;KubeVirt is not a container runtime replacement&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He likes to define KubeVirt as:&lt;/p&gt;
&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p&gt;KubeVirt is a Kubernetes extension that allows running traditional VM workloads natively side by side with Container workloads.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;But why KubeVirt?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Already have on-premise solutions like OpenStack, oVirt&lt;/li&gt;
  &lt;li&gt;And then there’s the public cloud, AWS, GCP, Azure&lt;/li&gt;
  &lt;li&gt;Why are we doing this VM management stuff yet again?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The answer is that the initial motivation for it was this idea of infrastructure convergence:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_infrastructure_convergence.png&quot; alt=&quot;kubevirt_infrastructure_convergence&quot; title=&quot;KubeVirt infrastructure convergence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The transition to the cloud model involves multiple stacks, containers and VMs, old code and new code. 
With KubeVirt all this is simplified with just one stack to manage containers and VMs to run old code and new code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_one_stack.png&quot; alt=&quot;kubevirt_one_stack&quot; title=&quot;KubeVirt one stack&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The workflow convergence means that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Converging VM management into container management workflows&lt;/li&gt;
  &lt;li&gt;Using the same tooling (kubectl) for containers and Virtual Machines&lt;/li&gt;
  &lt;li&gt;Keeping the declarative API for VM management (just like pods, deployments, etc…)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example of a VM Instance in YAML could be so simple as the following example:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;s&quot;&gt;$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachineInstance&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;na&quot;&gt;cores&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fedora29&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;The truth here is that a KubeVirt VM is a KVM+qemu process running inside a pod. It’s as simple as that.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_virtual_machine.png&quot; alt=&quot;kubevirt_virtual_machine&quot; title=&quot;KubeVirt VM = KVM+qemu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The VM Launch flow is shown in the following diagram. Since the user posts a VM manifest to the cluster until the Kubelet spins up the VM pod.
And finaly the virt-handler instructs the virt-launcher how to launch the qemu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_vm_launch_flow.png&quot; alt=&quot;kubevirt_vm_launch_flow&quot; title=&quot;KubeVirt VM launch flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The storage in KubeVirt is used in the same way as the pods, if there is a need to have persistent storage in a VM a PVC (Persistent Volume Claim) 
needs to be created.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_volumes.png&quot; alt=&quot;kubevirt_volumes&quot; title=&quot;KubeVirt volumes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, if you have a VM in your laptop, you can upload that image using the &lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer&quot;&gt;containerized-data-importer&lt;/a&gt; (CDI) to a PVC and then you can attach
that PVC to the VM pod to get it running.&lt;/p&gt;

&lt;p&gt;About the use of network services, the traffic routes to the KubeVirt VM in the same way it does to container workloads. Also with Multus there is
the possibility to have different network interfaces per VM.&lt;/p&gt;

&lt;p&gt;For using the Host Resources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;VM Guest CPU and NUMA Affinity
    &lt;ul&gt;
      &lt;li&gt;CPU Manager (pinning)&lt;/li&gt;
      &lt;li&gt;Topology Manager (NUMA nodes)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;VM Guest CPU/MEM requirements
    &lt;ul&gt;
      &lt;li&gt;POD resource request/limits&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;VM Guest use of Host Devices
    &lt;ul&gt;
      &lt;li&gt;Device Plugins for access to (/dev/kvm, SR-IOV, GPU passthrough)&lt;/li&gt;
      &lt;li&gt;POD resource request/limits for device allocation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gpuvgpu-in-kubevirt-vms&quot;&gt;GPU/vGPU in Kubevirt VMs&lt;/h2&gt;

&lt;p&gt;After the introduction of David, Vishesh takes over and talks in-depth the whys and hows of GPUs in VM. Lots of new Machine and Deep learning applications
are taking advance of the GPU workloads. Nowadays the Big data is one of the main consumers of GPUs but there are some gaps, the gaming and professional graphics sector 
still need to run VMs and have native GPU functionalities, that is why NVIDIA decided to work with KubeVirt.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/gpus_on_kubevirt.png&quot; alt=&quot;gpus_on_kubevirt&quot; title=&quot;GPU/vGPU on KubeVirt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To enable the device pass-through NVIDIA has developed the KubeVirt GPU device Plugin, it is available in &lt;a href=&quot;https://github.com/NVIDIA/kubevirt-gpu-device-plugin&quot;&gt;GitHub&lt;/a&gt;
It’s opensource and anybody can take a look to it and download it.&lt;/p&gt;

&lt;p&gt;Using the device plugin framework is a natural choice to provide GPU access to Kubevirt VMs, 
the following diagram shows the different layers involved in the GPU pass-through architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_gpu_passthrough.png&quot; alt=&quot;kubevirt_gpu_passthrough&quot; title=&quot;KubeVirt GPU passthrough&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Vishesh also comments an example of a YAML code where it can be seen the Node Status containing the NVIDIA card information (5 GPUS in that node), the Virtual Machine specification
containing the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;deviceName&lt;/code&gt; that points to that NVIDIA card and also the Pod Status where the user can set the limits and request for that resource as 
any other else in Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/gpu_pass_yaml.png&quot; alt=&quot;kubevirt_gpu_pass_yaml&quot; title=&quot;KubeVirt GPU passthrough yaml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The main Device Plugin Functions are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GPU and vGPU device Discovery
    &lt;ul&gt;
      &lt;li&gt;GPUs with VFIO-PCI driver on the host are identified&lt;/li&gt;
      &lt;li&gt;vGPUs configured using Nvidia vGPU manager are identified&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GPU and vGPU device Advertising
    &lt;ul&gt;
      &lt;li&gt;Discovered devices are advertised to kubelet as allocatable resources&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GPU and vGPU device Allocation
    &lt;ul&gt;
      &lt;li&gt;Returns the PCI address of allocated GPU device&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GPU and vGPU Health Check
    &lt;ul&gt;
      &lt;li&gt;Performs health check on the discovered GPU and vGPU devices&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To understand how the GPU passthrough lifecycle works Vishesh shows the different phases involve in the process using the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/gpu_pass_lifecycle.png&quot; alt=&quot;gpu_pass_lifecycle&quot; title=&quot;KubeVirt GPU passthrough lifecycle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the following diagram there are some of the Key features that NVIDIA is using with KubeVirt:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/NVIDIA_usecase_keyfeatures.png&quot; alt=&quot;NVIDIA_usecase_keyfeatures&quot; title=&quot;KubeVirt NVIDIA usecase keyfeatures&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are interested in the details of how the lifecycle works or in why NVIDIA is highly using some of the KubeVirt features listed above, you may be interested in
taking a look to the following video.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/Qejlyny0G58&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/davidvossel&quot;&gt;David Vossel&lt;/a&gt; David Vossel is a Principal Software Engineer at Red Hat. He is currently working on OpenShift’s Container Native Virtualization (CNV) 
and is a core contributor to the open source KubeVirt project.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/vishesh-tanksale&quot;&gt;Vishesh Tanksale&lt;/a&gt; is currently a Senior Software Engineer at NVIDIA. He is focussing on different aspects of enabling VM workload management on Kubernetes Cluster. 
He is specifically interested in GPU workloads on VMs. He is an active contributor to Kubevirt, a CNCF Sanbox Project.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Qejlyny0G58&quot;&gt;YouTube video: KubeVirt Deep Dive: Virtualized GPU Workloads on KubeVirt - David Vossel, Red Hat &amp;amp; Vishesh Tanksale, NVIDIA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://static.sched.com/hosted_files/kccncna19/31/KubeCon%202019%20-%20Virtualized%20GPU%20Workloads%20on%20KubeVirt.pdf&quot;&gt;Presentation: Virtualized GPU workloads on KubeVirt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kccncna19.sched.com/event/VnjX&quot;&gt;KubeCon NA 2019 event&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">In this video, David and Vishesh explore the architecture behind KubeVirt and how NVIDIA is leveraging that architecture to power GPU workloads on Kubernetes. Using NVIDIA’s GPU workloads as a case of study, they provide a focused view on how host device passthrough is accomplished with KubeVirt as well as providing some performance metrics comparing KubeVirt to standalone KVM.</summary></entry><entry><title type="html">NA KubeCon 2019 - KubeVirt introduction by Steve Gordon and Chandrakanth Jakkidi</title><link href="https://kubevirt.io//2020/KubeVirt_Intro-Virtual_Machine_Management_on_Kubernetes.html" rel="alternate" type="text/html" title="NA KubeCon 2019 - KubeVirt introduction by Steve Gordon and Chandrakanth Jakkidi" /><published>2020-02-01T00:00:00+00:00</published><updated>2020-02-01T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt_Intro-Virtual_Machine_Management_on_Kubernetes</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt_Intro-Virtual_Machine_Management_on_Kubernetes.html">&lt;p&gt;In this session, Steve and Chandra provide an introduction to the KubeVirt project, which turns Kubernetes into an 
orchestration engine for not just application containers but virtual machine workloads as well. This provides a 
unified development platform where developers can build, modify, and deploy applications made up of both application 
containers as well as the virtual machines (VM) in a common, shared environment.&lt;/p&gt;

&lt;p&gt;They show how the KubeVirt community is continuously growing and helping with their contributions to the code in
&lt;a href=&quot;https://github.com/kubevirt&quot;&gt;KubeVirt GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the session, you will learn more about why KubeVirt exists:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Growing velocity behind Kubernetes and surrounding ecosystem for new applications.&lt;/li&gt;
  &lt;li&gt;Reality that users will be dealing with virtual machine workloads for many years to come.&lt;/li&gt;
  &lt;li&gt;Focus on building transition paths for users with workloads that will either never be containerized:
    &lt;ul&gt;
      &lt;li&gt;Technical reasons (e.g. older operating system or kernel)&lt;/li&gt;
      &lt;li&gt;Business reasons (e.g. time to market, cost of conversion)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;…or will be decomposed over a longer time horizon.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They also explain the common use cases, how people are using it today:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;To run VMs to support new development
    &lt;ul&gt;
      &lt;li&gt;Build new applications relying on existing VM-based applications and APIs.&lt;/li&gt;
      &lt;li&gt;Leverage Kubernetes-based developer flows while bringing in these VM-based dependencies.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To run VMs to support applications that can’t lift and shift
    &lt;ul&gt;
      &lt;li&gt;Users with very old applications who are not in a position to change them significantly.&lt;/li&gt;
      &lt;li&gt;Vendors with appliances (customer kernels, custom kmods, optimized workflows to build appliances, …) they want to bring to the cloud-native ecosystem.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To run Kubernetes in Kubernetes (!)
    &lt;ul&gt;
      &lt;li&gt;KubeVirt as a Cluster API provider
        &lt;ul&gt;
          &lt;li&gt;Hard Multi-Tenancy&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Community provided cloud-provider-kubevirt&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To run Virtual Network Functions (VNFs) and other virtual appliances
    &lt;ul&gt;
      &lt;li&gt;VNFs in the context of Kubernetes are of continued interest, in parallel to Cloud-native Network Function exploration.
        &lt;ul&gt;
          &lt;li&gt;Kubernetes is an attractive target for VNFs.
            &lt;ul&gt;
              &lt;li&gt;Compute features and management approach is appealing.&lt;/li&gt;
              &lt;li&gt;But: VNFs are hard to containerize!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They also cover how the project actually works from an architectural perspective and the ideal environment.
&lt;img src=&quot;/assets/2020-01-29-KubeVirt_Intro-Virtual_Machine_Management_on_Kubernetes/containers_and_vms.png&quot; alt=&quot;architectural_perspective&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And how is the ideal environment with KubeVirt:
&lt;img src=&quot;/assets/2020-01-29-KubeVirt_Intro-Virtual_Machine_Management_on_Kubernetes/kubevirt_environment.png&quot; alt=&quot;kubevirt_environment&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A walk through the KubeVirt components is also shown:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;virt-api-server: The entry point to KubeVirt for all virtualization related flows and takes care to update the virtualization related custom resource definition (CRD)&lt;/li&gt;
  &lt;li&gt;virt-launcher: A VM is inside a POD launched by virt-launcher using Libvirt
&lt;img src=&quot;/assets/2020-01-29-KubeVirt_Intro-Virtual_Machine_Management_on_Kubernetes/pod_networking.png&quot; alt=&quot;pod_networking&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;virt-controller: Each Object has a corresponding controller&lt;/li&gt;
  &lt;li&gt;virt-handler: is a Daemonset that acts as a minion communication to Libvirt via socket&lt;/li&gt;
  &lt;li&gt;libvirtd: toolkit to manage virtualization platforms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the Video, a short demo of the project in action is shown. Eventually, Chandra shows how to install KubeVirt and bring up a virtual machine in a short time!&lt;/p&gt;

&lt;p&gt;Finally, you will hear about future plans for developing KubeVirt’s capabilities that are emerging from the community. Some hints:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Better support for deterministic workloads:
    &lt;ul&gt;
      &lt;li&gt;CPU Pinning○NUMA Topology Alignment&lt;/li&gt;
      &lt;li&gt;IO Thread pinning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Storage-assisted snapshot and cloning.&lt;/li&gt;
  &lt;li&gt;Forensic virtual machine capture&lt;/li&gt;
  &lt;li&gt;GPU passthrough&lt;/li&gt;
  &lt;li&gt;Policy-based live migration and additional migration modes.&lt;/li&gt;
  &lt;li&gt;Hotplugging of CPUs, RAM, disks, and NICs (not necessarily in that order!).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/_z5Pjyl0Dq4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/xsgordon&quot;&gt;Steve Gordon&lt;/a&gt; is currently a Principal Product Manager at Red Hat based in Toronto, Canada.&lt;br /&gt;
Focused on building infrastructure solutions for compute use cases using a spectrum of virtualization, containerization, 
and bare-metal provisioning technologies.&lt;/p&gt;

&lt;p&gt;He got his start in Open Source while building out and managing web-based solutions for the Earth Systems Science Computational 
Centre (ESSCC) at the University of Queensland. After graduating with degrees in Information Technology and Commerce, Stephen took 
a multi-year detour into the wonderful world of the z-Series mainframe while writing new COBOL applications for the Australian Tax Office (ATO).&lt;/p&gt;

&lt;p&gt;Stephen then landed at Red Hat where he has grown his knowledge of the infrastructure space working across multiple roles and solutions 
at the intersection of the Linux virtualization stack (KVM, QEMU, Libvirt), OpenStack, and more recently Kubernetes. Now he is working with a 
team attempting to realize a vision for unification of application containers and virtual machines enabled by the KubeVirt project.&lt;/p&gt;

&lt;p&gt;Stephen has previously presented on a variety of infrastructure topics at OpenStack Summit, multiple Red Hat Summit, KVM Forum, OpenStack Days Canada, 
OpenStack Silicon Valley, and local meetups.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/jakkidi-chandrakanth-reddy-149a5920/&quot;&gt;Chandrakanth Reddy Jakkidi&lt;/a&gt; is an active Open Source Contributor. He is involved in CNCF and open infrastructure community projects.
He has contributed to OpenStack and Kubernetes projects. Presently an active contributor to the Kubevirt Project.
Chandrakanth is having 14+ years experience in networking, virtualization, cloud, Kubernetes, SDN, NFV, OpenStack and infrastructure technologies.&lt;/p&gt;

&lt;p&gt;He is currently working with F5 Networks as Senior Software Engineer. He previously worked with Cisco Systems, Starent Networks, Emerson/Artesyn Embedded 
Technologies and NXP/Freescale Semiconductors/Intoto Network Security companies. He is a speaker and drives local open source meetups. His present passion 
is towards CNCF projects. In 2018, he was a speaker of the DevOpsDays Event.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_z5Pjyl0Dq4&quot;&gt;YouTube Video: KubeVirt Intro: Virtual Machine Management on Kubernetes - Steve Gordon &amp;amp; Chandrakanth Jakkidi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://static.sched.com/hosted_files/kccncna19/70/Introduction_to_KubeVirt-KUBECONNA19.pdf&quot;&gt;Presentation: KubeVirt Intro: Virtual Machine Management on Kubernetes - Steve Gordon &amp;amp; Chandrakanth Jakkidi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kccncna19.sched.com/event/VyBC&quot;&gt;KubeCon NA 2019 event&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">In this session, Steve and Chandra provide an introduction to the KubeVirt project, which turns Kubernetes into an orchestration engine for not just application containers but virtual machine workloads as well. This provides a unified development platform where developers can build, modify, and deploy applications made up of both application containers as well as the virtual machines (VM) in a common, shared environment.</summary></entry><entry><title type="html">Managing KubeVirt with Openshift Web Console</title><link href="https://kubevirt.io//2020/OKD-web-console-install.html" rel="alternate" type="text/html" title="Managing KubeVirt with Openshift Web Console" /><published>2020-01-24T00:00:00+00:00</published><updated>2020-01-24T00:00:00+00:00</updated><id>https://kubevirt.io//2020/OKD-web-console-install</id><content type="html" xml:base="https://kubevirt.io//2020/OKD-web-console-install.html">&lt;p&gt;In the previous post, &lt;a href=&quot;/2019/KubeVirt_UI_options.html&quot;&gt;KubeVirt user interface options&lt;/a&gt; were described and showed some features, pros and cons of using OKD console to manage our KubeVirt deployment. This blog post will focus on installing and running the OKD web console in a Kubernetes cluster so that it can leverage the deep integrations between KubeVirt and the OKD web console.&lt;/p&gt;

&lt;p&gt;There are two options to run the OKD web console to manage a Kubernetes cluster:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#binary-installation&quot;&gt;Executing the web console as a binary&lt;/a&gt;. This installation method is the only one described in the &lt;a href=&quot;https://github.com/openshift/console#build-everything&quot;&gt;OKD web console repository&lt;/a&gt;. Personally, looks like more targetted at developers who want to quickly iterate over the development process while hacking in the web console. This development approach is described in the &lt;a href=&quot;https://github.com/openshift/console#native-kubernetes&quot;&gt;native Kubernetes&lt;/a&gt; section of the OpenShift console code repository.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#containerized-installation&quot;&gt;Executing the web console as another pod&lt;/a&gt;. The idea is leveraging the containerized version available as origin-console in the &lt;a href=&quot;https://quay.io/repository/openshift/origin-console?tag=latest&amp;amp;tab=tags&quot;&gt;OpenShift container image repository&lt;/a&gt; and execute it inside a Kubernetes cluster as a regular application, e.g. as a pod.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-okd-web-console&quot;&gt;What is the OKD web console&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;a href=&quot;https://github.com/openshift/console&quot;&gt;OKD web console&lt;/a&gt; is a user interface accessible from a web browser. Developers can use the web console to visualize, browse, and manage the contents of namespaces. It is also referred as a more friendly &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; in the form of a single page webapp. It integrates with other services like monitoring, chargeback and the &lt;a href=&quot;https://github.com/operator-framework/operator-lifecycle-manager&quot;&gt;Operator Lifecycle Manager or OLM&lt;/a&gt;. Some things that go on behind the scenes include:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Proxying the Kubernetes API under /api/kubernetes&lt;/li&gt;
  &lt;li&gt;Providing additional non-Kubernetes APIs for interacting with the cluster&lt;/li&gt;
  &lt;li&gt;Serving all frontend static assets&lt;/li&gt;
  &lt;li&gt;User Authentication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As it is stated in the official GitHub’s repository, the OKD web console runs as a binary listening in a local port. The static assets required to run the web console are served by the binary itself. It is possible to customize the web console using extensions. The extensions have to be committed to be to the sources of the console directly.&lt;/p&gt;

&lt;p&gt;When the web console is accessed from a browser, it first loads all required static assets. Then makes requests to the Kubernetes API using the values defined as environment variables in the host where the console is running. Actually, there is a script called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;environment.sh&lt;/code&gt; that helps exporting the proper values for these environment variables.&lt;/p&gt;

&lt;p&gt;The web console uses WebSockets to maintain a persistent connection with the API server and receive updated information as soon as it is available. Note as well that JavaScript must be enabled to use the web console. For the best experience, use a web browser that supports WebSockets. OKD web console’s developers inform that Google Chrome/Chromium version 76 or greater is used in their integration tests.&lt;/p&gt;

&lt;p&gt;Unlike what is explained in the &lt;a href=&quot;https://github.com/openshift/console#native-kubernetes&quot;&gt;official repository&lt;/a&gt;, OKD actually executes the OKD web console in a pod. Therefore, even not officially mentioned, information how to run the OKD web console as a pod in a &lt;em&gt;native Kubernetes&lt;/em&gt; cluster will be described later.&lt;/p&gt;

&lt;h2 id=&quot;binary-installation&quot;&gt;Binary installation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;This method is suggested to be a development installation since it is mainly used by the OKD web console developers to test new features.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this section the OKD web console will be compiled from the source code and executed as a binary artifact in a &lt;strong&gt;CentOS 8&lt;/strong&gt; server which does not belong to any Kubernetes cluster. The following diagram shows the relationship between all the components: user, OKD web console and Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-01-24-OKD-web-console-install/OKD-console-kubevirt.png&quot; alt=&quot;Lab diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that it is possible to run the OKD web console in a Kubernetes master, in a regular node or, as shown, in a server outside the cluster. In the latter case, the external server must have access to the master API. &lt;em&gt;Notice also that it can be configured with different security and network settings or even different hardware resources&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The first step when using the binary installation is cloning the &lt;a href=&quot;https://github.com/openshift/console&quot;&gt;repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h3&gt;

&lt;p&gt;Below are detailed the dependencies needed to compile the OKD web console artifact:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt;. CentOS 8 was chosen as our operating system for the server running the OKD web console. Kubernetes cluster is running latest CentOS 7.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/redhat-release
CentOS Linux release 8.0.1905 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Core&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;. It has been deployed the latest available version at the moment of writing: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;v1.17&lt;/code&gt;. Kubernetes cluster is comprised by one master node and one regular node with enough CPU (4vCPUs) and memory (16Gi) to run KubeVirt and a couple of KubeVirt’s &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;VirtualMachineInstances&lt;/code&gt;. No extra storage was needed since the virtual machines will run as container-disk instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get nodes &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME                            STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
blog-master-00.kubevirt.local   Ready    master   29h   v1.17.0   192.168.123.250   &amp;lt;none&amp;gt;        CentOS Linux 7 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Core&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   3.10.0-957.27.2.el7.x86_64   docker://1.13.1
blog-worker-00.kubevirt.local   Ready    &amp;lt;none&amp;gt;   29h   v1.17.0   192.168.123.232   &amp;lt;none&amp;gt;        CentOS Linux 7 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Core&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   3.10.0-957.27.2.el7.x86_64   docker://1.13.1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Node.js &amp;gt;= 8&lt;/strong&gt;. Node.js 10 is available as an AppStream module.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum module &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;nodejs
Installed:
  nodejs-1:10.16.3-2.module_el8.0.0+186+542b25fc.x86_64   npm-1:6.9.0-1.10.16.3.2.module_el8.0.0+186+542b25fc.x86_64

Complete!
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;yarn &amp;gt;= 1.3.2&lt;/strong&gt;. Yarn is a dependency of Node.js. In this case, the official yarn repository has to be added as a local repositories.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;--silent&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--location&lt;/span&gt; https://dl.yarnpkg.com/rpm/yarn.repo | &lt;span class=&quot;nb&quot;&gt;sudo tee&lt;/span&gt; /etc/yum.repos.d/yarn.repo
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;rpm &lt;span class=&quot;nt&quot;&gt;--import&lt;/span&gt; https://dl.yarnpkg.com/rpm/pubkey.gpg
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;yarn

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;yarn &lt;span class=&quot;nt&quot;&gt;--version&lt;/span&gt;
1.21.1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;go &amp;gt;= 1.11+&lt;/strong&gt;. Golang is available as an AppStream module in CentOS 8:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum module &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;go-toolset

Installed:
  golang-1.11.6-1.module_el8.0.0+192+8b12aa21.x86_64

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;jq (for contrib/environment.sh)&lt;/strong&gt;. Finally, jq is installed in order to work with JSON data.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;jq

Installed:
  jq.x86_64 0:1.5-1.el7
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;compiling-okd-web-console&quot;&gt;Compiling OKD web console&lt;/h3&gt;

&lt;p&gt;Once all dependencies are met, access the cloned directory and export the correct variables that will be used during the building process. Then, execute &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;build.sh&lt;/code&gt; script which actually calls the build-frontend and build-backend scripts.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/openshift/console.git
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;console/
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBECONFIG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/.kube/config
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ./contrib/environment.sh
Using https://192.168.123.250:6443

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./build.sh
...
Done &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;215.91s.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result of the process is a binary file called &lt;strong&gt;bridge&lt;/strong&gt; inside the bin folder. Prior to run the &lt;em&gt;“bridge”&lt;/em&gt;, it has to be verified that the port where the OKD web console is expecting connections is not blocked.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;iptables &lt;span class=&quot;nt&quot;&gt;-A&lt;/span&gt; INPUT &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; tcp &lt;span class=&quot;nt&quot;&gt;--dport&lt;/span&gt; 9000 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; conntrack &lt;span class=&quot;nt&quot;&gt;--ctstate&lt;/span&gt; NEW,ESTABLISHED &lt;span class=&quot;nt&quot;&gt;-j&lt;/span&gt; ACCEPT
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, the artifact can be executed:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./bin/bridge
2020/01/7 10:21:17 cmd/main: cookies are not secure because base-address is not https!
2020/01/7 10:21:17 cmd/main: running with AUTHENTICATION DISABLED!
2020/01/7 10:21:17 cmd/main: Binding to 0.0.0.0:9000...
2020/01/7 10:21:17 cmd/main: not using TLS
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, the connection to the OKD web console from your network should be established successfully. Note that by default there is no authentication required to login into the console and the connection is using HTTP protocol. There are variables in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;environment.sh&lt;/code&gt; file that can change this default behaviour.&lt;/p&gt;

&lt;p&gt;Probably, the following issue will be faced when connecting to the web user interface: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;&quot;services is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace default&quot;&lt;/code&gt;. The problem is that the &lt;strong&gt;default service account&lt;/strong&gt; does not have enough privileges to show all the cluster objects.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-01-24-OKD-web-console-install/okd-serviceaccount-error.png&quot; alt=&quot;OKD sa error&quot; width=&quot;1110&quot; height=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;There are two options to fix the issue: one is granting cluster-admin permissions to the default service account. That, it is not recommended since default service account is, at its very name indicates, the default service account for all pods if not explicitly configured. This means granting cluster-admin permissions to some applications running in kube-system namespace and even future ones when no service account is configured.&lt;/p&gt;

&lt;p&gt;The other option is create a new service account called &lt;strong&gt;console&lt;/strong&gt;, grant cluster-admin permissions to it and configure the web console to run with this new service account:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create serviceaccount console &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create clusterrolebinding console &lt;span class=&quot;nt&quot;&gt;--clusterrole&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster-admin &lt;span class=&quot;nt&quot;&gt;--serviceaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system:console &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once created, modify the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;environment.sh&lt;/code&gt; file and change the line that starts with &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;secretname&lt;/code&gt; as shown below:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim contrib/environment.sh
&lt;span class=&quot;nv&quot;&gt;secretname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;kubectl get serviceaccount &lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;console&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;jsonpath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{.secrets[0].name}'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, variables configured in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;environment.sh&lt;/code&gt; file have to be exported again and the connection to the console must be reloaded.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ./contrib/environment.sh
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deploy-kubevirt-using-the-hyperconverged-cluster-operator-hco&quot;&gt;Deploy KubeVirt using the Hyperconverged Cluster Operator (HCO)&lt;/h2&gt;

&lt;p&gt;In order to ease the installation of KubeVirt, the unified operator called &lt;strong&gt;&lt;a href=&quot;https://github.com/kubevirt/hyperconverged-cluster-operator&quot;&gt;HCO&lt;/a&gt;&lt;/strong&gt; will be deployed. The goal of the hyperconverged-cluster-operator (HCO) is to provide a single entrypoint for multiple operators - &lt;a href=&quot;https://blog.openshift.com/a-first-look-at-kubevirt/&quot;&gt;kubevirt&lt;/a&gt;, &lt;a href=&quot;http://kubevirt.io/2018/CDI-DataVolumes.html&quot;&gt;cdi&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubevirt/cluster-network-addons-operator/blob/master/README.md&quot;&gt;networking&lt;/a&gt;, etc… - where users can deploy and configure them in a single object.&lt;/p&gt;

&lt;p&gt;This operator is sometimes referred to as a “meta operator” or an “operator for operators”. Most importantly, this operator doesn’t replace or interfere with OLM. It only creates operator CRs, which is the user’s prerogative. More information about HCO can be found in the post published in this blog by Ryan Hallisey: &lt;a href=&quot;https://kubevirt.io/2019/Hyper-Converged-Operator.html&quot;&gt;Hyper Converged Operator on OCP 4 and K8s(HCO)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The HCO repository provides plenty of information on how to install the operator. In this lab, it has been installed as &lt;a href=&quot;https://github.com/kubevirt/hyperconverged-cluster-operator#using-the-hco-without-olm-or-marketplace&quot;&gt;Using the HCO without OLM or Marketplace&lt;/a&gt;, which basically executes this deployment script:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/deploy.sh | bash
+ kubectl create ns kubevirt-hyperconverged
namespace/kubevirt-hyperconverged created
+ &lt;span class=&quot;nv&quot;&gt;namespaces&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;openshift&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
+ &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;namespace &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;namespaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[@]&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
++ kubectl get ns openshift
Error from server &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NotFound&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: namespaces &lt;span class=&quot;s2&quot;&gt;&quot;openshift&quot;&lt;/span&gt; not found
+ &lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;
+ kubectl create ns openshift
namespace/openshift created
++ kubectl config current-context
+ kubectl config set-context kubernetes-admin@kubernetes &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubevirt-hyperconverged
Context &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-admin@kubernetes&quot;&lt;/span&gt; modified.
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/cluster-network-addons00.crd.yaml
customresourcedefinition.apiextensions.k8s.io/networkaddonsconfigs.networkaddonsoperator.network.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/containerized-data-importer00.crd.yaml
customresourcedefinition.apiextensions.k8s.io/cdis.cdi.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/hco.crd.yaml
customresourcedefinition.apiextensions.k8s.io/hyperconvergeds.hco.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/kubevirt00.crd.yaml
customresourcedefinition.apiextensions.k8s.io/kubevirts.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/node-maintenance00.crd.yaml
customresourcedefinition.apiextensions.k8s.io/nodemaintenances.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/scheduling-scale-performance00.crd.yaml
customresourcedefinition.apiextensions.k8s.io/kubevirtcommontemplatesbundles.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/scheduling-scale-performance01.crd.yaml
customresourcedefinition.apiextensions.k8s.io/kubevirtmetricsaggregations.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/scheduling-scale-performance02.crd.yaml
customresourcedefinition.apiextensions.k8s.io/kubevirtnodelabellerbundles.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/scheduling-scale-performance03.crd.yaml
customresourcedefinition.apiextensions.k8s.io/kubevirttemplatevalidators.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/crds/v2vvmware.crd.yaml
customresourcedefinition.apiextensions.k8s.io/v2vvmwares.kubevirt.io created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/cluster_role.yaml
role.rbac.authorization.k8s.io/cluster-network-addons-operator created
clusterrole.rbac.authorization.k8s.io/hyperconverged-cluster-operator created
clusterrole.rbac.authorization.k8s.io/cluster-network-addons-operator created
clusterrole.rbac.authorization.k8s.io/kubevirt-operator created
clusterrole.rbac.authorization.k8s.io/kubevirt-ssp-operator created
clusterrole.rbac.authorization.k8s.io/cdi-operator created
clusterrole.rbac.authorization.k8s.io/node-maintenance-operator created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/service_account.yaml
serviceaccount/cdi-operator created
serviceaccount/cluster-network-addons-operator created
serviceaccount/hyperconverged-cluster-operator created
serviceaccount/kubevirt-operator created
serviceaccount/kubevirt-ssp-operator created
serviceaccount/node-maintenance-operator created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/cluster_role_binding.yaml
rolebinding.rbac.authorization.k8s.io/cluster-network-addons-operator created
clusterrolebinding.rbac.authorization.k8s.io/hyperconverged-cluster-operator created
clusterrolebinding.rbac.authorization.k8s.io/cluster-network-addons-operator created
clusterrolebinding.rbac.authorization.k8s.io/kubevirt-operator created
clusterrolebinding.rbac.authorization.k8s.io/kubevirt-ssp-operator created
clusterrolebinding.rbac.authorization.k8s.io/cdi-operator created
clusterrolebinding.rbac.authorization.k8s.io/node-maintenance-operator created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/operator.yaml
deployment.apps/hyperconverged-cluster-operator created
deployment.apps/cluster-network-addons-operator created
deployment.apps/virt-operator created
deployment.apps/kubevirt-ssp-operator created
deployment.apps/cdi-operator created
deployment.apps/node-maintenance-operator created
+ kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubevirt/hyperconverged-cluster-operator/master/deploy/hco.cr.yaml
hyperconverged.hco.kubevirt.io/hyperconverged-cluster created
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result is a new namespace called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubevirt-hyperconverged&lt;/code&gt; with all the operators, Custom Resources (CRs) and objects needed by KubeVirt:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt-hyperconverged &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME                                                  READY   STATUS    RESTARTS   AGE   IP                NODE                            NOMINATED NODE   READINESS GATES
bridge-marker-bwq6f                                   1/1     Running   0          12m   192.168.123.250   blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
bridge-marker-st7f7                                   1/1     Running   0          12m   192.168.123.232   blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
cdi-apiserver-6f59996849-2hmm9                        1/1     Running   0          12m   10.244.1.17       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
cdi-deployment-57c68dbddc-c4n8l                       1/1     Running   0          12m   10.244.1.22       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
cdi-operator-64bbf595c-48v7k                          1/1     Running   1          24m   10.244.1.12       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
cdi-uploadproxy-5cbf6f4897-95fn5                      1/1     Running   0          12m   10.244.1.16       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
cluster-network-addons-operator-5956598648-5r79l      1/1     Running   0          24m   10.244.1.10       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
hyperconverged-cluster-operator-d567b5dd8-7d8wq       0/1     Running   0          24m   10.244.1.9        blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-cni-linux-bridge-plugin-kljvq                    1/1     Running   0          12m   10.244.1.19       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-cni-linux-bridge-plugin-p6dkz                    1/1     Running   0          12m   10.244.0.7        blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-multus-ds-amd64-84gcj                            1/1     Running   1          12m   10.244.1.23       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-multus-ds-amd64-flq8s                            1/1     Running   2          12m   10.244.0.10       blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kubemacpool-mac-controller-manager-675ff47587-pb57c   1/1     Running   0          11m   10.244.1.20       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kubemacpool-mac-controller-manager-675ff47587-rf65j   1/1     Running   0          11m   10.244.0.8        blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kubevirt-ssp-operator-7b5dcb45c4-qd54h                1/1     Running   0          24m   10.244.1.11       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nmstate-handler-8r6d5                                 1/1     Running   0          11m   192.168.123.232   blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nmstate-handler-cq5vs                                 1/1     Running   0          11m   192.168.123.250   blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
node-maintenance-operator-7f8f78c556-q6flt            1/1     Running   0          24m   10.244.0.5        blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
ovs-cni-amd64-4z2qt                                   2/2     Running   0          11m   192.168.123.250   blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
ovs-cni-amd64-w8fzj                                   2/2     Running   0          11m   192.168.123.232   blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-api-7b7d486d88-hg4rd                             1/1     Running   0          11m   10.244.1.21       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-api-7b7d486d88-r9s2d                             1/1     Running   0          11m   10.244.0.9        blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-controller-754466fb86-js6r7                      1/1     Running   0          10m   10.244.1.25       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-controller-754466fb86-mcxwd                      1/1     Running   0          10m   10.244.0.11       blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-handler-cz7q2                                    1/1     Running   0          10m   10.244.0.12       blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-handler-k6npr                                    1/1     Running   0          10m   10.244.1.24       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-operator-84f5588df6-2k49b                        1/1     Running   0          24m   10.244.1.14       blog-worker-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-operator-84f5588df6-zzrsb                        1/1     Running   1          24m   10.244.0.4        blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Note&lt;/strong&gt; that only once HCO is completely deployed, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;VirtualMachines&lt;/code&gt; can be managed from the web console. This is because the web console is shipped with an specific plugin that detects a KubeVirt installation by the presence of KubeVirt Custom Resource Definition (CRDs) in the cluster. Once detected, it automatically shows a new option under the Workload left pane menu to manage KubeVirt resources.&lt;/p&gt;

&lt;p&gt;It is worth noting that there is an ongoing effort to adapt the OpenShift web console’s user interface in native Kubernetes additionally to OKD or OpenShift as they are expected. &lt;a href=&quot;https://github.com/openshift/console/pull/3848&quot;&gt;As an example&lt;/a&gt;, a few days ago, the non applicable Virtual Machine Templates option from the Workload menu was removed and the VM Wizard was made fully functional when native Kubernetes is detected.
&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;1110&quot; height=&quot;650&quot; src=&quot;https://www.youtube.com/embed/XQw4GkGHs44&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;containerized-installation&quot;&gt;Containerized installation&lt;/h2&gt;

&lt;p&gt;The OKD web console actually runs as a pod in OKD along with its deployment, services and all objects needed to run properly. The idea is to take advantage of the containerized OKD web console available and execute it in one of the nodes of a native Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that unlike the binary installation the pod must run in a node inside our Kubernetes cluster&lt;/em&gt;. On the other side, running the OKD web console as a native Kubernetes application will benefit from all the Kubernetes advantages: rolling deployments, easy upgrades, high availability, scalability, auto-restart in case of failure, liveness and readiness probes… An example of how easy it is to update the OKD web console to a newer version will be presented as well.&lt;/p&gt;

&lt;h3 id=&quot;deploying-okd-web-console&quot;&gt;Deploying OKD web console&lt;/h3&gt;

&lt;p&gt;In order to configure the deployment of the OKD web console the proper Kubernetes objects have to be created. As shown in the previously &lt;a href=&quot;#compiling-okd-web-console&quot;&gt;Compiling OKD web console&lt;/a&gt; there are quite a few environment variables that needs to be set. When dealing with Kubernetes objects these variables should be included in the deployment object.&lt;/p&gt;

&lt;p&gt;A YAML file containing a deployment and service objects that mimic the binary installation is already prepared. It can be downloaded from &lt;a href=&quot;../assets/2020-01-24-OKD-web-console-install/okd-web-console-install.yaml&quot;&gt;here&lt;/a&gt; and configured depending on the user’s local installation.&lt;/p&gt;

&lt;p&gt;Then, create a specific service account (&lt;strong&gt;console&lt;/strong&gt;) for running the OpenShift web console in case it is not created &lt;a href=&quot;#compiling-okd-web-console&quot;&gt;previously&lt;/a&gt; and grant cluster-admin permissions:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create serviceaccount console &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create clusterrolebinding console &lt;span class=&quot;nt&quot;&gt;--clusterrole&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster-admin &lt;span class=&quot;nt&quot;&gt;--serviceaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system:console &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, extract the &lt;strong&gt;token secret name&lt;/strong&gt; associated with the console service account:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get serviceaccount console &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;jsonpath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{.secrets[0].name}'&lt;/span&gt;
console-token-ppfc2
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the downloaded YAML file must be modified assigning the proper values to the &lt;strong&gt;token&lt;/strong&gt; section. The following command may help to extract the token name from the user console, which is a user created by&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-deployment&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-app&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;quay.io/openshift/origin-console:4.2&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_USER_AUTH&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;disabled&lt;/span&gt;         &lt;span class=&quot;c1&quot;&gt;# no authentication required&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_MODE&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;off-cluster&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_MODE_OFF_CLUSTER_ENDPOINT&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://kubernetes.default&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#master api&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_MODE_OFF_CLUSTER_SKIP_VERIFY_TLS&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# no tls enabled&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_AUTH&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bearer-token&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_AUTH_BEARER_TOKEN&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-token-ppfc2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# console serviceaccount token&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;token&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-np-service&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NodePort&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# nodePort configuration&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9000&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9000&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;nodePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;30036&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the deployment and service objects can be created. The deployment will trigger the download and installation of the OKD web console image.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; okd-web-console-install.yaml
deployment.apps/console-deployment created
service/console-service created

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
NAME                                                    READY   STATUS    RESTARTS   AGE     IP                NODE                            NOMINATED NODE   READINESS GATES
console-deployment-59d8956db5-td462                     1/1     Running   0          4m49s   10.244.0.13       blog-master-00.kubevirt.local   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get svc &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                  AGE
console-np-service   NodePort    10.96.195.45   &amp;lt;none&amp;gt;        9000:30036/TCP           19m
kube-dns             ClusterIP   10.96.0.10     &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   20d
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once running, a connection to the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;nodeport&lt;/code&gt; defined in the service object can be established. It can be checked that the OKD web console is up and running version 4.2.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-01-24-OKD-web-console-install/okd-pod-4.2.resized.png&quot; alt=&quot;OKD 4.2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
It can be verified that it is possible to see and manage &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;VirtualMachines&lt;/code&gt; running inside of the native Kubernetes cluster.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-01-24-OKD-web-console-install/okd-console-vm.resized.png&quot; alt=&quot;OKD vmr&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;upgrade-okd-web-console&quot;&gt;Upgrade OKD web console&lt;/h3&gt;

&lt;p&gt;The upgrade process is really straightforward. All available image versions of the OpenShift console can be consulted in the &lt;a href=&quot;https://quay.io/repository/openshift/origin-console?tab=tags&quot;&gt;official OpenShift container image repository&lt;/a&gt;. Then, the deployment object must be modified accordingly to run the desired version of the OKD web console.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/2020-01-24-OKD-web-console-install/quay-okd-repo.resized.png&quot; alt=&quot;OKD vmr&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this case, the we console will be updated to the newest version, which is 4.5.0/4.5. &lt;em&gt;Note that this is not linked with the latest tag, actually &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;latest&lt;/code&gt; tag is the same as version &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;4.4&lt;/code&gt;&lt;/em&gt;. Upgrading process only involves updating the image value to the desired container image: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;quay.io/openshift/origin-console:4.5&lt;/code&gt; and save.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-deployment&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-app&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;quay.io/openshift/origin-console:4.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#new image version&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_USER_AUTH&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;disabled&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_MODE&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;off-cluster&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_MODE_OFF_CLUSTER_ENDPOINT&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://kubernetes.default&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_MODE_OFF_CLUSTER_SKIP_VERIFY_TLS&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_AUTH&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bearer-token&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BRIDGE_K8S_AUTH_BEARER_TOKEN&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;console-token-ppfc2&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;token&lt;/span&gt;

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Once the deployment has been saved, a new pod with the configured version of the OKD web console is created and eventually will replace the old one.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
NAME                                                    READY   STATUS              RESTARTS   AGE
console-deployment-5588f98644-bw7jr                     0/1     ContainerCreating   0          5s
console-deployment-59d8956db5-td462                     1/1     Running             0          16h
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/2020-01-24-OKD-web-console-install/okd-console-4.5.resized.png&quot; alt=&quot;OKD web console 4.5&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In the video below, the procedure explained in this section is shown.
&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;1110&quot; height=&quot;650&quot; src=&quot;https://www.youtube.com/embed/xoL0UFI657I&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this post &lt;em&gt;two ways to install the OKD web console to manage a KubeVirt deployment in a native Kubernetes cluster have been explored&lt;/em&gt;. Running the OKD web console will allow you to create, manage and delete virtual machines running in a native cluster from a friendly user interface. Also you will be able to delegate to the developers or other users the creation and maintenance of their virtual machines without having a deep knowledge of Kubernetes.&lt;/p&gt;

&lt;p&gt;Personally, I would like to see more user interfaces to manage and configure KubeVirt deployments and their virtual machines. In a previous post, &lt;a href=&quot;https://kubevirt.io/2019/KubeVirt_UI_options.html&quot;&gt;KubeVirt user interface options&lt;/a&gt;, some options were explored, however only OKD web console was found to be deeply integrated with KubeVirt.&lt;/p&gt;

&lt;p&gt;Ping us or feel free to comment this post in case there are some other existing options that we did not notice.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2019/KubeVirt_UI_options.html&quot;&gt;KubeVirt user interface options&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xoL0UFI657I&quot;&gt;Managing KubeVirt with OpenShift web console running as a container application on Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XQw4GkGHs44&amp;amp;t=37s&quot;&gt;ManagManaging KubeVirt with OpenShift web console running as a compiled binary on Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2019/KubeVirt_lab1_use_kubevirt.html&quot;&gt;Kubevirt Laboratory 1 blogpost: Use Kubevirt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KC03G60shIc&quot;&gt;KubeVirt basic operations video on Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;Kubevirt installation notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt - Katacoda scenario&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alberto Losada Grande</name></author><category term="news" /><summary type="html">In the previous post, KubeVirt user interface options were described and showed some features, pros and cons of using OKD console to manage our KubeVirt deployment. This blog post will focus on installing and running the OKD web console in a Kubernetes cluster so that it can leverage the deep integrations between KubeVirt and the OKD web console.</summary></entry><entry><title type="html">KubeVirt Laboratory 3, upgrades</title><link href="https://kubevirt.io//2020/KubeVirt_lab3_upgrade.html" rel="alternate" type="text/html" title="KubeVirt Laboratory 3, upgrades" /><published>2020-01-21T00:00:00+00:00</published><updated>2020-01-21T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt_lab3_upgrade</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt_lab3_upgrade.html">&lt;p&gt;In this video, we are showing the step by step of the &lt;a href=&quot;/labs/kubernetes/lab3&quot;&gt;KubeVirt Laboratory 3: Upgrades&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;

&lt;p&gt;In the video, there is a Kubernetes cluster already running.  Also, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; command is already installed and available in the PATH.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/OAPzOvqp0is&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;The following operations are shown in the video:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Install a specific version of the KubeVirt Operator and Custom Resource&lt;/li&gt;
  &lt;li&gt;Create a cirros Virtual Machine running in KubeVirt&lt;/li&gt;
  &lt;li&gt;Connect to the Virtual Machine using SSH&lt;/li&gt;
  &lt;li&gt;Upgrade to a specific version of KubeVirt while the Virtual Machine is running&lt;/li&gt;
  &lt;li&gt;Check the new KubeVirt version&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/labs/kubernetes/lab3&quot;&gt;Kubevirt Laboratory 3: Upgrades&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2019/KubeVirt_lab2_experiment_with_cdi.html&quot;&gt;Kubevirt Laboratory 2 blogpost: Experimenting with CDI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/labs/kubernetes/lab2&quot;&gt;Kubevirt Laboratory 2: Experimenting with CDI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2019/KubeVirt_lab1_use_kubevirt.html&quot;&gt;Kubevirt Laboratory 1 blogpost: Use Kubevirt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/labs/kubernetes/lab1&quot;&gt;Kubevirt Laboratory 1: Use KubeVirt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KC03G60shIc&quot;&gt;KubeVirt basic operations video on youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;Kubevirt installation notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt - Katacoda scenario&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">In this video, we are showing the step by step of the KubeVirt Laboratory 3: Upgrades</summary></entry><entry><title type="html">KubeVirt user interface options</title><link href="https://kubevirt.io//2019/KubeVirt_UI_options.html" rel="alternate" type="text/html" title="KubeVirt user interface options" /><published>2019-12-17T00:00:00+00:00</published><updated>2019-12-17T00:00:00+00:00</updated><id>https://kubevirt.io//2019/KubeVirt_UI_options</id><content type="html" xml:base="https://kubevirt.io//2019/KubeVirt_UI_options.html">&lt;blockquote&gt;
  &lt;p&gt;The user interface (UI), in the industrial design field of human–computer interaction, is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators’ decision-making process. &lt;a href=&quot;https://en.wikipedia.org/wiki/User_interface&quot;&gt;Wikipedia:User Interface&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this blogpost we show the results of a research about the different options existing in the market to enable KubeVirt with a user interface to manage, access and control the life cycle of the Virtual Machines inside Kubernetes with KubeVirt.&lt;/p&gt;

&lt;p&gt;The different UI options available for KubeVirt that we have been checking, at the moment of writing this article, are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/vmware-tanzu/octant&quot;&gt;Octant&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/openshift/okd&quot;&gt;OKD: The Origin Community Distribution of Kubernetes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/openshift/console&quot;&gt;Openshift console&lt;/a&gt; running on vanilla Kubernetes&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cockpit-project.org/&quot;&gt;Cockpit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://novnc.com/info.html&quot;&gt;noVNC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;octant&quot;&gt;Octant&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/octant-logo.png&quot; alt=&quot;Octant logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the &lt;a href=&quot;https://octant.dev/&quot;&gt;Octant webpage&lt;/a&gt; claims:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Octant is an open-source developer-centric web interface for Kubernetes that lets you inspect a Kubernetes cluster and its applications. Octant is a tool for developers to understand how applications run on a Kubernetes cluster. It aims to be part of the developer’s toolkit for gaining insight and approaching complexity found in Kubernetes. Octant offers a combination of introspective tooling, cluster navigation, and object management along with a plugin system to further extend its capabilities.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some of the key features of this tool can be checked in their &lt;a href=&quot;https://octant.dev/docs/master/&quot;&gt;latest release notes&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Resource Viewer&lt;/strong&gt;: Graphically visualize relationships between objects in a Kubernetes cluster. The status of individual objects is represented by colour to show workload performance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Summary View&lt;/strong&gt;: Consolidated status and configuration information in a single page aggregated from output typically found using multiple kubectl commands.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Port Forward&lt;/strong&gt;: Forward a local port to a running pod with a single button for debugging applications and even port forward multiple pods across namespaces.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Log Stream&lt;/strong&gt;: View log streams of pod and container activity for troubleshooting or monitoring without holding multiple terminals open.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Label Filter&lt;/strong&gt;: Organize workloads with label filtering for inspecting clusters with a high volume of objects in a namespace.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cluster Navigation&lt;/strong&gt;: Easily change between namespaces or contexts across different clusters. Multiple kubeconfig files are also supported.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt;: Highly extensible plugin system for users to provide additional functionality through gRPC. Plugin authors can add components on top of existing views.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We installed it and found out that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Octant provides a very basic dashboard for Kubernetes and it is pretty straightforward to install. It can be installed in your laptop or in a remote server.&lt;/li&gt;
  &lt;li&gt;Regular Kubernetes objects can be seen from the UI. Pod logs can be checked as well. However, mostly everything is in view mode, even the YAML description of the objects. Therefore, as a developer or cluster operator you cannot edit YAML files directly from the UI&lt;/li&gt;
  &lt;li&gt;Custom resources (CRs) and custom resource definitions (CRDs) are automatically detected and shown in the UI. This means that KubeVirt CRs can be viewed from the dashboard. However, VirtualMachines and VirtualMachineInstances cannot be modified from Octant, they can only be deleted.&lt;/li&gt;
  &lt;li&gt;There is an option to extend the functionality adding &lt;a href=&quot;https://octant.dev/docs/master/plugins/reference/&quot;&gt;plugins&lt;/a&gt; to the dashboard.&lt;/li&gt;
  &lt;li&gt;No specific options to manage KubeVirt workloads have been found.&lt;/li&gt;
&lt;/ul&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; src=&quot;/assets/2019-12-19-KubeVirt_UI_options/octant.mp4&quot; type=&quot;video/mp4&quot; width=&quot;1280&quot; height=&quot;720&quot;&gt;&lt;/video&gt;

&lt;p&gt;With further work and investigation, it could be an option to develop a specific plugin to enable remote console or VNC access to KubeVirt workloads.&lt;/p&gt;

&lt;h2 id=&quot;okd-the-origin-community-distribution-of-kubernetes&quot;&gt;OKD: The Origin Community Distribution of Kubernetes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/okd_logo.png&quot; alt=&quot;OKD logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As defined in the &lt;a href=&quot;https://www.okd.io/&quot;&gt;official webpage&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;OKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams. OKD is the upstream Kubernetes distribution embedded in Red Hat OpenShift.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;OKD embeds Kubernetes and extends it with security and other integrated concepts. OKD is also referred to as Origin in github and in the documentation. An OKD release corresponds to the Kubernetes distribution - for example, OKD 1.10 includes Kubernetes 1.10.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A few weeks ago Kubernetes distribution &lt;a href=&quot;https://github.com/openshift/okd&quot;&gt;OKD4&lt;/a&gt; was released as preview. OKD is the official upstream version of Red Hat’s Openshift. Since Openshift includes KubeVirt (Red Hat calls it &lt;a href=&quot;https://docs.openshift.com/container-platform/4.2/cnv/cnv_install/cnv-about-cnv.html&quot;&gt;CNV&lt;/a&gt;) as a tech-preview feature since a couple of releases, there is already a lot of integration going on between OKD console and KubeVirt.&lt;/p&gt;

&lt;p&gt;Note that OKD4 is in preview, which means that only a subset of platforms and functionality will be available until it reaches beta. That being said, we have we found a similar behaviour as testing KubeVirt with Openshift. We have noticed that from the UI a user can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install the KubeVirt operator from the operator marketplace.&lt;/li&gt;
  &lt;li&gt;Create Virtual Machines by importing YAML files or following a wizard. The wizard prevents you from moving to the next screen until you provide values in the required fields.&lt;/li&gt;
  &lt;li&gt;Modify the status of the Virtual Machine: stop, start, migrate, clone, edit label, edit annotations, edit CD-ROMs and delete&lt;/li&gt;
  &lt;li&gt;Edit network interfaces. It is possible to add multiple network interfaces to the VM.&lt;/li&gt;
  &lt;li&gt;Add disks to the VM&lt;/li&gt;
  &lt;li&gt;Connect to the VM via serial or VNC console.&lt;/li&gt;
  &lt;li&gt;Edit the YAML object files online.&lt;/li&gt;
  &lt;li&gt;Create VM templates. The web console features an interactive wizard that guides you through the Basic Settings, Networking, and Storage screens to simplify the process of creating virtual machine templates.&lt;/li&gt;
  &lt;li&gt;Check VM events in real time.&lt;/li&gt;
  &lt;li&gt;Gather metrics and utilization of the VM.&lt;/li&gt;
  &lt;li&gt;Pretty much everything you can do with KubeVirt from the command line.&lt;/li&gt;
&lt;/ul&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; src=&quot;/assets/2019-12-19-KubeVirt_UI_options/okd.mp4&quot; type=&quot;video/mp4&quot; width=&quot;1280&quot; height=&quot;720&quot;&gt;&lt;/video&gt;

&lt;p&gt;One of the drawbacks is that the current &lt;a href=&quot;https://operatorhub.io/operator/kubevirt&quot;&gt;KubeVirt HCO operator&lt;/a&gt; contains KubeVirt version 0.18.1, which is quite outdated. Note that last week version 0.24 of KubeVirt was released. Using such an old release could cause some issues when creating VMs using newer container disk images. For instance, we have not been able to run the latest &lt;a href=&quot;https://hub.docker.com/r/kubevirt/fedora-cloud-container-disk-demo&quot;&gt;Fedora cloud container disk image&lt;/a&gt; and instead we were forced to use the one tagged as v0.18.1 which matches the version of KubeVirt deployed.&lt;/p&gt;

&lt;p&gt;If for any reason there is a need to deploy the latest version, it can be done by running the following script which applies directly the HCO operator: &lt;a href=&quot;https://github.com/kubevirt/hyperconverged-cluster-operator#using-the-hco-without-olm-or-marketplace&quot;&gt;unreleased bundles using the hco without marketplace&lt;/a&gt;. Note that in this case automatic updates to KubeVirt are not triggered or advised automatically in OKD as it happens with the operator.&lt;/p&gt;

&lt;h2 id=&quot;openshift-console-bridge&quot;&gt;OpenShift console (bridge)&lt;/h2&gt;

&lt;p&gt;There is actually a &lt;a href=&quot;https://github.com/kubevirt/web-ui&quot;&gt;KubeVirt Web User Interface&lt;/a&gt;, however the standalone project was deprecated in favor of OpenShift Console where it is included as a plugin.&lt;/p&gt;

&lt;p&gt;As we reviewed previously the &lt;a href=&quot;https://github.com/openshift/console&quot;&gt;OpenShift web console&lt;/a&gt; is just another piece inside OKD. It is an independent part and, as it is stated in their official GitHub repository, it can run on top of native Kubernetes. OpenShift Console a.k.a bridge is defined as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;a friendly kubectl in the form of a single page webapp. It also integrates with other services like monitoring, chargeback, and OLM. Some things that go on behind the scenes include:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Proxying the Kubernetes API under /api/kubernetes&lt;/li&gt;
  &lt;li&gt;Providing additional non-Kubernetes APIs for interacting with the cluster&lt;/li&gt;
  &lt;li&gt;Serving all frontend static assets&lt;/li&gt;
  &lt;li&gt;User Authentication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then, as briefly explained in their &lt;a href=&quot;https://github.com/openshift/console#native-kubernetes&quot;&gt;repository&lt;/a&gt;, our Kubernetes cluster can be configured to run the OpenShift Console and leverage its integrations with KubeVirt. Features related to KubeVirt are similar to the ones found in the OKD installation except:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;KubeVirt installation is done using the &lt;a href=&quot;https://github.com/kubevirt/hyperconverged-cluster-operator#using-the-hco-without-olm-or-marketplace&quot;&gt;Hyperconverged Cluster Operator (HCO) without OL or Marketplace&lt;/a&gt; instead of the KubeVirt operator. Therefore, available updates to KubeVirt are not triggered or advised automatically&lt;/li&gt;
  &lt;li&gt;Virtual Machines objects can only be created from YAML. Although the wizard dialog is still available in the console, it does not function properly because it uses specific OpenShift objects under the hood. These objects are not available in our native Kubernetes deployment.&lt;/li&gt;
  &lt;li&gt;Connection to the VM via serial or VNC console is flaky.&lt;/li&gt;
  &lt;li&gt;VM templates can only be created from YAML. The wizard dialog is based on OpenShift templates.&lt;/li&gt;
&lt;/ul&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; src=&quot;/assets/2019-12-19-KubeVirt_UI_options/bridge-k8s.mp4&quot; type=&quot;video/mp4&quot; width=&quot;1280&quot; height=&quot;720&quot;&gt;&lt;/video&gt;

&lt;p&gt;Note that the OpenShift console documentation briefly points out how to integrate the OpenShift console with a native Kubernetes deployment. It is uncertain if it can be installed in any other Kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&quot;cockpit&quot;&gt;Cockpit&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/cockpit-logo.png&quot; alt=&quot;Cockpit logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When testing cockpit in a CentOS 7 server with a Kubernetes cluster and KubeVirt we have realised that some of the containers/k8s features have to be enabled installing extra cockpit packages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To see the containers and images the package &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cockpit-docker&lt;/code&gt; has to be installed, then a new option called containers appears in the menu.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/cockpit_containers_800.png&quot; alt=&quot;Containers&quot; title=&quot;cockpit containers&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To see the k8s cluster the package &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cockpit-kubernetes&lt;/code&gt; has to be installed and a new tab appears in the left menu. The new options allow you to:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Overview&lt;/strong&gt;: filtering by project, it shows Pods, volumes, Nodes, services and resources used.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/cockpit_k8s_cluster_overview_800.png&quot; alt=&quot;Cluster overview&quot; title=&quot;cockpit cluster overview&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Nodes&lt;/strong&gt;: nodes and the resources used are being shown here.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Containers&lt;/strong&gt;: a full list of containers and some metadata about them is displayed in this option.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Topology&lt;/strong&gt;: A graph with the pods, services and nodes is shown in this option.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/cockpit_k8s_topology_800.png&quot; alt=&quot;Cluster topology&quot; title=&quot;cockpit cluster topology&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Details&lt;/strong&gt;: allows to filter by project and type of resource and shows some metadata in the results.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Volumes&lt;/strong&gt;: allows to filter by project and shows the volumes with the type and the status.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In CentOS 7 there are also the following packages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cockpit-machines.x86_64&lt;/code&gt; : Cockpit user interface for virtual machines. If “virt-install” is installed, you can also create new virtual machines. 
It adds a new option in the main menu called Virtual Machines but it uses libvirt and is not KubeVirt related.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cockpit-machines-ovirt.noarch&lt;/code&gt; : Cockpit user interface for oVirt virtual machines, like the package above but with support for ovirt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the moment none of the cockpit complements has support for KubeVirt Virtual Machine.&lt;/p&gt;

&lt;p&gt;KubeVirt support for cockpit was &lt;a href=&quot;https://bugzilla.redhat.com/show_bug.cgi?id=1629608&quot;&gt;removed from fedora 29&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;novnc&quot;&gt;noVNC&lt;/h2&gt;

&lt;p&gt;noVNC is a JavaScript VNC client using WebSockets and HTML5 Canvas.
It just allows you to connect through VNC to the virtual Machine already deployed in KubeVirt.&lt;/p&gt;

&lt;p&gt;No VM management or even a dashboard is enabled with this option, it’s a pure DIY code that can embed the VNC access to the VM into HTML in any application or webpage.
There is a &lt;a href=&quot;/2019/Access-Virtual-Machines-graphic-console-using-noVNC.html&quot;&gt;noVNC blogpost&lt;/a&gt; detailing how to install noVNC.&lt;/p&gt;

&lt;p&gt;In this animation you can see the feature of connecting to the Virtual Machine with noVNC:
&lt;img src=&quot;/assets/2019-12-19-KubeVirt_UI_options/virtvnc.gif&quot; alt=&quot;noVNC&quot; title=&quot;noVNC&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;From the different options we have investigated, we can conclude that OpenShift Console along with OKD Kubernetes distribution provides a poweful way to manage and control our KubeVirt objects. From the user interface, a developer or operator can do pretty much everything you do in the command line. Additionally, users can create custom reusable templates to deploy their virtual machines with specific requirements. Wizard dialogs are provided as well in order to guide new users during the creation of their VMs.&lt;/p&gt;

&lt;p&gt;OpenShift Console can also be considered as an interesting option in case your KubeVirt installation is running on a native Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;On the other hand, noVNC provides a lightweight interface to simply connect to the console of your virtual machine.&lt;/p&gt;

&lt;p&gt;Octant, although it does not have any specific integration with KubeVirt, looks like a promising Kubernetes user interface that could be extended to manage our KubeVirt instances in the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note: We encourage our readers to let us know of user interfaces that can be used to manage our KubeVirt virtual machines. Then, we can include them in this list.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://octant.dev&quot;&gt;Octant&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.okd.io/&quot;&gt;OKD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/openshift/origin-web-console&quot;&gt;OKD Console&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cockpit-project.org/&quot;&gt;Cockpit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wavezhang/virtVNC/&quot;&gt;virtVNC, noVNC for Kubevirt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alberto Losada Grande, Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">The user interface (UI), in the industrial design field of human–computer interaction, is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators’ decision-making process. Wikipedia:User Interface</summary></entry><entry><title type="html">KubeVirt Laboratory 2, experimenting with CDI</title><link href="https://kubevirt.io//2019/KubeVirt_lab2_experiment_with_cdi.html" rel="alternate" type="text/html" title="KubeVirt Laboratory 2, experimenting with CDI" /><published>2019-12-10T00:00:00+00:00</published><updated>2019-12-10T00:00:00+00:00</updated><id>https://kubevirt.io//2019/KubeVirt_lab2_experiment_with_cdi</id><content type="html" xml:base="https://kubevirt.io//2019/KubeVirt_lab2_experiment_with_cdi.html">&lt;p&gt;In this video, we are showing the step by step of the &lt;a href=&quot;/labs/kubernetes/lab2&quot;&gt;KubeVirt Laboratory 2: Experiment with CDI&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;

&lt;p&gt;In the video, there is a Kubernetes cluster together with KubeVirt already running. If you need help for preparing that setup you can check the &lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;KubeVirt installation notes&lt;/a&gt; or try it yourself in the &lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt&lt;/a&gt; Katacoda scenario.
Also, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; command is already installed and available in the PATH.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/ZHqcHbCxzYM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;The following operations are shown in the video:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Configure the storage for the Virtual Machine&lt;/li&gt;
  &lt;li&gt;Install the CDI Operator and the CR for the importer&lt;/li&gt;
  &lt;li&gt;Create and customize the Fedora Virtual Machine from the cloud image&lt;/li&gt;
  &lt;li&gt;Connect to the console of the Virtual Machine&lt;/li&gt;
  &lt;li&gt;Connect to the Virtual Machine using SSH&lt;/li&gt;
  &lt;li&gt;Redirect a host port to the Virtual Machine to enable external SSH connectivity&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/labs/kubernetes/lab2&quot;&gt;Kubevirt Laboratory 2: Experimenting with CDI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2019/KubeVirt_lab1_use_kubevirt.html&quot;&gt;Kubevirt Laboratory 1 blogpost: Use Kubevirt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/labs/kubernetes/lab1&quot;&gt;Kubevirt Laboratory 1: Use KubeVirt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KC03G60shIc&quot;&gt;KubeVirt basic operations video on youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;Kubevirt installation notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt - Katacoda scenario&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">In this video, we are showing the step by step of the KubeVirt Laboratory 2: Experiment with CDI</summary></entry><entry><title type="html">KubeVirt Laboratory 1, use KubeVirt</title><link href="https://kubevirt.io//2019/KubeVirt_lab1_use_kubevirt.html" rel="alternate" type="text/html" title="KubeVirt Laboratory 1, use KubeVirt" /><published>2019-12-04T00:00:00+00:00</published><updated>2019-12-04T00:00:00+00:00</updated><id>https://kubevirt.io//2019/KubeVirt_lab1_use_kubevirt</id><content type="html" xml:base="https://kubevirt.io//2019/KubeVirt_lab1_use_kubevirt.html">&lt;p&gt;In this video, we are showing the step by step of the &lt;a href=&quot;/labs/kubernetes/lab1&quot;&gt;KubeVirt Laboratory 1: Use KubeVirt&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;

&lt;p&gt;In the video, there is a Kubernetes cluster together with KubeVirt already running. If you need help for preparing that setup you can check the &lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;KubeVirt installation notes&lt;/a&gt; or try it yourself in the &lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt&lt;/a&gt; Katacoda scenario.
Also, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; command is already installed and available in the PATH.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/eQZPCeOs9-c&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;The following operations are shown in the video:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Creating a Virtual Machine from a YAML and a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;containerdisk&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Starting the Virtual Machine&lt;/li&gt;
  &lt;li&gt;Connecting to the Virtual Machine using the console&lt;/li&gt;
  &lt;li&gt;Stopping and removing the Virtual Machine&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/labs/kubernetes/lab1&quot;&gt;Kubevirt Laboratory 1: Use KubeVirt instructions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KC03G60shIc&quot;&gt;KubeVirt basic operations video on youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;Kubevirt installation notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt - Katacoda scenario&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">In this video, we are showing the step by step of the KubeVirt Laboratory 1: Use KubeVirt</summary></entry><entry><title type="html">KubeVirt basic operations video</title><link href="https://kubevirt.io//2019/KubeVirt_basic_operations_video.html" rel="alternate" type="text/html" title="KubeVirt basic operations video" /><published>2019-11-28T00:00:00+00:00</published><updated>2019-11-28T00:00:00+00:00</updated><id>https://kubevirt.io//2019/KubeVirt_basic_operations_video</id><content type="html" xml:base="https://kubevirt.io//2019/KubeVirt_basic_operations_video.html">&lt;p&gt;In this video, we are showing how KubeVirt can be used from a beginner point of view.&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;

&lt;p&gt;In the video, there is a Kubernetes cluster together with KubeVirt already running. If you need help for preparing that setup you can check the &lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;KubeVirt installation notes&lt;/a&gt; or try it yourself in the &lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt&lt;/a&gt; Katacoda scenario.
Also, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; command is already installed and available in the PATH.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/KC03G60shIc&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;The following operations are shown in the video:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Creating a Virtual Machine from a YAML and a cloud image&lt;/li&gt;
  &lt;li&gt;Starting the Virtual Machine&lt;/li&gt;
  &lt;li&gt;Connecting through the console and SSH to the Virtual Machine&lt;/li&gt;
  &lt;li&gt;Connecting to the Virtual Machine through VNC&lt;/li&gt;
  &lt;li&gt;Stopping and removing the Virtual Machine&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KC03G60shIc&quot;&gt;KubeVirt basic operations video on youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/docs/latest/administration/intro.html&quot;&gt;Kubevirt installation notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.katacoda.com/kubevirt/scenarios/kubevirt-101&quot;&gt;First steps with KubeVirt - Katacoda scenario&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><category term="news" /><summary type="html">In this video, we are showing how KubeVirt can be used from a beginner point of view.</summary></entry><entry><title type="html">Jenkins Infra upgrade</title><link href="https://kubevirt.io//2019/jenkins-ci-server-upgrade-and-jobs-for-kubevirt.html" rel="alternate" type="text/html" title="Jenkins Infra upgrade" /><published>2019-11-22T00:00:00+00:00</published><updated>2019-11-22T00:00:00+00:00</updated><id>https://kubevirt.io//2019/jenkins-ci-server-upgrade-and-jobs-for-kubevirt</id><content type="html" xml:base="https://kubevirt.io//2019/jenkins-ci-server-upgrade-and-jobs-for-kubevirt.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the article &lt;a href=&quot;/2019/jenkins-jobs-for-kubevirt-lab-validation.html&quot;&gt;Jenkins Jobs for KubeVirt Lab Validation&lt;/a&gt;, we covered how Jenkins did get the information about the labs and jobs to perform from the KubeVirt repositories.&lt;/p&gt;

&lt;p&gt;In this article, we’ll cover the configuration changes in both Jenkins and the JenkinsFiles required to get our CI setup updated to latest versions and syntax  .&lt;/p&gt;

&lt;h2 id=&quot;jenkins&quot;&gt;Jenkins&lt;/h2&gt;

&lt;p&gt;Our Jenkins instance, is running on top of &lt;a href=&quot;https://console.apps.ci.centos.org:8443/console/&quot;&gt;CentOS OpenShift&lt;/a&gt; and is one of the OS-enhanced Jenkins instances, that provide persistent storage and other pieces bundled, required for non-testing setups.&lt;/p&gt;

&lt;p&gt;What we found is that Jenkins was already complaining because of pending updates (security, engine, etc), but the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;jenkins.war&lt;/code&gt; was embedded in the container image we were using.&lt;/p&gt;

&lt;p&gt;Initial attempts tried to use environment variables to override the WAR to use, but our image was not prepared for it, so we were given the option to just generate a new container for it, but this seemed a bad approach as our image, also contained custom libraries (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;contra-lib&lt;/code&gt;) that enables communicating with OpenShift to run the tests inside containers there.&lt;/p&gt;

&lt;p&gt;During the investigation and testing, we found that the persistent storage folder we were using (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;/var/lib/jenkins&lt;/code&gt;) contained a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;war&lt;/code&gt; folder which contained the unarchived &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;jenkins.war&lt;/code&gt;, so the next attempt was to manually download the latest &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;jenkins.war&lt;/code&gt;, and unzip it on that folder, which finally allowed us to upgrade Jenkins core.&lt;/p&gt;

&lt;h2 id=&quot;the-plugins&quot;&gt;The plugins&lt;/h2&gt;

&lt;p&gt;After upgrading the Jenkins core, we could use the internal plugin manager to upgrade all the remaining plugins, however, that meant a big change in the plugins, configurations, etc.&lt;/p&gt;

&lt;p&gt;After initially being able to run the lab validations for a while, on next morning we wanted to release a new image (from Cloud-Image-Builder) and it failed to build because of the external libraries, and also affected the lab validations again, so we got back to square one for the upgrade process, leaving us with the decision to go forward with the full upgrade: the latest stable jenkins and available plugins and reconfigure what was changed to suit the upgraded requirements.&lt;/p&gt;

&lt;p&gt;Here we’ll show you the configuration settings for each one of the new/updated plugins.&lt;/p&gt;

&lt;h3 id=&quot;openshift-plugin&quot;&gt;OpenShift Plugin&lt;/h3&gt;

&lt;p&gt;Updated to configure the instance of CentOS OpenShift with the system account for accessing it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-44-56.png&quot; alt=&quot;Jenkins OpenShift Client configuration&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;openshift-jenkins-sync&quot;&gt;OpenShift Jenkins Sync&lt;/h3&gt;

&lt;p&gt;Updated and configured to use the console as well with &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubevirt&lt;/code&gt; namespace:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-46-47.png&quot; alt=&quot;Jenkins OpenShift sync configuration&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;global-pipeline-libraries&quot;&gt;Global Pipeline Libraries&lt;/h3&gt;

&lt;p&gt;Here we added the libraries we used, but instead of a specific commit, targetting the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;master&lt;/code&gt; branch.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;contra-lib: &lt;a href=&quot;https://github.com/openshift/contra-lib.git&quot;&gt;https://github.com/openshift/contra-lib.git&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;cico-pipeline-library: &lt;a href=&quot;https://github.com/CentOS/cico-pipeline-library.git&quot;&gt;https://github.com/CentOS/cico-pipeline-library.git&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;contra-library: &lt;a href=&quot;https://github.com/CentOS-PaaS-SIG/contra-env-sample-project&quot;&gt;https://github.com/CentOS-PaaS-SIG/contra-env-sample-project&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For all of them, we ticked:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Load Implicitly&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Allow default version to be overriden&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Include @Library changes in job recent changes&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jenkins replied with the ‘currently maps to revision: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;hash&lt;/code&gt;’ for each one of them, after having loaded them properly, indicating that it was successful.&lt;/p&gt;

&lt;h3 id=&quot;slack-plugin&quot;&gt;Slack plugin&lt;/h3&gt;

&lt;p&gt;In addition to regular plugins used for builds, we incorporated the slack plugin to validate the notifications of build status to a test slack channel. Configuration is really easy, from within Slack, when added the jenkins notifications plugin, a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;token&lt;/code&gt; is provided that must be configured in Jenkins as well as a default &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;room&lt;/code&gt; to send notifications to.&lt;/p&gt;

&lt;p&gt;This allows us to get notified when a new build is started and the resulting status, just in case something was generated with errors or something external changed (remember that we use latest KubeVirt release, latest tools for virtctl, kubectl and a new image is generated out of them to validate the labs).&lt;/p&gt;

&lt;h3 id=&quot;kubernetes-cloud&quot;&gt;Kubernetes ‘Cloud’&lt;/h3&gt;

&lt;p&gt;In addition, Kubernets &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Cloud&lt;/code&gt; was configured pointing to the same console access and using the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubevirt&lt;/code&gt; namespace:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-51-19.png&quot; alt=&quot;OpenShift Cloud definition and URL and tunnel settings&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The libraries we added, automatically add some pod templates for &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;jenkins-contra-slave&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-54-24.png&quot; alt=&quot;Jenkins-contra-slave container configuration&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;other-changes&quot;&gt;Other changes&lt;/h2&gt;

&lt;p&gt;Our environment also used other helper tools as regular OpenShift builds (contra):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-55-41.png&quot; alt=&quot;OpenShift Build repositories and status&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We had to update the repositories from using some older forks (no longer valid and outdated) to use the latest versions, and for the ansible-executor we also created a fork to use the newest libraries for accessing Google Cloud environment and tuning some other variables (&lt;a href=&quot;https://github.com/CentOS-PaaS-SIG/contra-env-infra/pull/59&quot;&gt;https://github.com/CentOS-PaaS-SIG/contra-env-infra/pull/59&lt;/a&gt;) (changes have now landed the upstream repo).&lt;/p&gt;

&lt;p&gt;The issue that we were facing was related with the failure to write temporary files to the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;$HOME&lt;/code&gt; folder for the user so ansible configuration was forced to use one in a temporary and writable folder.&lt;/p&gt;

&lt;p&gt;Additionally, Google Cloud access required updating libraries for authentication that were failing as well, that is fixed via the Dockerfile that generated &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ansible-executor&lt;/code&gt; container image.&lt;/p&gt;

&lt;h2 id=&quot;job-changes&quot;&gt;Job Changes&lt;/h2&gt;

&lt;p&gt;Our Jenkins Jobs were defined (as documented in prior article) inside each repository that made that part easy on one side, but also required some tuning and changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have disabled minikube validation as it was failing for both AWS and GCP unless using baremetal (so we’re wondering about using another approach here)&lt;/li&gt;
  &lt;li&gt;We’ve added code to do the actual ‘Slack’ notification we mentioned above&lt;/li&gt;
  &lt;li&gt;Extend the try/catch block to include a ‘finally’ to send the notifications&lt;/li&gt;
  &lt;li&gt;Change the syntax for ‘artifacts’ as it was previously &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ArchiveArtifacts&lt;/code&gt; and now it’s a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;postBuild&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-outcome&quot;&gt;The outcome&lt;/h2&gt;

&lt;p&gt;After several attempts for fine-tuning the configuration, the builds started succeeding:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-11-02-34.png&quot; alt=&quot;Sunny build status&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, one of the advantages is that builds happen automatically every day or on code changes on the repositories.&lt;/p&gt;

&lt;p&gt;There’s still room for improvement identified that will happen in next iterations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find not needed running instances on Cloud providers for reducing the bills&lt;/li&gt;
  &lt;li&gt;Trigger builds when new releases of KubeVirt happen (out of kubevirt/kubevirt repo)&lt;/li&gt;
  &lt;li&gt;Unify testing on &lt;a href=&quot;/2019/prow-jobs-for-kubevirt.html&quot;&gt;Prow instance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, the builds can be failing for external reasons (like VM in cloud provider taking longer to start up and have SSH available, or the nested VM inside after importing, etc), but still a great help for checking if things are working as they should and of course, to get the validations improved for reduce the number of false positives.&lt;/p&gt;</content><author><name>Pablo Iranzo Gómez</name></author><category term="news" /><summary type="html">Introduction</summary></entry></feed>