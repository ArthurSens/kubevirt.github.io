<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="https://kubevirt.io//feed/news.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2020-07-30T14:20:15+00:00</updated><id>https://kubevirt.io//feed/news.xml</id><title type="html">KubeVirt.io | News</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Minikube KubeVirt addon</title><link href="https://kubevirt.io//2020/Minikube_KubeVirt_Addon.html" rel="alternate" type="text/html" title="Minikube KubeVirt addon" /><published>2020-07-20T00:00:00+00:00</published><updated>2020-07-20T00:00:00+00:00</updated><id>https://kubevirt.io//2020/Minikube_KubeVirt_Addon</id><content type="html" xml:base="https://kubevirt.io//2020/Minikube_KubeVirt_Addon.html">&lt;h2 id=&quot;deploying-kubevirt-has-just-gotten-easier&quot;&gt;Deploying KubeVirt has just gotten easier!&lt;/h2&gt;
&lt;p&gt;With the latest release (v1.12) of
&lt;a href=&quot;https://minikube.sigs.k8s.io/docs/&quot;&gt;minikube&lt;/a&gt; we can now deploy KubeVirt with
a one-liner.&lt;/p&gt;

&lt;h2 id=&quot;deploy-minikube&quot;&gt;Deploy minikube&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Start minikube.  Since my host is Fedora 32 I will use --driver=kvm2 and
  I will also use --container-runtime=crio&lt;br /&gt;
    &lt;code&gt;minikube start --driver=kvm2 --container-runtime=cri-o&lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-07-20-Minikube_KubeVirt_Addon/1.png&quot; width=&quot;115&quot; height=&quot;72&quot; itemprop=&quot;thumbnail&quot; alt=&quot;minikube start&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Check that kubectl client is working correctly&lt;br /&gt;
    &lt;code&gt;kubectl cluster-info&lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-07-20-Minikube_KubeVirt_Addon/2.png&quot; width=&quot;115&quot; height=&quot;11&quot; itemprop=&quot;thumbnail&quot; alt=&quot;kubectl cluster-info&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Enable the minikube kubevirt addon&lt;br /&gt;
    &lt;code&gt;minikube addons enable kubevirt&lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-07-20-Minikube_KubeVirt_Addon/3.png&quot; width=&quot;115&quot; height=&quot;10&quot; itemprop=&quot;thumbnail&quot; alt=&quot;minikube addons enable kubevirt&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Verify KubeVirt components have been deployed to the kubevirt namespace&lt;br /&gt;
    &lt;code&gt;kubectl get ns; kubectl get all -n kubevirt&lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-07-20-Minikube_KubeVirt_Addon/4.png&quot; width=&quot;115&quot; height=&quot;77&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Verify KubeVirt namespace and components&quot; /&gt;
    &lt;/div&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;success&quot;&gt;SUCCESS!&lt;/h3&gt;

&lt;p&gt;From here a user can proceed on to the
&lt;a href=&quot;/labs/kubernetes/lab1&quot;&gt;Kubevirt Laboratory 1: Use KubeVirt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As you can see it is now much easier to deploy KubeVirt in a minikube
Kubernetes environment.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Chris Callegari</name></author><category term="news" /><category term="kubevirt" /><category term="Kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="minikube" /><category term="addons" /><summary type="html">Deploying KubeVirt has just gotten easier! With the latest release (v1.12) of minikube we can now deploy KubeVirt with a one-liner.</summary></entry><entry><title type="html">Common-templates</title><link href="https://kubevirt.io//2020/Common_templates.html" rel="alternate" type="text/html" title="Common-templates" /><published>2020-07-01T00:00:00+00:00</published><updated>2020-07-01T00:00:00+00:00</updated><id>https://kubevirt.io//2020/Common_templates</id><content type="html" xml:base="https://kubevirt.io//2020/Common_templates.html">&lt;h2 id=&quot;what-is-a-virtual-machine-template&quot;&gt;What is a virtual machine template?&lt;/h2&gt;

&lt;p&gt;The KubeVirt project provides a set of templates (https://github.com/kubevirt/common-templates) to create VMS to handle common usage scenarios. These templates provide a combination of some key factors that could be further customized and processed to have a Virtual Machine object. With common templates you can easily start in a few minutes many VMS with predefined hardware resources (e.g. number of CPUs, requested memory, etc.).&lt;/p&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Beware&lt;/p&gt;&lt;p&gt;common templates work only on OpenShift. Kubernetes doesn’t have support for templates.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;what-does-a-vm-template-cover&quot;&gt;What does a VM template cover?&lt;/h2&gt;

&lt;p&gt;The key factors which define a template are&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Guest Operating System (OS) This allows to ensure that the emulated hardware is compatible with the guest OS. Furthermore, it allows to maximize the stability of the VM, and allows performance optimizations. Currently common templates support RHEL 6, 7, 8, Centos 6, 7, 8, Fedora 31 and newer, Windows 10, Windows server 2008, 2012 R2, 2016, 2019. The &lt;a href=&quot;https://docs.ansible.com/ansible/latest/user_guide/playbooks.html&quot;&gt;Ansible playbook&lt;/a&gt; &lt;a href=&quot;https://github.com/kubevirt/common-templates/blob/master/generate-templates.yaml&quot;&gt;generate-templates.yaml&lt;/a&gt; describes all combinations of templates that should be generated.&lt;/li&gt;
  &lt;li&gt;Workload type of most virtual machines should be server or desktop to have maximum flexibility; the highperformance workload trades some of this flexibility (ioThreadsPolicy is set to shared) to provide better performances (e.g. IO threads).&lt;/li&gt;
  &lt;li&gt;Size (flavor) Defines the amount of resources (CPU, memory) to allocate to the VM. There are 4 sizes: tiny (1 core, 1 Gi memory), small (1 core, 2 Gi memory), medium (1 core, 4 Gi memory), large (2 cores, 8 Gi memory). If these predefined sizes don’t suit you, you can create a new template based on common templates via UI (choose &lt;code class=&quot;highlighter-rouge&quot;&gt;Workloads&lt;/code&gt; in the left panel » press &lt;code class=&quot;highlighter-rouge&quot;&gt;Virtualization&lt;/code&gt; » press &lt;code class=&quot;highlighter-rouge&quot;&gt;Virtual Machine Templates&lt;/code&gt; » press &lt;code class=&quot;highlighter-rouge&quot;&gt;Create Virtual Machine Template&lt;/code&gt; blue button) or CLI (update yaml template and create new template).&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;zoom&quot;&gt;
  &lt;img src=&quot;/assets/2020-07-01-Common_templates/create_template.jpg&quot; width=&quot;100&quot; height=&quot;60&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Create new template&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;accessing-the-virtual-machine-templates&quot;&gt;Accessing the virtual machine templates&lt;/h2&gt;
&lt;p&gt;If you installed KubeVirt using a &lt;a href=&quot;https://github.com/kubevirt/hyperconverged-cluster-operator&quot;&gt;supported method&lt;/a&gt;, you should find the common templates preinstalled in the cluster. If you want to upgrade the templates, or install them from scratch, you can use one of the &lt;a href=&quot;https://github.com/kubevirt/common-templates/releases&quot;&gt;supported releases&lt;/a&gt;
There are two ways to install and configure templates:&lt;/p&gt;

&lt;h2 id=&quot;via-cli&quot;&gt;Via CLI:&lt;/h2&gt;

&lt;h6 id=&quot;to-install-the-templates&quot;&gt;To install the templates:&lt;/h6&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ export VERSION=&quot;v0.11.2&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ oc create -f https://github.com/kubevirt/common-templates/releases/download/$VERSION/common-templates-$VERSION.yaml&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&quot;to-create-vm-from-template&quot;&gt;To create VM from template:&lt;/h6&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ oc process rhel8-server-tiny PVCNAME=mydisk NAME=rheltinyvm | oc apply -f -&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&quot;to-start-vm-from-created-object&quot;&gt;To start VM from created object:&lt;/h6&gt;
&lt;p&gt;The created object is now a regular VirtualMachine object and from now it can be controlled by accessing Kubernetes API resources. The preferred way to do this is to use virtctl tool.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ virtctl start rheltinyvm&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;An alternative way to start the VM is with the oc patch command. Example:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ oc patch virtualmachine rheltinyvm --type merge -p '{&quot;spec&quot;:{&quot;running&quot;:true}}'&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;As soon as VM starts, openshift creates a new type of object - &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstance&lt;/code&gt;. It has a similar name to VirtualMachine.&lt;/p&gt;

&lt;h2 id=&quot;via-ui&quot;&gt;Via UI:&lt;/h2&gt;
&lt;p&gt;The Kubevirt project has an official plugin in OpenShift Cluster Console Web UI. This UI supports the creation of VMS using templates and template features - flavors and workload profiles.&lt;/p&gt;

&lt;h6 id=&quot;to-install-the-templates-1&quot;&gt;To install the templates:&lt;/h6&gt;

&lt;p&gt;Install OpenShift virtualization operator from &lt;code class=&quot;highlighter-rouge&quot;&gt;Operators&lt;/code&gt; &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;OperatorHub&lt;/code&gt;. The operator-based deployment takes care of installing various components, including the common templates.&lt;/p&gt;

&lt;div class=&quot;zoom&quot;&gt;
  &lt;img src=&quot;/assets/2020-07-01-Common_templates/operator.jpg&quot; width=&quot;100&quot; height=&quot;60&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Install operator&quot; /&gt;
&lt;/div&gt;

&lt;h6 id=&quot;to-create-vm-from-template-1&quot;&gt;To create VM from template:&lt;/h6&gt;
&lt;p&gt;To create a VM from a template, choose &lt;code class=&quot;highlighter-rouge&quot;&gt;Workloads&lt;/code&gt; in the left panel » press &lt;code class=&quot;highlighter-rouge&quot;&gt;Virtualization&lt;/code&gt; » press &lt;code class=&quot;highlighter-rouge&quot;&gt;Create Virtual Machine&lt;/code&gt; blue button » choose &lt;code class=&quot;highlighter-rouge&quot;&gt;New with Wizard&lt;/code&gt;. Next, you have to see &lt;code class=&quot;highlighter-rouge&quot;&gt;Create Virtual Machine&lt;/code&gt; window&lt;/p&gt;

&lt;div class=&quot;zoom&quot;&gt;
  &lt;img src=&quot;/assets/2020-07-01-Common_templates/create_vm.jpg&quot; width=&quot;100&quot; height=&quot;60&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Create vm from template&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This wizard leads you through the basic setup of vm (like guest operating system, workload, flavor, …). After vm is created you can start requested vm.&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;after the generation step (UI and CLI), VM objects and template objects have no relationship with each other besides the &lt;code class=&quot;highlighter-rouge&quot;&gt;vm.kubevirt.io/template: rhel8-server-tiny-v0.10.0&lt;/code&gt; label. This means that changes in templates do not automatically affect VMS, or vice versa.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;</content><author><name>Karel Simon</name></author><category term="news" /><category term="kubevirt" /><category term="Kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="common-templates" /><summary type="html">What is a virtual machine template?</summary></entry><entry><title type="html">Migrate a sample Windows workload to Kubernetes using KubeVirt and CDI</title><link href="https://kubevirt.io//2020/win_workload_in_k8s.html" rel="alternate" type="text/html" title="Migrate a sample Windows workload to Kubernetes using KubeVirt and CDI" /><published>2020-06-22T00:00:00+00:00</published><updated>2020-06-22T00:00:00+00:00</updated><id>https://kubevirt.io//2020/win_workload_in_k8s</id><content type="html" xml:base="https://kubevirt.io//2020/win_workload_in_k8s.html">&lt;p&gt;The goal of this blog is to demonstrate that a web service can continue to run
after a Windows guest virtual machine providing the service is migrated from
MS Windows and Oracle VirtualBox to a guest virtual machine orchestrated by
Kubernetes and KubeVirt on a Fedora Linux host.  Yes!  It can be done!&lt;/p&gt;

&lt;h3 id=&quot;source-details&quot;&gt;Source details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Host platform: Windows 2019 Datacenter&lt;/li&gt;
  &lt;li&gt;Virtualization platform: Oracle VirtualBox 6.1&lt;/li&gt;
  &lt;li&gt;Guest platform: Windows 2019 Datacenter (guest to be migrated)
&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;
&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;
&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Guest application: My favorite dotnet application
&lt;a href=&quot;https://jellyfin.org/&quot;&gt;Jellyfin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;target-details&quot;&gt;Target details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Host platform: Fedora 32 with latest updates applied&lt;/li&gt;
  &lt;li&gt;Kubernetes cluster created&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/quickstart_minikube/&quot;&gt;KubeVirt&lt;/a&gt; and &lt;a href=&quot;https://kubevirt.io/user-guide/#/installation/image-upload&quot;&gt;CDI&lt;/a&gt; installed in the Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;procedure&quot;&gt;Procedure&lt;/h2&gt;

&lt;h3 id=&quot;tasks-to-performed-on-source-host&quot;&gt;Tasks to performed on source host&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Before we begin let's take a moment to ensure the service is running and
  web browser accessible&lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/1-1.png&quot; width=&quot;100&quot; height=&quot;60&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Ensure application service is running&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/1-2.png&quot; width=&quot;100&quot; height=&quot;60&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Confirm web browser access&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Power down the guest virtual machine to ensure all changes to the
  filesystem are quiesced to disk.&lt;br /&gt;
    &lt;code&gt;VBoxManage.exe controlvm testvm poweroff&lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/1-3.png&quot; width=&quot;115&quot; height=&quot;20&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Power down the guest virtual machine&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Upload the guest virtual machine disk image to the Kubernetes cluster
  and a target DataVolume called testvm
    &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;
      &lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;
    &lt;/sup&gt;
    &lt;br /&gt;
    &lt;code&gt;
      virtctl.exe image-upload dv testvm
      --size=14Gi
      --image-path=&quot;C:\Users\Administrator\VirtualBox VMs\testvm\testvm.vdi&quot;
    &lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/1-4.png&quot; width=&quot;100&quot; height=&quot;60&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Upload disk image&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Verify the PersistentVolumeClaim created via the DataVolume
  image upload in the previous step&lt;br /&gt;
    &lt;code&gt;
      kubectl describe pvc/testvm
    &lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/2-1.png&quot; width=&quot;125&quot; height=&quot;75&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Verify PersistentVolumeClaim&quot; /&gt;
    &lt;/div&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Create a guest virtual machine definition that references the
  DataVolume containing our guest virtual machine disk image&lt;br /&gt;
    &lt;code&gt;kubectl create -f vm_testvm.yaml&lt;/code&gt;
    &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;
      &lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;
    &lt;/sup&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/2-2.png&quot; width=&quot;125&quot; height=&quot;75&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Create the guest virtual machine&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Expose the Jellyfin service in Kubernetes via a NodePort type
  service&lt;br /&gt;
    &lt;code&gt;
      kubectl create -f service_jellyfin.yaml
    &lt;/code&gt;
    &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;
      &lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;
    &lt;/sup&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/2-3.png&quot; width=&quot;100&quot; height=&quot;75&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Create NodePort service&quot; /&gt;
    &lt;/div&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;&lt;li&gt;Let's verify the running guest virtual machine by using the virtctl
  command to open a vnc session to the MS Window console.  While we are here
  let's also open a web browser and confirm web browser access to the
  application.&lt;br /&gt;
    &lt;code&gt;virtctl vnc testvm&lt;/code&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/2-4.png&quot; width=&quot;125&quot; height=&quot;70&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Verify running guest virtual machine&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/2-5.png&quot; width=&quot;125&quot; height=&quot;70&quot; itemprop=&quot;thumbnail&quot; alt=&quot;Web browser access to application&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;task-to-performed-on-user-workstation&quot;&gt;Task to performed on user workstation&lt;/h3&gt;

&lt;ol&gt;
  And finally let's confirm web browser access via the Kubernetes service url.&lt;br /&gt;
    &lt;div class=&quot;zoom&quot;&gt;
      &lt;img src=&quot;/assets/2020-06-22-win_workload_in_k8s/2-6.png&quot; width=&quot;125&quot; height=&quot;70&quot; alt=&quot;Web browser access to Kubernetes service&quot; /&gt;
    &lt;/div&gt;
    &lt;br /&gt;&lt;br /&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;success&quot;&gt;SUCCESS!&lt;/h3&gt;

&lt;p&gt;Here we have successfully demonstrated how simple it can be to migrate an
existing MS Windows platform and application to Kubernetes control. For
questions feel free to join the conversation via one of the project forums.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h5&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-noteref&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-noteref&quot;&gt;
      Fedora virtio drivers need to be installed on Windows hosts or virtual
      machines that will be migrated into a Kubernetes environment. Drivers can
      be found
      &lt;a href=&quot;https://docs.fedoraproject.org/en-US/quick-docs/creating-windows-virtual-machines-using-virtio-drivers/&quot;&gt;
        here
      &lt;/a&gt;.
      &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-noteref&quot;&gt;&amp;#8617;&lt;/a&gt;
    &lt;/li&gt;&lt;li id=&quot;fn:2&quot; role=&quot;doc-noteref&quot;&gt;
      Please note:
      &lt;br /&gt;
      &amp;#8226; Users without certificate authority trusted certificates added to
      the kubernetes api and cdi cdi-proxyuploader secret will require the
      &lt;code&gt;--insecure&lt;/code&gt; arg.
      &lt;br /&gt;
      &amp;#8226; Users without the uploadProxyURLOverride patch to the cdi
      cdiconfig.cdi.kubevirt.io/config crd will require the
      &lt;code&gt;--uploadProxyURL&lt;/code&gt; arg.
      &lt;br /&gt;
      &amp;#8226; Users need a correctly configured $HOME/.kube/config along with
      client authentication certificate.
      &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-noteref&quot;&gt;&amp;#8617;&lt;/a&gt;
    &lt;/li&gt;&lt;li id=&quot;fn:3&quot; role=&quot;doc-noteref&quot;&gt;
      &lt;a href=&quot;/assets/2020-06-22-win_workload_in_k8s/vm_testvm.yaml&quot;&gt;
        vm_testvm.yaml
      &lt;/a&gt;: Virtual machine manifest
      &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-noteref&quot;&gt;&amp;#8617;&lt;/a&gt;
    &lt;/li&gt;&lt;li id=&quot;fn:4&quot; role=&quot;doc-noteref&quot;&gt;
      &lt;a href=&quot;/assets/2020-06-22-win_workload_in_k8s/service_jellyfin.yaml&quot;&gt;
        service_jellyfin.yaml
      &lt;/a&gt;: Service manifest
      &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-noteref&quot;&gt;&amp;#8617;&lt;/a&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Chris Callegari</name></author><category term="news" /><category term="kubevirt" /><category term="Kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="images" /><category term="storage" /><category term="windows" /><summary type="html">The goal of this blog is to demonstrate that a web service can continue to run after a Windows guest virtual machine providing the service is migrated from MS Windows and Oracle VirtualBox to a guest virtual machine orchestrated by Kubernetes and KubeVirt on a Fedora Linux host. Yes! It can be done!</summary></entry><entry><title type="html">SELinux, from basics to KubeVirt</title><link href="https://kubevirt.io//2020/SELinux-from-basics-to-KubeVirt.html" rel="alternate" type="text/html" title="SELinux, from basics to KubeVirt" /><published>2020-05-25T00:00:00+00:00</published><updated>2020-05-25T00:00:00+00:00</updated><id>https://kubevirt.io//2020/SELinux-from-basics-to-KubeVirt</id><content type="html" xml:base="https://kubevirt.io//2020/SELinux-from-basics-to-KubeVirt.html">&lt;p&gt;SELinux is one of many security mechanisms leveraged by KubeVirt.&lt;br /&gt;
For an overview of KubeVirt security, please first read &lt;a href=&quot;/2020/KubeVirt-Security-Fundamentals.html&quot;&gt;this excellent article&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;selinux-101&quot;&gt;SELinux 101&lt;/h2&gt;

&lt;p&gt;At its core, SELinux is a whitelist-based security policy system intended to limit interactions between Linux processes and files. Simplified, it can be visualized as a “syscall firewall”.&lt;/p&gt;

&lt;p&gt;Policies are based on statically defined types, that can be assigned to files, processes and other objects.&lt;/p&gt;

&lt;p&gt;A simple policy example would be to allow a &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/test&lt;/code&gt; program to read its &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/test.conf&lt;/code&gt; configuration file.&lt;/p&gt;

&lt;p&gt;The policy for that would include directives to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Assign types to files and processes, like &lt;code class=&quot;highlighter-rouge&quot;&gt;test_bin_t&lt;/code&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/test&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;test_conf_t&lt;/code&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/test.conf&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;test_t&lt;/code&gt; for instances of the test program&lt;/li&gt;
  &lt;li&gt;Configure a &lt;em&gt;transition&lt;/em&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;test_bin_t&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;test_t&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Allow &lt;code class=&quot;highlighter-rouge&quot;&gt;test_t&lt;/code&gt; processes to read &lt;code class=&quot;highlighter-rouge&quot;&gt;test_conf_t&lt;/code&gt; files.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-selinux-standard-reference-policy&quot;&gt;The SELinux standard Reference Policy&lt;/h2&gt;

&lt;p&gt;Since SELinux policies are whitelists, a setup running with the above policy would not be allowed to do anything, except for that test program.&lt;/p&gt;

&lt;p&gt;A policy for an entire Linux distribution as seen in the wild is made of millions of lines, which wouldn’t be practical to write and maintain on a per-distribution basis.&lt;/p&gt;

&lt;p&gt;That is why the &lt;a href=&quot;https://github.com/SELinuxProject/refpolicy&quot;&gt;Reference Policy&lt;/a&gt; (refpolicy) was written. The refpolicy implements various mechanisms to simplify policy writing, but also contains modules for most core Linux applications.&lt;/p&gt;

&lt;p&gt;Most use-cases can be addressed with the “standard” refpolicy, plus optionally some custom modules for specific applications not covered by the Reference Policy.&lt;/p&gt;

&lt;p&gt;Limitations start to arise for use-cases that run the same binary multiple times concurrently, and expect instances to be isolated from each other. Virtualization is one of those use cases. Indeed if 2 virtual machines are running on the same system, it is usually desirable that one VM can’t see the resources of the other one.&lt;/p&gt;

&lt;p&gt;As an example, if qemu processes are labeled &lt;code class=&quot;highlighter-rouge&quot;&gt;qemu_t&lt;/code&gt; and disk files are labeled &lt;code class=&quot;highlighter-rouge&quot;&gt;qemu_disk_t&lt;/code&gt;, allowing &lt;code class=&quot;highlighter-rouge&quot;&gt;qemu_t&lt;/code&gt; to read/write &lt;code class=&quot;highlighter-rouge&quot;&gt;qemu_disk_t&lt;/code&gt; files would allow all qemu processes to access all disk files.&lt;/p&gt;

&lt;p&gt;Another mechanism is necessary to provide VM isolation. That is what SELinux MCS addresses.&lt;/p&gt;

&lt;h2 id=&quot;selinux-multi-category-security-mcs&quot;&gt;SELinux Multi-Category Security (MCS)&lt;/h2&gt;

&lt;p&gt;Multi-Category Security, or MCS, provides the ability to dynamically add numerical IDs (called categories) to any SELinux type on any object (file/process/socket/…).&lt;/p&gt;

&lt;p&gt;Categories range from 0 to 1023. Since only 1024 unique IDs would be quite limiting, most virtualization-related applications combine 2 categories, which add up to about 500,000 combinations. It’s important to note that categories have no order, so &lt;code class=&quot;highlighter-rouge&quot;&gt;c42,c42&lt;/code&gt; is equivalent to &lt;code class=&quot;highlighter-rouge&quot;&gt;c42&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;c1,c2&lt;/code&gt; is equivalent to &lt;code class=&quot;highlighter-rouge&quot;&gt;c2,c1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In the example above, we can now:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dynamically compute a unique random category for each VM&lt;/li&gt;
  &lt;li&gt;Assign the corresponding categories to all VM resources, like qemu instance and disk files&lt;/li&gt;
  &lt;li&gt;Only allow access when all the involved resources have the same category number.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And that is exactly what libvirt does when compiled with SELinux support, as shown in the diagram below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-05-25-SELinux-from-basics-to-KubeVirt/libvirt.svg&quot; alt=&quot;Components View&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: MCS can do a lot more, this article only describes the bits that are used by libvirt and kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;mcs-and-containers&quot;&gt;MCS and containers&lt;/h3&gt;

&lt;p&gt;Another application that leverages MCS is Linux containers.&lt;/p&gt;

&lt;p&gt;In fact, containers use very few SELinux types and rely mostly on MCS to provide container isolation. For example, all the files and processes in container filesystems have the same SELinux types. For a non-super-privileged container, those types are usually &lt;code class=&quot;highlighter-rouge&quot;&gt;container_file_t&lt;/code&gt; for file and &lt;code class=&quot;highlighter-rouge&quot;&gt;container_t&lt;/code&gt; for processes. Most operations are permitted within those types, and the categories are really what matters.&lt;/p&gt;

&lt;p&gt;As with libvirt, categories have to match for access to be granted, effectively blocking inter-container communication.&lt;/p&gt;

&lt;p&gt;Super-privileged containers however are exempt from categories. They use the &lt;code class=&quot;highlighter-rouge&quot;&gt;spc_t&lt;/code&gt; SELinux type, which allows them to do pretty much anything, at least as far as SELinux is concerned.&lt;/p&gt;

&lt;p&gt;That is all defined as an SELinux module in the &lt;a href=&quot;https://github.com/containers/container-selinux&quot;&gt;container-selinux Github repository&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;mcs-and-container-orchestrators&quot;&gt;MCS and container orchestrators&lt;/h3&gt;

&lt;p&gt;Container orchestrators add a level of management. They define pods of containers, and within a pod, cross-container communication is acceptable and often even necessary.&lt;/p&gt;

&lt;p&gt;Categories are therefore managed at the pod level, and all the containers that belong to the same pod are assigned the same categories, as illustrated by the following diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-05-25-SELinux-from-basics-to-KubeVirt/kubernetes.svg&quot; alt=&quot;Components View&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;selinux-in-kubevirt&quot;&gt;SELinux in Kubevirt&lt;/h2&gt;

&lt;p&gt;Finally getting to KubeVirt, which relies on all of the above, as it runs libvirt in a container managed by a container orchestrator on SELinux-enabled systems.&lt;/p&gt;

&lt;p&gt;In that context, libvirt runs inside a regular container and can’t manage SELinux object like types and categories. However, MCS isolation is provided by the container orchestrator, and every VM runs in its own pod (virt-launcher). And since no 2 virt-launcher pods will ever have the same categories on a given node, SELinux isolation of VMs is guaranteed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-05-25-SELinux-from-basics-to-KubeVirt/kubevirt.svg&quot; alt=&quot;Components View&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: As some host configuration is usually required for VMs to run, each node also runs a super-privileged pod (virt-handler), dedicated to such operations.&lt;/p&gt;</content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="design" /><category term="architecture" /><category term="security" /><category term="libvirt" /><category term="qemu" /><summary type="html">SELinux is one of many security mechanisms leveraged by KubeVirt. For an overview of KubeVirt security, please first read this excellent article.</summary></entry><entry><title type="html">KubeVirt VM Image Usage Patterns</title><link href="https://kubevirt.io//2020/KubeVirt-VM-Image-Usage-Patterns.html" rel="alternate" type="text/html" title="KubeVirt VM Image Usage Patterns" /><published>2020-05-12T00:00:00+00:00</published><updated>2020-05-12T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt-VM-Image-Usage-Patterns</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt-VM-Image-Usage-Patterns.html">&lt;h1 id=&quot;building-a-vm-image-repository&quot;&gt;Building a VM Image Repository&lt;/h1&gt;

&lt;p&gt;You know what I hear a lot from new KubeVirt users?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“How do I manage VM images with KubeVirt? There’s a million options and I have no idea where to start.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And I agree. It’s not obvious. There are a million ways to use and manipulate VM images with KubeVirt. That’s by design. KubeVirt is meant to be as flexible as possible, but in the process I think we dropped the ball on creating some well defined workflows people can use as a starting point.&lt;/p&gt;

&lt;p&gt;So, that’s what I’m going to attempt to do. I’ll show you how to make your images accessible in the cluster. I’ll show you how to make a custom VM image repository for use within the cluster. And I’ll show you how to use this at scale using the same patterns you may have used in AWS or GCP.&lt;/p&gt;

&lt;p&gt;The pattern we’ll use here is…&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Import a base VM image into the cluster as an PVC&lt;/li&gt;
  &lt;li&gt;Use KubeVirt to create a new immutable custom image with application assets&lt;/li&gt;
  &lt;li&gt;Scale out as many VMIs as we’d like using the pre-provisioned immutable custom image.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Remember, this isn’t “the definitive” way of managing VM images in KubeVirt. This is just an example workflow to help people get started.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;importing-a-base-image&quot;&gt;Importing a Base Image&lt;/h2&gt;

&lt;p&gt;Let’s start with importing a base image into a PVC.&lt;/p&gt;

&lt;p&gt;For our purposes in this workflow, the base image is meant to be immutable. No VM will use this image directly, instead VMs spawn with their own unique copy of this base image. Think of this just like you would containers. A container image is immutable, and a running container instance is using a copy of an image instead of the image itself.&lt;/p&gt;

&lt;h3 id=&quot;step-0-install-kubevirt-with-cdi&quot;&gt;Step 0. Install KubeVirt with CDI&lt;/h3&gt;

&lt;p&gt;I’m not covering this. Use our documentation linked to below. Understand that CDI (containerized data importer) is the tool we’ll be using to help populate and manage PVCs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/#/installation/installation&quot;&gt;Installing KubeVirt&lt;/a&gt;
&lt;a href=&quot;https://kubevirt.io/user-guide/#/installation/image-upload?id=install-cdi&quot;&gt;Installing CDI&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-1-create-a-namespace-for-our-immutable-vm-images&quot;&gt;Step 1. Create a namespace for our immutable VM images.&lt;/h3&gt;

&lt;p&gt;We’ll give users the ability to clone VM images living on PVCs from this namespace to their own namespace, but not directly create VMIs within this namespace.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create namespace vm-images
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-import-your-image-to-a-pvc-in-the-image-namespace&quot;&gt;Step 2. Import your image to a PVC in the image namespace&lt;/h3&gt;

&lt;p&gt;Below are a few options for importing. For each example, I’m using the Fedora Cloud qcow2 image that can be downloaded &lt;a href=&quot;https://download.fedoraproject.org/pub/fedora/linux/releases/31/Cloud/x86_64/images/Fedora-Cloud-Base-31-1.9.x86_64.qcow2&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you try these examples yourself, you’ll need to download the &lt;strong&gt;Fedora-Cloud-Base-31-1.9.x86_64.qcow2&lt;/strong&gt; image file in your working directory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example: Import a local VM from your desktop environment using virtctl&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you don’t have ingress setup for the cdi-uploadproxy service endpoint (which you don’t if you’re reading this) we can set up a local port forward using kubectl. That gives a route into the cluster to upload the image. Leave the command below executing to open the port.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl port-forward &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; cdi service/cdi-uploadproxy 18443:443
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In a separate terminal upload the image over the port forward connection using the virtctl tool. Note that the size of the PVC must be the size of what the qcow image will expand to when converted to a raw image. In this case I chose 5 gigabytes as the PVC size.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtctl image-upload dv fedora-cloud-base-31 &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; vm-images  &lt;span class=&quot;nt&quot;&gt;--size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5Gi &lt;span class=&quot;nt&quot;&gt;--image-path&lt;/span&gt; Fedora-Cloud-Base-31-1.9.x86_64.qcow2  &lt;span class=&quot;nt&quot;&gt;--uploadproxy-url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://127.0.0.1:18443 &lt;span class=&quot;nt&quot;&gt;--insecure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once that completes, you’ll have a PVC in the vm-images namespace that contains the Fedora Cloud image.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pvc &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; vm-images
NAME               STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
fedora-cloud-base-31   Bound    local-pv-e824538e   5Gi       RWO            &lt;span class=&quot;nb&quot;&gt;local          &lt;/span&gt;60s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Example: Import using a container registry&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If the image’s footprint is small like our Fedora Cloud Base qcow image, then it probably makes sense to use a container image registry to import our image from a container image to a PVC.&lt;/p&gt;

&lt;p&gt;In the example below, I start by building a container image with the Fedora Cloud Base qcow VM image in it, and push that container image to my container registry.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;END&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; &amp;gt; Dockerfile
FROM scratch
ADD Fedora-Cloud-Base-31-1.9.x86_64.qcow2 /disk/
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;END
&lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; quay.io/dvossel/fedora:cloud-base-31 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
docker push quay.io/dvossel/fedora:cloud-base-31
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next a CDI DataVolume is used to import the VM image into a new PVC from the container image you just uploaded to your container registry. Posting the DataVolume manifest below will result in a new 5 gigabyte PVC being created and the VM image being placed on that PVC in a way KubeVirt can consume it.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;END&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; &amp;gt; fedora-cloud-base-31-datavolume.yaml
apiVersion: cdi.kubevirt.io/v1alpha1
kind: DataVolume
metadata:
  name: fedora-cloud-base-31
  namespace: vm-images
spec:
  source:
    registry:
      url: &quot;docker://quay.io/dvossel/fedora:cloud-base-31&quot;
  pvc:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 5Gi
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;END
&lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; fedora-cloud-base-31-datavolume.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can observe the CDI complete the import by watching the DataVolume object.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl describe datavolume fedora-cloud-base-31 &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; vm-images
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
Status:
  Phase:     Succeeded
  Progress:  100.0%
Events:
  Type    Reason            Age                   From                   Message
  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;    &lt;span class=&quot;nt&quot;&gt;------&lt;/span&gt;            &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                   &lt;span class=&quot;nt&quot;&gt;-------&lt;/span&gt;
  Normal  ImportScheduled   2m49s                 datavolume-controller  Import into fedora-cloud-base-31 scheduled
  Normal  ImportInProgress  2m46s                 datavolume-controller  Import into fedora-cloud-base-31 &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;progress
  Normal  Synced            40s &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;x11 over 2m51s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;  datavolume-controller  DataVolume synced successfully
  Normal  ImportSucceeded   40s                   datavolume-controller  Successfully imported into PVC fedora-cloud-base-31
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the import is complete, you’ll see the image available as a PVC in your vm-images namespace. The PVC will have the same name given to the DataVolume.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pvc &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; vm-images
NAME                   STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
fedora-cloud-base-31   Bound    local-pv-e824538e   5Gi       RWO            &lt;span class=&quot;nb&quot;&gt;local          &lt;/span&gt;60s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Example: Import an image from an http or s3 endpoint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While I’m not going to provide a detailed example here, another option for importing VM images into a PVC is to host the image on an http server (or as an s3 object) and then use a DataVolume to import the VM image into the PVC from a URL.&lt;/p&gt;

&lt;p&gt;Replace the url in this example with one hosting the qcow2 image. More information about this import method can be found &lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer/blob/master/doc/datavolumes.md#https3registry-source&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: DataVolume
metadata:
  name: fedora-cloud-base-31
  namespace: vm-images
spec:
  &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;:
    http:
      url: http://your-web-server-here/images/Fedora-Cloud-Base-31-1.9.x86_64.qcow2
  pvc:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;provisioning-new-custom-vm-image&quot;&gt;Provisioning New Custom VM Image&lt;/h2&gt;

&lt;p&gt;The base image itself isn’t that useful to us. Typically what we really want is an immutable VM image preloaded with all our application related assets. This way when the VM boots up, it already has everything it needs pre-provisioned. The pattern we’ll use here is to provision the VM image once, and then use clones of the pre-provisioned VM image as many times as we’d like.&lt;/p&gt;

&lt;p&gt;For this example, I want a new immutable VM image preloaded with an nginx webserver. We can actually describe this entire process of creating this new VM image using the single VM manifest below. Note that I’m starting the VM inside the vm-images namespace. This is because I want the resulting VM image’s cloned PVC to remain in our vm-images repository namespace.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachine&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io/vm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-provisioner&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-provisioner&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vm-images&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;runStrategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;RerunOnFailure&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io/vm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-provisioner&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;disks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;datavolumedisk1&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;machine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;terminationGracePeriodSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;dataVolume&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fedora-31-nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;datavolumedisk1&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;cloudInitNoCloud&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;userData&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;#!/bin/sh&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;yum install -y nginx&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;systemctl enable nginx&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;# removing instances ensures cloud init will execute again after reboot&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;rm -rf /var/lib/cloud/instances&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;shutdown now&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;dataVolumeTemplates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fedora-31-nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5Gi&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vm-images&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fedora-cloud-base-31&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are a few key takeaways from this manifest worth discussing.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Usage of &lt;strong&gt;runStrategy: “RerunOnFailure”&lt;/strong&gt;. This tells KubeVirt to treat the VM’s execution similar to a Kubernetes Job. We want the VM to continue retrying until the VM guest shuts itself down gracefully.&lt;/li&gt;
  &lt;li&gt;Usage of the &lt;strong&gt;cloudInitNoCloud volume&lt;/strong&gt;. This volume allows us to inject a script into the VM’s startup procedure. In our case, we want this script to install nginx, configure nginx to launch on startup, and then immediately shutdown the guest gracefully once that is complete.&lt;/li&gt;
  &lt;li&gt;Usage of the &lt;strong&gt;dataVolumeTemplates section&lt;/strong&gt;. This allows us to define a new PVC which is a clone of our fedora-cloud-base-31 base image. The resulting VM image attached to our VM will be a new image pre-populated with nginx.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After posting the VM manifest to the cluster, wait for the corresponding VMI to reach the Succeeded phase.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get vmi &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; vm-images
NAME                AGE     PHASE       IP            NODENAME
nginx-provisioner   2m26s   Succeeded   10.244.0.22   node01
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tells us the VM successfully executed the cloud-init script which installed nginx and shut down the guest gracefully. A VMI that never shuts down or repeatedly fails means something is wrong with the provisioning.&lt;/p&gt;

&lt;p&gt;All that’s left now is to delete the VM and leave the resulting PVC behind as our immutable artifact. We do this by deleting the VM using the –cascade=false option. This tells Kubernetes to delete the VM, but leave behind anything owned by the VM. In this case we’ll be leaving behind the PVC that has nginx provisioned on it.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl delete vm nginx-provisioner &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; vm-images &lt;span class=&quot;nt&quot;&gt;--cascade&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After deleting the VM, you can see the nginx provisioned PVC in your vm-images namespace.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pvc &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; vm-images
NAME               STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
fedora-cloud-base-31   Bound    local-pv-e824538e   5Gi       RWO            &lt;span class=&quot;nb&quot;&gt;local          &lt;/span&gt;60s
fedora-31-nginx            Bound    local-pv-8dla23ds    5Gi       RWO            &lt;span class=&quot;nb&quot;&gt;local          &lt;/span&gt;60s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;understanding-the-vm-image-repository&quot;&gt;Understanding the VM Image Repository&lt;/h2&gt;
&lt;p&gt;At this point we have a namespace, vm-images, that contains PVCs with our VM images on them. Those PVCs represent VM images in the same way AWS’s AMIs represent VM images and this &lt;strong&gt;vm-images namespace is our VM image repository.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using CDI’s i&lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer/blob/master/doc/clone-datavolume.md#how-to-clone-an-image-from-one-dv-to-another-one&quot;&gt;cross namespace cloning feature&lt;/a&gt;, VM’s can now be launched across multiple namespaces throughout the entire cluster using the PVCs in this “repository”. Note that non-admin users need a special RBAC role to allow for this cross namespace PVC cloning. Any non-admin user who needs the ability to access the vm-images namespace for PVC cloning will need the RBAC permissions outlined &lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer/blob/master/doc/RBAC.md#pvc-cloning&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is an example of the RBAC necessary to enable cross namespace cloning from the vm-images namespace to the default namespace using the default service account.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cdi-cloner&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cdi.kubevirt.io&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;datavolumes/source&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RoleBinding&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default-cdi-cloner&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vm-images&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ServiceAccount&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;roleRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cdi-cloner&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;horizontally-scaling-vms-using-custom-image&quot;&gt;Horizontally Scaling VMs Using Custom Image&lt;/h1&gt;

&lt;p&gt;Now that we have our immutable custom VM image, we can create as many VMs as we want using that custom image.&lt;/p&gt;

&lt;h2 id=&quot;example-scale-out-vmi-instances-using-the-custom-vm-image&quot;&gt;Example: Scale out VMI instances using the custom VM image.&lt;/h2&gt;

&lt;p&gt;Clone the custom VM image from the vm-images namespace into the namespace the VMI instances will be running in as a &lt;strong&gt;ReadOnlyMany&lt;/strong&gt; PVC. This will allow concurrent access to a single PVC.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cdi.kubevirt.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DataVolume&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-rom&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vm-images&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fedora-31-nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadOnlyMany&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, create a VirtualMachineInstanceReplicaSet that references the nginx-rom PVC as an ephemeral volume. With an ephemeral volume, KubeVirt will mount the PVC read only, and use a cow (copy on write) &lt;a href=&quot;https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=ephemeral&quot;&gt;ephemeral volume&lt;/a&gt; on local storage to back each individual VMI. This ephemeral data’s life cycle is limited to the life cycle of each VMI.&lt;/p&gt;

&lt;p&gt;Here’s an example manifest of a VirtualMachineInstanceReplicaSet starting 5 instances of our nginx server in separate VMIs.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io/vmReplicaSet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io/vmReplicaSet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;disks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-image&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;machine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;terminationGracePeriodSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ephemeral&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-image&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;persistentVolumeClaim&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;claimName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-rom&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;cloudInitNoCloud&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;userData&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;# add any custom logic you want to occur on startup here.&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;echo “cloud-init script execution&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;example-launching-a-single-pet-vm-from-custom-image&quot;&gt;Example: Launching a Single “Pet” VM from Custom Image&lt;/h2&gt;

&lt;p&gt;In the manifest below, we’re starting a new VM with a PVC cloned from our pre-provisioned VM image that contains the nginx server. When the VM boots up, a new PVC will be created in the VM’s namespace that is a clone of the PVC referenced in our vm-images namespace.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachine&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io/vm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;running&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io/vm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;disks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;datavolumedisk1&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;machine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;terminationGracePeriodSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;dataVolume&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;datavolumedisk1&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;cloudInitNoCloud&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;userData&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;# add any custom logic you want to occur on startup here.&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;echo “cloud-init script execution&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;dataVolumeTemplates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5Gi&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vm-images&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fedora-31-nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;other-custom-creation-image-tools&quot;&gt;Other Custom Creation Image Tools&lt;/h1&gt;

&lt;p&gt;In my example I imported a VM base image into the cluster and used KubeVirt to provision a custom image with a technique that used cloud-init. This may or may not make sense for your use case. It’s possible you need to pre-provision the VM image before importing into the cluster at all.&lt;/p&gt;

&lt;p&gt;If that’s the case, I suggest looking into two tools.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://packer.io/docs/builders/qemu.html&quot;&gt;Packer.io using the qemu builder&lt;/a&gt;. This allows you to automate building custom images on your local machine using configuration files that describe all the build steps. I like this tool because it closely matches the Kubernetes “declarative” approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://libguestfs.org/virt-customize.1.html&quot;&gt;Virt-customize&lt;/a&gt; is a cli tool that allows you to customize local VM images by injecting/modifying files on disk and installing packages.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://linux.die.net/man/1/virt-install&quot;&gt;Virt-install&lt;/a&gt; is a cli tool that allows you to automate a VM install as if you were installing it from a cdrom. You’ll want to look into using a kickstart file to fully automate the process.&lt;/p&gt;

&lt;p&gt;The resulting VM image artifact created from any of these tools can then be imported into the cluster in the same way we imported the base image earlier in this document.&lt;/p&gt;</content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="images" /><category term="storage" /><summary type="html">Building a VM Image Repository</summary></entry><entry><title type="html">KubeVirt Operation Fundamentals</title><link href="https://kubevirt.io//2020/KubeVirt-Operation-Fundamentals.html" rel="alternate" type="text/html" title="KubeVirt Operation Fundamentals" /><published>2020-04-30T00:00:00+00:00</published><updated>2020-04-30T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt-Operation-Fundamentals</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt-Operation-Fundamentals.html">&lt;h2 id=&quot;simplicity-above-all-else&quot;&gt;Simplicity Above All Else&lt;/h2&gt;

&lt;p&gt;In the late 1970s and early 1980s there were two video recording tape formats competing for market domination. The Betamax format was the technically superior option. Yet despite having better audio, video, and build quality, Betamax still eventually lost to the technically inferior VHS format. VHS won because it was “close enough” in terms of quality and drastically reduced the cost to the consumer.&lt;/p&gt;

&lt;p&gt;I’ve seen this same pattern play out in the open source world as well. It doesn’t matter how technically superior one project might be over another if no one can operate the thing. The “cost” here is operational complexity. The project people can actually get up and running in 5 minutes as a proof of concept is usually going to win over another project they struggle to stand up for several hours or days.&lt;/p&gt;

&lt;p&gt;With KubeVirt, our aim is Betamax for quality and VHS for operational complexity costs. When we have to choose between the two, the option that involves less operational complexity wins 9 out of 10 times.&lt;/p&gt;

&lt;p&gt;Essentially, above all else, KubeVirt must be simple to use.&lt;/p&gt;

&lt;h2 id=&quot;installation-made-easy&quot;&gt;Installation Made Easy&lt;/h2&gt;

&lt;p&gt;From my experience, the first (and perhaps the largest) hurdle a user faces when approaching a new project is installation. When the KubeVirt architecture team placed their bet’s on what technical direction to take the project early on, picking a design that was easy to install was a critical component of the decision making process.&lt;/p&gt;

&lt;p&gt;As a result, our goal from day one has always been to make installing KubeVirt as simple as posting manifests to the cluster with standard Kubernetes client tooling (like kubectl). No per node package installations, no host level configurations. All KubeVirt components have to be delivered as containers and managed with Kubernetes.&lt;/p&gt;

&lt;p&gt;We’ve maintained this simplicity today. Installing KubeVirt v0.27.0 is as simple as…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; posting the KubeVirt operator manifest&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/kubevirt/kubevirt/releases/download/v0.27.0/kubevirt-operator.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; posting the KubeVirt install object, which you can use to define exactly what version you want to install using the KubeVirt operator. In our example here, this custom resource defaults to the release that matches the installed operator.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/kubevirt/kubevirt/releases/download/v0.27.0/kubevirt-cr.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; and then optionally waiting for the KubeVirt install object’s “Available” condition, which indicates installation has succeeded.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt &lt;span class=&quot;nb&quot;&gt;wait &lt;/span&gt;kv kubevirt &lt;span class=&quot;nt&quot;&gt;--for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Available
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Maintaining this simplicity played a critical role in our design process early on. At one point we had to make a decision whether to use the existing Kubernetes container runtimes or create our own special virtualization runtime to run in parallel to the cluster’s container runtime. We certainly had more control with our own runtime, but there was no practical way of delivering our own CRI implementation that would be easy to install on existing Kubernetes clusters. The installation would require invasive per node modifications and fall outside of the scope of what we could deliver using Kubernetes manifests alone, so we dropped the idea. Lucky for us, reusing the existing container runtime was both the simplest approach operationally and eventually proved to be the superior approach technically for our use case.&lt;/p&gt;

&lt;h2 id=&quot;zero-downtime-updates&quot;&gt;Zero Downtime Updates&lt;/h2&gt;

&lt;p&gt;While installation is likely the first hurdle for evaluating a project, how to perform updates quickly becomes the next hurdle before placing a project into production. This is why we created the KubeVirt &lt;strong&gt;virt-operator.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you go back and look at the installation steps in the previous section, you’ll notice the first step is to post the virt-operator manifest and the second step is posting a custom resource object. What we’re doing here is bringing up the virt-operator somewhere in the cluster, and then posting a custom resource object representing the KubeVirt install. That second step is telling virt-operator to install KubeVirt. The third step is simply watching our install object to determine when virt-operator has reported the install is complete.&lt;/p&gt;

&lt;p&gt;Using our default installation instructions, zero downtime updates are as simple as posting a new virt-operator deployment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Update virt-operator from our original install of v0.27.0 to v0.28.0 by applying a new virt-operator manifest.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/kubevirt/kubevirt/releases/download/v0.28.0/kubevirt-operator.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Watch the install object to see when the installation completes. Eventually it will report v0.28.0 as the observed version which indicates the update has completed.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get kv &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; yaml &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;observedKubeVirtVersion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Behind the scenes, virt-operator is coordinating the roll out of all the new KubeVirt components in a way that ensures existing virtual machine workloads are not disrupted.&lt;/p&gt;

&lt;p&gt;The KubeVirt community supports and tests the update path between each KubeVirt minor release to ensure workloads remain available both before, during, and after an update has completed. Furthermore, there are a set of functional tests that run on every pull request made to the project that validate the code about to be submitted does not disrupt the update path from the latest KubeVirt release. Our merge process won’t even allow code to enter the code base without first passing these update functional tests on a live cluster.&lt;/p&gt;</content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="design" /><category term="architecture" /><category term="operation" /><summary type="html">Simplicity Above All Else</summary></entry><entry><title type="html">KubeVirt Security Fundamentals</title><link href="https://kubevirt.io//2020/KubeVirt-Security-Fundamentals.html" rel="alternate" type="text/html" title="KubeVirt Security Fundamentals" /><published>2020-04-29T00:00:00+00:00</published><updated>2020-04-29T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt-Security-Fundamentals</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt-Security-Fundamentals.html">&lt;h2 id=&quot;security-guidelines&quot;&gt;Security Guidelines&lt;/h2&gt;

&lt;p&gt;In KubeVirt, our approach to security can be summed up by adhering to the following guidelines.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Maintain the &lt;strong&gt;principle of least privilege&lt;/strong&gt; for all our components, meaning each component only has access to exactly the minimum privileges required to operate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Establish boundaries between trusted vs untrusted components.&lt;/strong&gt; In our case, an untrusted component is typically anything that executes user third party logic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inter-component network communication &lt;strong&gt;must be secured by TLS with mutual peer authentication.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s take a look at what each of these guidelines mean for us practically when it comes to KubeVirt’s design.&lt;/p&gt;

&lt;h2 id=&quot;the-principle-of-least-privilege&quot;&gt;The Principle of Least Privilege&lt;/h2&gt;

&lt;p&gt;By limiting each component to only the exact privileges it needs to operate, we reduce the blast radius that occurs if a component is compromised.&lt;/p&gt;

&lt;p&gt;Here’s a simple and rather obvious example. If a component needs access to a secret in a specific namespace, then we give that component read-only access to that single secret and not access to read all secrets. If that component is compromised, we’ve then limited the blast radius for what can be exploited.&lt;/p&gt;

&lt;p&gt;For KubeVirt, the principle of least privilege can be broken into two categories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cluster Level Access:&lt;/strong&gt; The resources and APIs a component is permitted to access on the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Host Level Access:&lt;/strong&gt; The local resources a component is permitted to access on the host it is running on.&lt;/p&gt;

&lt;h3 id=&quot;cluster-level-access&quot;&gt;Cluster Level Access&lt;/h3&gt;

&lt;p&gt;For cluster level access the primary tools we have to grant and restrict access to cluster resources are cluster Namespaces and RBAC (Role Based Access Control). Each KubeVirt component only has access to the exact RBAC permissions within the limited set of Namespaces it requires to operate.&lt;/p&gt;

&lt;p&gt;For example, let’s take a look at the KubeVirt control plane and runtime components highlighted in orange below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-04-29-KubeVirt-Security-Fundamentals/component-view.png&quot; alt=&quot;Components View&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-controller&lt;/strong&gt; is the component responsible for spinning up pods across the entire cluster for virtual machines to live in. As a result, this component needs access to RBAC permissions to manage pods. However, another part of virt-controller’s operation involves needing access to a single secret that contains its TLS certificate information. We aren’t going to give virt-controller access to manage secrets as well as pods simply because it needs access to read a single secret. In fact we aren’t even going to give virt-controller direct API access to any secrets at all. Instead we use the ability to pass a cluster secret as a pod volume into the virt-controller’s pod in order to provide read-only access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-api&lt;/strong&gt; is the component that validates our api and provides virtual machine console and VNC access. This component doesn’t need access to create Pods like virt-controller does. Instead it mostly only requires read and modify access to existing KubeVirt API objects. As a result, if virt-api is compromised the blast radius is mostly limited to KubeVirt objects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-handler&lt;/strong&gt; is a privileged daemonset that resides at the host level on every node that is capable of spinning up KubeVirt virtual machines. This component needs cluster access to the KubeVirt VirtualMachineInstance objects in order to manage the startup flow of virtual machines. However it doesn’t need cluster access to the pod objects the virtual machines live in. Similar to virt-api, what little cluster access this component has is mostly read-only and limited to the KubeVirt API.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-launcher&lt;/strong&gt; is a non-privileged component that resides in every virtual machine’s pod. This component is responsible for starting and monitoring the qemu-kvm process. Since this process lives within an “untrusted” environment that is executing third party logic, we’ve designed this component to require no cluster API access. As a result, this pod only receives the default service account for the namespace the pod resides in. If virt-launcher is compromised, cluster API access should not be impacted.&lt;/p&gt;

&lt;h3 id=&quot;host-level-access&quot;&gt;Host Level Access&lt;/h3&gt;

&lt;p&gt;For host level access, the primary tools we have at our disposal for limiting access primarily reside within the Pod specification’s &lt;strong&gt;securityContext&lt;/strong&gt; section. It’s here that we can define settings like what local user a container runs with, whether a container has access to host namespaces, and SELinux related options. Other tools for host level access involve exposing &lt;strong&gt;hostPath volumes&lt;/strong&gt; for shared host directory access and &lt;strong&gt;DevicePlugins&lt;/strong&gt; to pass host devices into the pod environment.&lt;/p&gt;

&lt;p&gt;Let’s take a look at a few examples of how host access is managed for our components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-controller&lt;/strong&gt; and &lt;strong&gt;virt-api&lt;/strong&gt; are cluster level components only, and have no need for access to host resources. These components run as non-privileged and non-root within their own isolated namespaces. No special host level access is granted to these components. For OpenShift clusters, the &lt;strong&gt;SCC&lt;/strong&gt; (Security Context Constraint) feature even provides the ability to restrict virt-controller’s permissions in a way that prevents it from creating pods with host access.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-launcher&lt;/strong&gt; is a host level component that is non-privileged and untrusted. However this component still needs access to host level devices (like /dev/kvm, gpus, and network devices) in order to start the virtual machine. Through the use of the Kubernetes &lt;strong&gt;Device Plugin&lt;/strong&gt; feature, we can expose host devices into a pod’s environment in a controlled way that doesn’t compromise namespace isolation or require hostPath volumes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virt-handler&lt;/strong&gt; is a host level component that is both privileged and trusted. This component’s responsibilities involve reaching into the virt-launcher’s pod to perform actions we don’t want the untrusted virt-launcher component to have permissions to perform itself. The primary method we have to restrict virt-handler’s access to the host is through SELinux. Since virt-handler requires maintaining some limited persistent state, hostPath volumes are also utilized to allow virt-handler to store persistent information on the host that can persist through virt-handler updates.&lt;/p&gt;

&lt;h2 id=&quot;trusted-vs-untrusted-components&quot;&gt;Trusted vs Untrusted Components&lt;/h2&gt;

&lt;p&gt;For KubeVirt, the separation between trusted and untrusted components comes when users can execute their own third party logic within a component’s environment. We can clearly illustrate this concept using the boundary between our two host level components, virt-launcher and virt-handler.&lt;/p&gt;

&lt;h3 id=&quot;establishing-boundaries&quot;&gt;Establishing Boundaries&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-04-29-KubeVirt-Security-Fundamentals/trusted-v-untrusted-boundary.png&quot; alt=&quot;Trusted vs Untrusted Boundary&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The virt-launcher pod is an untrusted environment.&lt;/strong&gt; The third party code executed within this environment is the user’s kvm virtual machine. KubeVirt has no control over what is executing within this virtual machine guest, so if there is a security vulnerability that allows breaking out of the kvm hypervisor, we want to make sure the environment that’s broken into is as limited as possible. This is why the virt-launcher’s pod has such restricted cluster and host access.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;virt-handler pod, on the other hand, is a trusted environment&lt;/strong&gt; that does not involve executing any third party code. During the virtual machine startup flow, there are privileged tasks that need to take place on the host in order to prepare the virtual machine for starting. This ranges from performing the Device Plugin logic that injects a host device into a pod’s environment, to setting up network bridges and interfaces within a pod’s environment. To accomplish this, we use the trusted virt-handler component to reach into the untrusted virt-launcher environment to perform privileged tasks.&lt;/p&gt;

&lt;p&gt;The boundary established here is that we trust virt-handler with the ability to influence and provide information about all virtual machines running on a host, and limit virt-launcher to only influence and provide information about itself.&lt;/p&gt;

&lt;h3 id=&quot;securing-boundaries&quot;&gt;Securing Boundaries&lt;/h3&gt;

&lt;p&gt;Any communication channel that gives an untrusted environment the ability to present information to a trusted environment must be heavily scrutinized to prevent the possibility of privilege escalation. For example, the boundary between virt-handler and virt-launcher is meant to work like a one way mirror. The trusted virt-handler component can reach directly into the untrusted virt-launcher environments, but each virt-launcher can’t reach outside of its own isolated environment. Host namespace isolation provides a reasonable guarantee that virt-launcher can’t reach outside of its own environment directly, however we still have to be mindful about indirection communication.&lt;/p&gt;

&lt;p&gt;Virt-handler observes information presented to it by each virt-launcher pod. If a virt-launcher environment is able to present fake information about another virtual machine, then the untrusted virt-launcher environment could indirectly influence the execution of another workload.&lt;/p&gt;

&lt;p&gt;To counter this, when designing communication channels between trusted and untrusted components, we have to be careful to only allow communication from untrusted sources to influence itself and furthermore only influence itself in a way that can’t result in escalated privileges.&lt;/p&gt;

&lt;h2 id=&quot;mutual-tls-authentication&quot;&gt;Mutual TLS Authentication&lt;/h2&gt;

&lt;p&gt;There is a built in trust that components have for interacting with one another. For example, virt-api is allowed to establish virtual machine Console/VNC streams with virt-handler, and live migration is performed by streaming information between two virt-handler instances.&lt;/p&gt;

&lt;p&gt;However for these types of interactions to work, we have to have a strong guarantee that the endpoints we’re talking to are in fact who they present themselves to be. Otherwise we could live migrate a virtual machine to an untrusted location, or provide VNC access to a virtual machine to an unauthorized endpoint.&lt;/p&gt;

&lt;p&gt;In KubeVirt we solve this issue of inter-component communication trust in the same way Kubernetes solves it. Each component receives a unique TLS certificate signed by a cluster Certificate Authority which is used to guarantee the component is who they say they are. The certificate and CA information is injected into each component using a secret passed in as a Pod volume. Whenever a component acts as a client establishing a new connection with another component, it uses its unique certificate to prove its identify. Likewise, the server accepting the clients connection also presents its certificate to the client. This mutual peer certificate authentication allows both the client and server to establish trust.&lt;/p&gt;

&lt;p&gt;So, when virt-api attempts to establish a VNC console stream with a virt-handler component, virt-handler is configured to only allow that stream to be opened by an endpoint providing a valid virt-api certificate, and virt-api will only talk to a server that presents the expected virt-handler certificate.&lt;/p&gt;

&lt;h2 id=&quot;ca-and-certificate-rotation&quot;&gt;CA and Certificate Rotation&lt;/h2&gt;

&lt;p&gt;In KubeVirt both our CA and certificates are rotated on a user defined recurring interval. In the event that either the CA key or a certificate is compromised, this information will eventually be rendered stale and unusable regardless if the compromise is known or not. If the compromise is known, a forced CA and certificate rotation can be invoked by the cluster admin simply by deleting the corresponding secrets in the KubeVirt install namespace.&lt;/p&gt;</content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="design" /><category term="architecture" /><category term="security" /><summary type="html">Security Guidelines</summary></entry><entry><title type="html">KubeVirt Architecture Fundamentals</title><link href="https://kubevirt.io//2020/KubeVirt-Architecture-Fundamentals.html" rel="alternate" type="text/html" title="KubeVirt Architecture Fundamentals" /><published>2020-04-28T00:00:00+00:00</published><updated>2020-04-28T00:00:00+00:00</updated><id>https://kubevirt.io//2020/KubeVirt-Architecture-Fundamentals</id><content type="html" xml:base="https://kubevirt.io//2020/KubeVirt-Architecture-Fundamentals.html">&lt;h2 id=&quot;placing-our-bets&quot;&gt;Placing our Bets&lt;/h2&gt;

&lt;p&gt;Back in 2017 the KubeVirt architecture team got together and placed their bets on a set of core design principles that became the foundation of what KubeVirt is today. At the time, our decisions broke convention. We chose to take some calculated risks with the understanding that those risks had a real chance of not playing out in our favor.&lt;/p&gt;

&lt;p&gt;Luckily, time has proven our bets were well placed. Since those early discussions back in 2017, KubeVirt has grown from a theoretical prototype into a project deployed in production environments with a thriving open source community. While KubeVirt has grown in maturity and sophistication throughout the past few years, the initial set of guidelines established in those early discussions still govern the project’s architecture today.&lt;/p&gt;

&lt;p&gt;Those guidelines can be summarized nearly entirely by the following two key decisions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virtual machines run in Pods using the existing container runtimes.&lt;/strong&gt; This decision came at a time when other Kubernetes virtualization efforts were creating their own virtualization specific CRI runtimes. We took a bet on our ability to successfully launch virtual machines using existing and future container runtimes within an unadulterated Pod environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virtual machines are managed using a custom “Kubernetes like” declarative API.&lt;/strong&gt; When this decision was made, imperative APIs were the defacto standard for how other platforms managed virtual machines. However, we knew in order to succeed in our mission to deliver a truly cloud-native API managed using existing Kubernetes tooling (like kubectl), we had to adhere fully to the declarative workflow. We took a bet that the lackluster Kubernetes Third Party Resource support (now known as CRDs) would eventually provide the ability to create custom declarative APIs as first class citizens in the cluster.&lt;/p&gt;

&lt;p&gt;Let’s dive into these two points a bit and take a look at how these two key decisions permeated throughout our entire design.&lt;/p&gt;

&lt;h2 id=&quot;virtual-machines-as-pods&quot;&gt;Virtual Machines as Pods&lt;/h2&gt;

&lt;p&gt;We often pitch KubeVirt by saying something like “KubeVirt allows you to run virtual machines side by side with your container workloads”. However, the reality is &lt;strong&gt;we’re delivering virtual machines as container workloads.&lt;/strong&gt; So as far as Kubernetes is concerned, there are no virtual machines, just pods and containers. Fundamentally, KubeVirt virtual machines just look like any other containerized application to the rest of the cluster. It’s our KubeVirt API and control plane that make these containerized virtual machines behave like you’d expect from using other virtual machine management platforms.&lt;/p&gt;

&lt;p&gt;The payoff from running virtual machines within a Kubernetes Pod has been huge for us. There’s an entire ecosystem that continues to grow around how to provide pods with access to networks, storage, host devices, cpu, memory, and more. This means every time a problem or feature is added to pods, it’s yet another tool we can use for virtual machines.&lt;/p&gt;

&lt;p&gt;Here are a few examples of how pod features meet the needs of virtual machines as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Virtual machines need persistent disks. Users should be able to stop a VM, start a VM, and have the data persist. There’s a Kubernetes storage abstraction called a PVC (persistent volume claim) that allows persistent storage to be attached to a pod. This means by placing the virtual machine in a pod, we can use the existing PVC mechanisms of delivering persistent storage to deliver our virtual machine disks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Network:&lt;/strong&gt; Virtual machines need access to cluster networking. Pods are provided network interfaces that tie directly into the pod network via CNI. We can give a virtual machine running in a pod access to the pod network using the default CNI allocated network interfaces already present in the pod’s environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU/Memory:&lt;/strong&gt; Users need the ability to assign cpu and memory resources to Virtual machines. We can assign cpu and memory to pods using the resource requests/limits on the pod spec. This means through the use of pod resource requests/limits we are able to assign resources directly to virtual machines as well.&lt;/p&gt;

&lt;p&gt;This list goes on and on. As problems are solved for pods, KubeVirt leverages the solution and translates it to the virtual machine equivalent.&lt;/p&gt;

&lt;h2 id=&quot;the-declarative-kubevirt-virtualization-api&quot;&gt;The Declarative KubeVirt Virtualization API&lt;/h2&gt;

&lt;p&gt;While a KubeVirt virtual machine runs within a pod, that doesn’t change the fact that people working with virtual machines have a different set of expectations for how virtual machines should work compared to how pods are managed.&lt;/p&gt;

&lt;p&gt;Here’s the conflict.&lt;/p&gt;

&lt;p&gt;Pods are &lt;strong&gt;mortal workloads&lt;/strong&gt;. A pod is declared by posting it’s manifest to the cluster, the pod runs once to completion, and that’s it. It’s done.&lt;/p&gt;

&lt;p&gt;Virtual machines are &lt;strong&gt;immortal workloads&lt;/strong&gt;. A virtual machine doesn’t just run once to completion. Virtual machines have state. They can be started, stopped, and restarted any number of times. Virtual machines have concepts like live migration as well. Furthermore if the node a virtual machine is running on dies, the expectation is for that exact same virtual machine to resurrect on another node maintaining its state.&lt;/p&gt;

&lt;p&gt;So, pods run once and virtual machines live forever. How do we reconcile the two? Our solution came from taking a play directly out of the Kubernetes playbook.&lt;/p&gt;

&lt;p&gt;The Kubernetes core apis have this concept of layering objects on top of one another through the use of &lt;strong&gt;workload controllers&lt;/strong&gt;. For example, the Kubernetes ReplicaSet is a workload controller layered on top of pods. The ReplicaSet controller manages ensuring that there are always ‘x’ number of pod replicas running within the cluster. If a ReplicaSet object declares that 5 pod replicas should be running, but a node dies bringing that total to 4, then the ReplicaSet workload controller manages spinning up a 5th pod in order to meet the declared replica count. The workload controller is always reconciling on the ReplicaSet objects desired state.&lt;/p&gt;

&lt;p&gt;Using this established Kubernetes pattern of layering objects on top of one another, we came up with our own virtualization specific API and corresponding workload controller called a &lt;strong&gt;“VirtualMachine”&lt;/strong&gt; (big surprise there on the name, right?). Users declare a VirtualMachine object just like they would a pod by posting the VirtualMachine object’s manifest to the cluster. The big difference here that deviates from how pods are managed is that we allow VirtualMachine objects to be declared to exist in different states. For example, you can declare you want to “start” a virtual machine by setting “running: true” on the VirtualMachine object’s spec. Likewise you can declare you want to “stop” a virtual machine by setting “running: false” on the VirtualMachine object’s spec. Behind the scenes, setting the “running” field to true or false results in the workload controller creating or deleting a pod for the virtual machine to live in.&lt;/p&gt;

&lt;p&gt;In the end, we essentially created the concept of an &lt;strong&gt;immortal VirtualMachine&lt;/strong&gt; by laying our own custom API on top of mortal pods. Our API and controller knows how to resurrect a “stopped” VirtualMachine by constructing a pod with all the right network, storage volumes, cpu, and memory attached to in order to accurately bring the VirtualMachine back to life with the exact same state it stopped with.&lt;/p&gt;</content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="design" /><category term="architecture" /><summary type="html">Placing our Bets</summary></entry><entry><title type="html">Live Migration in KubeVirt</title><link href="https://kubevirt.io//2020/Live-migration.html" rel="alternate" type="text/html" title="Live Migration in KubeVirt" /><published>2020-03-22T00:00:00+00:00</published><updated>2020-03-22T00:00:00+00:00</updated><id>https://kubevirt.io//2020/Live-migration</id><content type="html" xml:base="https://kubevirt.io//2020/Live-migration.html">&lt;!-- TOC depthFrom:2 depthTo:6 orderedList:false --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#enabling-live-migration&quot;&gt;Enabling Live Migration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#configuring-live-migration&quot;&gt;Configuring Live Migration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#performing-the-live-migration&quot;&gt;Performing the Live Migration&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cancelling-a-live-migration&quot;&gt;Cancelling a Live Migration&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-can-go-wrong&quot;&gt;What can go wrong?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#node-eviction&quot;&gt;Node Eviction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This blog post will be explaining on KubeVirt’s ability to perform live migration of virtual machines.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Live Migration is a process during which a running Virtual Machine Instance moves to another compute node while the guest workload continues to run and remain accessible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The concept of live migration is already well-known among virtualization platforms and enables administrators to keep user workloads running while the servers can be moved to maintenance for any reason that you might think of like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hardware maintenance (physical, firmware upgrades, etc)&lt;/li&gt;
  &lt;li&gt;Power management, by moving workloads to a lower number of hypervisors during off-peak hours&lt;/li&gt;
  &lt;li&gt;etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KubeVirt also includes support for virtual machine migration within Kubernetes when enabled.&lt;/p&gt;

&lt;p&gt;Keep reading to learn how!&lt;/p&gt;

&lt;h2 id=&quot;enabling-live-migration&quot;&gt;Enabling Live Migration&lt;/h2&gt;

&lt;p&gt;To enable live migration we need to enable the &lt;code class=&quot;highlighter-rouge&quot;&gt;feature-gate&lt;/code&gt; for it by adding &lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigration&lt;/code&gt; to the key:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ConfigMap&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt-config&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;feature-gates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LiveMigration&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A current &lt;code class=&quot;highlighter-rouge&quot;&gt;kubevirt-config&lt;/code&gt; can be edited to append “&lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigration&lt;/code&gt;” to an existing configuration:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl edit configmap kubevirt-config &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;feature-gates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DataVolumes,LiveMigration&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuring-live-migration&quot;&gt;Configuring Live Migration&lt;/h2&gt;

&lt;p&gt;If we want to alter the defaults for Live-Migration, we can further edit the &lt;code class=&quot;highlighter-rouge&quot;&gt;kubevirt-config&lt;/code&gt; like:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ConfigMap&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt-config&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;feature-gates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LiveMigration&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;migrations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|-&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;parallelMigrationsPerCluster: 5&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;parallelOutboundMigrationsPerNode: 2&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;bandwidthPerMigration: 64Mi&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;completionTimeoutPerGiB: 800&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;progressTimeout: 150&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Parameters are explained in the below table (check the documentation for more details):&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Parameter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Default value&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parallelMigrationsPerCluster&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;How many migrations might happen at the same time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parallelOutboundMigrationsPerNode&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;How many outbound migrations for a particular node&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bandwidthPerMigration&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;64Mi&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;MiB/s to have the migration limited to, in order to not affect other systems&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;completionTimeoutPerGiB&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;800&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Time for a GiB of data to wait to be completed before aborting the migration.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;progressTimeout&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;150&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Time to wait for Live Migration to progress in transferring data&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;performing-the-live-migration&quot;&gt;Performing the Live Migration&lt;/h2&gt;

&lt;div class=&quot;premonition error&quot;&gt;&lt;div class=&quot;fa fa-exclamation-triangle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Limitations&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Virtual Machines using PVC must have a &lt;code class=&quot;highlighter-rouge&quot;&gt;RWX&lt;/code&gt; access mode to be Live-Migrated&lt;/li&gt;
  &lt;li&gt;Additionally, pod network binding of bridge interface is not allowed&lt;/li&gt;
&lt;/ol&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Live migration is initiated by posting an object &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstanceMigration&lt;/code&gt; to the cluster, indicating the VM name to migrate, like in the following example:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachineInstanceMigration&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;migration-job&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;vmiName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vmi-fedora&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will trigger the process for the VM.&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;NOTE&lt;/p&gt;
&lt;p&gt;When a VM is started, a calculation has been already performed indicating if the VM is live-migratable or not. This information is stored in the &lt;code class=&quot;highlighter-rouge&quot;&gt;VMI.status.conditions&lt;/code&gt;. Currently, most of the calculation is based on the &lt;code class=&quot;highlighter-rouge&quot;&gt;Access Mode&lt;/code&gt; for the VMI volumes but can be based on multiple parameters. For example:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Conditions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LiveMigratable&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Migration Method&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BlockMigration&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If the VM is Live-Migratable, the request will submit successfully. The status change will be reported under &lt;code class=&quot;highlighter-rouge&quot;&gt;VMI.status&lt;/code&gt;. Once live migration is complete, a status of &lt;code class=&quot;highlighter-rouge&quot;&gt;Completed&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Failed&lt;/code&gt; will be indicated.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Watch out!&lt;/p&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;Migration Method&lt;/code&gt; field can contain:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;BlockMigration&lt;/code&gt; : meaning that the disk data is being copied from source to destination&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigration&lt;/code&gt;: meaning that only the memory is copied from source to destination&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMs with block devices located on shared storage backends like the ones provided by &lt;a href=&quot;https://rook.io/&quot;&gt;Rook&lt;/a&gt; that provide PVCs with ReadWriteMany access have the option to live-migrate only memory contents instead of having to also migrate the block devices.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;cancelling-a-live-migration&quot;&gt;Cancelling a Live Migration&lt;/h3&gt;

&lt;p&gt;If we want to abort the Live Migration, ‘Kubernetes-Style’, we’ll just delete the object we created for triggering it.&lt;/p&gt;

&lt;p&gt;In this case, the VM status for migration will report some additional information:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;Migration State&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Abort Requested&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Abort Status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Succeeded&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Completed&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;End Timestamp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2019-03-29T04:02:49Z&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Failed&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Migration Config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Completion Timeout Per GiB&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;800&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Progress Timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;150&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Migration UID&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;57a693d6-51d7-11e9-b370-525500d15501&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Source Node&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;node02&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Start Timestamp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2019-03-29T04:02:47Z&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Target Direct Migration Node Ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;39445&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;43345&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;49152&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;44222&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;49153&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Target Node&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;node01&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Target Node Address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.128.0.46&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Target Node Domain Detected&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Target Pod&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-launcher-testvmimcbjgw6zrzcmp8wpddvztvzm7x2k6cjbdgktwv8tkq&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that there are some additional fields that indicate that &lt;code class=&quot;highlighter-rouge&quot;&gt;Abort Requested&lt;/code&gt; happened and in the above example that it has &lt;code class=&quot;highlighter-rouge&quot;&gt;Succeded&lt;/code&gt;, in this case, the original fields for migration will report as &lt;code class=&quot;highlighter-rouge&quot;&gt;Completed&lt;/code&gt; (because there’s no running migration) and &lt;code class=&quot;highlighter-rouge&quot;&gt;Failed&lt;/code&gt; set to true.&lt;/p&gt;

&lt;h2 id=&quot;what-can-go-wrong&quot;&gt;What can go wrong?&lt;/h2&gt;

&lt;p&gt;Live-migration is a complex process that requires transferring data from one ‘VM’ in one node to another ‘VM’ into another one, this requires that the activity of the VM being live-migrated to be compatible with the network configuration and throughput so that all the data can be migrated &lt;em&gt;faster&lt;/em&gt; than the data is changed at the original VM, this is usually referred to as &lt;em&gt;converging&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Some values can be adjusted (check the &lt;a href=&quot;#configuring-live-migration&quot;&gt;table&lt;/a&gt; for settings that can be tuned), to allow it to succeed but as a trade-off:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Increasing the number of VMs that can migrate at once, will reduce the available bandwidth.&lt;/li&gt;
  &lt;li&gt;Increasing the bandwidth could affect applications running on that node (origin and target).&lt;/li&gt;
  &lt;li&gt;Storage migration (check the &lt;code class=&quot;highlighter-rouge&quot;&gt;Info&lt;/code&gt; note in the &lt;a href=&quot;#performing-the-live-migration&quot;&gt;Performing the Live Migration &lt;/a&gt; section on the differences) might also consume bandwidth and resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;node-eviction&quot;&gt;Node Eviction&lt;/h2&gt;

&lt;p&gt;Sometimes, a node requires to be put on maintenance and it includes workloads on it, either containers or, in KubeVirt’s case, VM’s.&lt;/p&gt;

&lt;p&gt;It is possible to use &lt;strong&gt;selectors&lt;/strong&gt;, for example, move all the virtual machines to another node via &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl drain &amp;lt;nodename&amp;gt;&lt;/code&gt;, for example, evicting all KubeVirt VM’s from a node can be done via:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl drain &amp;lt;node name&amp;gt; &lt;span class=&quot;nt&quot;&gt;--delete-local-data&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--ignore-daemonsets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--pod-selector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubevirt.io&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;virt-launcher
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Reenabling node after eviction&lt;/p&gt;
&lt;p&gt;Once the node has been tainted for eviction, we can use &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl uncordon &amp;lt;nodename&amp;gt;&lt;/code&gt; to make it schedulable again.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;According to documentation, &lt;code class=&quot;highlighter-rouge&quot;&gt;--delete-local-data&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;--ignore-daemonsets&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;--force&lt;/code&gt; are required because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pods using &lt;code class=&quot;highlighter-rouge&quot;&gt;emptyDir&lt;/code&gt; can be deleted because the data is ephemeral.&lt;/li&gt;
  &lt;li&gt;VMI will have &lt;code class=&quot;highlighter-rouge&quot;&gt;DaemonSets&lt;/code&gt; via &lt;code class=&quot;highlighter-rouge&quot;&gt;virt-handler&lt;/code&gt; so it’s safe to proceed.&lt;/li&gt;
  &lt;li&gt;VMIs are not owned by a &lt;code class=&quot;highlighter-rouge&quot;&gt;ReplicaSet&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;DaemonSet&lt;/code&gt;, so kubectl can’t guarantee that those are restarted. KubeVirt has its own controllers for it managing VMI, so kubectl shouldn’t bother about it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we omit the &lt;code class=&quot;highlighter-rouge&quot;&gt;--pod-selector&lt;/code&gt;, we’ll force eviction of all Pods and VM’s from a node.&lt;/p&gt;

&lt;div class=&quot;premonition important&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;
&lt;p&gt;In order to have VMIs using &lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigration&lt;/code&gt; for eviction, we have to add a specific spec in the VMI YAML, so that when the node is tainted with &lt;code class=&quot;highlighter-rouge&quot;&gt;kubevirt.io/drain:NoSchedule&lt;/code&gt; is added to a node.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;evictionStrategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LiveMigrate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From that point, when &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl taint nodes &amp;lt;foo&amp;gt; kubevirt.io/drain=draining:NoSchedule&lt;/code&gt; is executed, the migrations will start.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As a briefing on the above data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigrate&lt;/code&gt; needs to be enabled on KubeVirt as a feature gate.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigrate&lt;/code&gt; will add status to the VMI object indicating if it’s a candidate or not and if so, which mode to use (Block or Live)
    &lt;ul&gt;
      &lt;li&gt;Based on the storage backend and other conditions, it will enable &lt;code class=&quot;highlighter-rouge&quot;&gt;LiveMigration&lt;/code&gt; or just &lt;code class=&quot;highlighter-rouge&quot;&gt;BlockMigration&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/#/installation/live-migration?id=live-migration&quot;&gt;Live Migration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/#/installation/node-eviction?id=how-to-evict-all-vms-on-a-node&quot;&gt;Node Drain/Eviction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rook.io/&quot;&gt;Rook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pablo Iranzo Gómez</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Live Migration" /><category term="node drain" /><summary type="html"></summary></entry><entry><title type="html">Advanced scheduling using affinity and anti-affinity rules</title><link href="https://kubevirt.io//2020/Advanced-scheduling-with-affinity-rules.html" rel="alternate" type="text/html" title="Advanced scheduling using affinity and anti-affinity rules" /><published>2020-02-25T00:00:00+00:00</published><updated>2020-02-25T00:00:00+00:00</updated><id>https://kubevirt.io//2020/Advanced-scheduling-with-affinity-rules</id><content type="html" xml:base="https://kubevirt.io//2020/Advanced-scheduling-with-affinity-rules.html">&lt;p&gt;This blog post shows how KubeVirt can take advantage of Kubernetes inner features to provide an advanced scheduling mechanism to virtual machines (VMs). The same or even more complex &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&quot;&gt;affinity and anti-affinity&lt;/a&gt; rules can be assigned to VMs or Pods in Kubernetes than in traditional virtualization solutions.&lt;/p&gt;

&lt;p&gt;It is important to notice that from the Kubernetes scheduler stand point, which will be explained later, it only manages Pod and node scheduling. Since the VM is wrapped up in a Pod, the same scheduling rules are completely valid to KubeVirt VMs.&lt;/p&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;As informed in the &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity&quot;&gt;official Kubernetes documentation&lt;/a&gt;: inter-pod affinity and anti-affinity require substantial amount of processing which can slow-down scheduling in large clusters significantly. This can be specially notorious in clusters larger than several hundred nodes.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In a Kubernetes cluster, &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&quot;&gt;kube-scheduler&lt;/a&gt; is the default scheduler and runs as part of the control plane. Kube-scheduler is in charge of selecting an optimal node for every newly created or unscheduled pod to run on. However, every container within a pod and the pods themselves, have different requirements for resources. Therefore, existing nodes need to be filtered according to the specific requirements.&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;If you want and need to, you can write your own scheduling component and use it instead.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When we talk about scheduling, we refer basically to making sure that Pods are matched to Nodes so that a Kubelet can run them. Actually, kube-scheduler selects a node for the pod in a 2-step operation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Filtering.&lt;/strong&gt; The filtering step finds the set of candidate Nodes where it’s possible to schedule the Pod. The result is a list of Nodes, usually more than one.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scoring.&lt;/strong&gt; In the scoring step, the scheduler ranks the remaining nodes to choose the most suitable Pod placement. This is accomplished based on a score obtained from a list of scoring rules that are applied by the scheduler.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The obtained list of candidate nodes is evaluated using multiple priority criteria, which add up to a weighted score. Nodes with higher values are better candidates to run the pod. Among the criteria are affinity and anti-affinity rules; nodes with higher affinity for the pod have a higher score, and nodes with higher anti-affinity have a lower score. Finally, kube-scheduler assigns the Pod to the Node with the highest score. If there is more than one node with equal scores, kube-scheduler selects one of these randomly.&lt;/p&gt;

&lt;p&gt;In this blog post we are going to focus on examples of affinity and anti-affinity rules applied to solve real use cases. A common use for affinity rules is to schedule related pods to be close to each other for performance reasons. A common use case for anti-affinity rules is to schedule related pods not too close to each other for high availability reasons.&lt;/p&gt;

&lt;h2 id=&quot;goal-run-my-customapp&quot;&gt;Goal: Run my customapp&lt;/h2&gt;

&lt;p&gt;In this example, our mission is to run a customapp that is composed of 3 tiers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A web proxy cache based on varnish HTTP cache.&lt;/li&gt;
  &lt;li&gt;A web appliance delivered by a third provider.&lt;/li&gt;
  &lt;li&gt;A clustered database running on MS Windows.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Instructions were delivered to deploy the application in our production Kubernetes cluster taking advantage of the existing KubeVirt integration and making sure the application is resilient to any problems that can occur. The current status of the cluster is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A stretched Kubernetes cluster is already up and running.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/#/&quot;&gt;KubeVirt&lt;/a&gt; is already installed.&lt;/li&gt;
  &lt;li&gt;There is enough free CPU, Memory and disk space in the cluster to deploy customapp stack.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;information&lt;/p&gt;&lt;p&gt;The Kubernetes stretched cluster is running in 3 different geographical locations to provide high availability. Also, all locations are close and well-connected to provide low latency between the nodes.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Topology used is common for large data centers, such as cloud providers, which is based in organizing hosts into regions and zones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;region&lt;/strong&gt; is a set of hosts in a close geographic area, which guarantees high-speed connectivity between them.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;zone&lt;/strong&gt;, also called an availability zone, is a set of hosts that might fail together because they share common critical infrastructure components, such as a network, storage, or power.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some important labels when creating advanced scheduling workflows with affinity and anti-affinity rules. As explained previously, they are very close linked to common topologies used in datacenters. Labels such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;topology.kubernetes.io/zone&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;topology.kubernetes.io/region&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;kubernetes.io/hostname&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;kubernetes.io/arch&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;kubernetes.io/os&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;As it is detailed in the &lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/&quot;&gt;labels and annotations official documentation&lt;/a&gt;, starting in v1.17, label &lt;em&gt;failure-domain.beta.kubernetes.io/region&lt;/em&gt; and &lt;em&gt;failure-domain.beta.kubernetes.io/zone&lt;/em&gt; are deprecated in favour of &lt;strong&gt;topology.kubernetes.io/region&lt;/strong&gt; and &lt;strong&gt;topology kubernetes.io/zone respectively&lt;/strong&gt;.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Previous labels are just prepopulated Kubernetes labels that the system uses to denote such a topology domain. In our case, the cluster is running in &lt;em&gt;Iberia&lt;/em&gt; &lt;strong&gt;region&lt;/strong&gt; across three different &lt;strong&gt;zones&lt;/strong&gt;: &lt;em&gt;scu, bcn and sab&lt;/em&gt;. Therefore, it must be labelled accordingly since advanced scheduling rules are going to be applied:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-25-Advanced-scheduling-with-affinity-rules/kubevirt-blog-affinity.resized.png&quot; alt=&quot;cluster labelling&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;Pod anti-affinity requires nodes to be consistently labelled, i.e. every node in the cluster must have an appropriate label matching &lt;strong&gt;topologyKey&lt;/strong&gt;. If some or all nodes are missing the specified topologyKey label, it can lead to unintended behavior.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Below you can find a cluster labeling where topology is based in one region and several zones spread across geographically. Additionally, special &lt;strong&gt;high performing nodes&lt;/strong&gt; composed by nodes with a high number of resources available including memory, cpu, storage and network are marked as well.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node kni-worker topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;scu
node/kni-worker labeled
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node kni-worker2 topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;scu &lt;span class=&quot;nv&quot;&gt;performance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;high
node/kni-worker2 labeled
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node kni-worker3 topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bcn
node/kni-worker3 labeled
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node kni-worker4 topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bcn &lt;span class=&quot;nv&quot;&gt;performance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;high
node/kni-worker4 labeled
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node kni-worker5 topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sab
node/kni-worker5 labeled
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node kni-worker6 topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sab &lt;span class=&quot;nv&quot;&gt;performance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;high
node/kni-worker6 labeled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, Kubernetes cluster nodes are labelled as expected:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get nodes &lt;span class=&quot;nt&quot;&gt;--show-labels&lt;/span&gt;

NAME                STATUS   ROLES    AGE   VERSION   LABELS
kni-control-plane   Ready    master   18m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-control-plane,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,node-role.kubernetes.io/master&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
kni-worker          Ready    &amp;lt;none&amp;gt;   17m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-worker,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia,topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;scu
kni-worker2         Ready    &amp;lt;none&amp;gt;   17m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-worker2,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,performance&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;high,topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia,topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;scu
kni-worker3         Ready    &amp;lt;none&amp;gt;   17m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-worker3,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia,topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bcn
kni-worker4         Ready    &amp;lt;none&amp;gt;   17m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-worker4,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,performance&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;high,topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia,topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bcn
kni-worker5         Ready    &amp;lt;none&amp;gt;   17m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-worker5,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia,topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sab
kni-worker6         Ready    &amp;lt;none&amp;gt;   17m   v1.17.0   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,kubernetes.io/hostname&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kni-worker6,kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,performance&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;high,topology.kubernetes.io/region&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iberia,topology.kubernetes.io/zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the cluster is ready to run and deploy our specific &lt;em&gt;customapp&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-clustered-database&quot;&gt;The clustered database&lt;/h3&gt;

&lt;p&gt;A MS Windows 2016 Server virtual machine is already containerized and ready to be deployed. As we have to deploy 3 replicas of the operating system a &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/code&gt; has been created. Once the replicas are up and running, database administrators will be able to reach the VMs running in our Kubernetes cluster through Remote Desktop Protocol (RDP). Eventually, MS SQL2016 database is installed and configured as a clustered database to provide high availability to our customapp.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;Check &lt;a href=&quot;/2020/KubeVirt-installing_Microsoft_Windows_from_an_iso.html&quot;&gt;KubeVirt: installing Microsoft Windows from an ISO&lt;/a&gt; if you need further information on how to deploy a MS Windows VM on KubeVirt.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Regarding the scheduling, a Kubernetes node of each zone has been labelled as high-performance, e.g. it has more memory, storage, CPU and higher performing disk and network than the other node that shares the same zone. This specific Kubernetes node was provisioned to run the database VM due to the hardware requirements to run database applications. Therefore, a scheduling rule is needed to be sure that all MSSQL2016 instances run &lt;em&gt;only&lt;/em&gt; in these high-performance servers.&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;These nodes were labelled as &lt;strong&gt;performance=high&lt;/strong&gt;.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;There are two options to accomplish our requirement, use &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector&quot;&gt;nodeSelector&lt;/a&gt; or configure &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity&quot;&gt;nodeAffinity&lt;/a&gt; rules. In our first approach, &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeSelector&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeAffinity&lt;/code&gt; rule is used. &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeSelector&lt;/code&gt; matches the nodes where the &lt;code class=&quot;highlighter-rouge&quot;&gt;performance&lt;/code&gt; key is equal to &lt;code class=&quot;highlighter-rouge&quot;&gt;high&lt;/code&gt; and makes the &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstance&lt;/code&gt; to run on top of the matching nodes. The following code snippet shows the configuration:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;nodeSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#nodeSelector matches nodes where performance key has high as value.&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;performance&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;high&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;disks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;containerdisk&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;bus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtio&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cloudinitdisk&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;interfaces&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;bridge&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;machine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;16Gi&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;networks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;pod&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, the &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/code&gt; configuration partially shown previously is applied successfully.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; vmr-windows-mssql.yaml
virtualmachineinstancereplicaset.kubevirt.io/mssql2016 created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, it is expected that the 3 &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstances&lt;/code&gt; will eventually run on the nodes where matching key/value label is configured. Actually, based on the hostname those are the &lt;em&gt;even&lt;/em&gt; nodes.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME                                 READY   STATUS              RESTARTS   AGE   IP       NODE          NOMINATED NODE   READINESS GATES
virt-launcher-mssql2016p948r-257pn   0/2     ContainerCreating   0          16s   &amp;lt;none&amp;gt;   kni-worker4   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-launcher-mssql2016rd4lk-6zz9d   0/2     ContainerCreating   0          16s   &amp;lt;none&amp;gt;   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-launcher-mssql2016z2qnw-t924b   0/2     ContainerCreating   0          16s   &amp;lt;none&amp;gt;   kni-worker6   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@eko1 ind-affinity]# kubectl get vmi &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME             AGE   PHASE        IP    NODENAME   LIVE-MIGRATABLE
mssql2016p948r   34s   Scheduling
mssql2016rd4lk   34s   Scheduling
mssql2016z2qnw   34s   Scheduling

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get vmi &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME             AGE     PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql2016p948r   3m25s   Running   10.244.1.4   kni-worker4   False
mssql2016rd4lk   3m25s   Running   10.244.2.4   kni-worker2   False
mssql2016z2qnw   3m25s   Running   10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nodeSelector&lt;/code&gt; provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature greatly expands the types of constraints you can express.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let’s test what happens if the node running the database must be rebooted due to an upgrade or any other valid reason. First, a &lt;a href=&quot;/2019/NodeDrain-KubeVirt.html&quot;&gt;node drain&lt;/a&gt; must be executed in order to evacuate all pods running and mark the node as unschedulable.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl drain kni-worker2 &lt;span class=&quot;nt&quot;&gt;--delete-local-data&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--ignore-daemonsets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt;
node/kni-worker2 already cordoned
evicting pod &lt;span class=&quot;s2&quot;&gt;&quot;virt-launcher-mssql2016rd4lk-6zz9d&quot;&lt;/span&gt;
pod/virt-launcher-mssql2016rd4lk-6zz9d evicted
node/kni-worker2 evicted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result is an unwanted scenario, where two databases are being executed in the same high performing server. &lt;em&gt;This leads us to more advanced scheduling features like affinity and anti-affinity.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get vmi &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME             AGE     PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql201696sz9   7m16s   Running   10.244.5.5   kni-worker6   False
mssql2016p948r   19m     Running   10.244.1.4   kni-worker4   False
mssql2016z2qnw   19m     Running   10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&quot;&gt;affinity/anti-affinity rules&lt;/a&gt; solve much more complex scenarios compared to &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector&quot;&gt;nodeSelector&lt;/a&gt;. Some of the key enhancements are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The language is more expressive (not just “AND or exact match”).&lt;/li&gt;
  &lt;li&gt;You can indicate that the rule is “soft”/”preference” rather than a hard requirement, so if the scheduler can’t satisfy it, the pod will still be scheduled.&lt;/li&gt;
  &lt;li&gt;You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before going into more detail, it should be noticed that there are currently two types of &lt;strong&gt;affinity&lt;/strong&gt; that applies to both &lt;em&gt;Node and Pod affinity&lt;/em&gt;. They are called &lt;code class=&quot;highlighter-rouge&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;. You can think of them as &lt;em&gt;hard&lt;/em&gt; and &lt;em&gt;soft&lt;/em&gt; respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (just like nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee.&lt;/p&gt;

&lt;p&gt;Said that, it is time to edit the &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/code&gt; YAML file. Actually, &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeSelector&lt;/code&gt; must be removed and two different affinity rules created instead.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;nodeAffinity rule&lt;/strong&gt;. This rule ensures that during scheduling time the application (MS SQL2016) must be placed &lt;em&gt;only&lt;/em&gt; on nodes where the key performance contains the value high. Note the word only, there is no room for other nodes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;podAntiAffinity rule&lt;/strong&gt;. This rule ensures that two applications with the key &lt;code class=&quot;highlighter-rouge&quot;&gt;kubevirt.io/domain&lt;/code&gt; equals to &lt;code class=&quot;highlighter-rouge&quot;&gt;mssql2016&lt;/code&gt; must not run in the same zone. Notice that the only application with this key value is the database itself and more important, notice that this rule applies to the topologyKey &lt;code class=&quot;highlighter-rouge&quot;&gt;topology.kubernetes.io/zone&lt;/code&gt;. This means that only one database instance can run in each zone, e.g. one database in &lt;em&gt;scu, bcn and sab&lt;/em&gt; respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In principle, the &lt;code class=&quot;highlighter-rouge&quot;&gt;topologyKey&lt;/code&gt; can be any legal label-key. However, for performance and security reasons, there are some constraints on &lt;code class=&quot;highlighter-rouge&quot;&gt;topologyKey&lt;/code&gt; that need to be taken into consideration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For affinity and for &lt;code class=&quot;highlighter-rouge&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; pod anti-affinity, empty &lt;code class=&quot;highlighter-rouge&quot;&gt;topologyKey&lt;/code&gt; is not allowed.&lt;/li&gt;
  &lt;li&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; pod anti-affinity, empty &lt;code class=&quot;highlighter-rouge&quot;&gt;topologyKey&lt;/code&gt; is interpreted as “all topologies” (“all topologies” here is now limited to the combination of &lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes.io/hostname&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;topology.kubernetes.io/zone&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;topology.kubernetes.io/region&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; pod anti-affinity, the admission controller &lt;code class=&quot;highlighter-rouge&quot;&gt;LimitPodHardAntiAffinityTopology&lt;/code&gt; was introduced to limit &lt;code class=&quot;highlighter-rouge&quot;&gt;topologyKey&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes.io/hostname&lt;/code&gt;. Verify if it is enabled or disabled.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/code&gt; object replaced.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl edit virtualmachineinstancereplicaset.kubevirt.io/mssql2016
virtualmachineinstancereplicaset.kubevirt.io/mssql2016 edited
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, it contains both affinity rules:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016replicaset&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;nodeAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures the application (MS SQL2016) must ONLY be placed on nodes where the key performance contains the value high&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;nodeSelectorTerms&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;performance&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;high&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;podAntiAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that two applications with the key kubevirt.io/domain equals to mssql2016 cannot run in the same zone&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;labelSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mssql2016&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;topologyKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;topology.kubernetes.io/zone&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that the VM or POD placement is executed only during the scheduling process, therefore we need to delete one of the &lt;code class=&quot;highlighter-rouge&quot;&gt;VirtualMachineInstances&lt;/code&gt; (VMI) running in the same node. Deleting the VMI will make Kubernetes spin up a new one to reconcile the desired number of replicas (3).&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;Remember to mark the kni-worker2 as schedulable again.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl uncordon kni-worker2
node/kni-worker2 uncordoned
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below shows the current status, where two databases are running in the &lt;code class=&quot;highlighter-rouge&quot;&gt;kni-worker6&lt;/code&gt; node. By applying the previous affinity rules this should not happen again:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get vmi &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME             AGE   PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql201696sz9   12m   Running   10.244.5.5   kni-worker6   False
mssql2016p948r   24m   Running   10.244.1.4   kni-worker4   False
mssql2016z2qnw   24m   Running   10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we delete one of the VMIs running in &lt;code class=&quot;highlighter-rouge&quot;&gt;kni-worker6&lt;/code&gt; and wait for the rules to be applied at scheduling time. As can be seen, databases are distributed across zones and high performing nodes:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete vmi mssql201696sz9
virtualmachineinstance.kubevirt.io &lt;span class=&quot;s2&quot;&gt;&quot;mssql201696sz9&quot;&lt;/span&gt; deleted

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get vmi &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME             AGE   PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql2016p948r   40m   Running   10.244.1.4   kni-worker4   False
mssql2016tpj6n   22s   Running   10.244.2.5   kni-worker2   False
mssql2016z2qnw   40m   Running   10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;During the deployment of the clustered database &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeAffinity&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeSelector&lt;/code&gt; rules were compared, however, there are a couple of things to take into consideration when creating node affinity rules, it is worth taking a look at &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&quot;&gt;node affinity in Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;If you remove or change the label of the node where the Pod is scheduled, the Pod will not be removed. In other words, the affinity selection works only at the time of scheduling the Pod.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;the-proxy-http-cache&quot;&gt;The proxy http cache&lt;/h3&gt;

&lt;p&gt;Now, that the database is configured by database administrators and running across multiple zones, it’s time to spin up the varnish http-cache container image. This time we are going to run it as a Pod instead of as a KubeVirt VM., however, scheduling rules are still valid for both objects.&lt;/p&gt;

&lt;p&gt;A detailed explanation on how to run a &lt;a href=&quot;https://varnish-cache.org/releases/index.html&quot;&gt;Varnish Cache&lt;/a&gt; in a Kubernetes cluster can be found in &lt;a href=&quot;https://github.com/mittwald/kube-httpcache&quot;&gt;kube-httpcache&lt;/a&gt; repository. Below are detailed the steps taken:&lt;/p&gt;

&lt;p&gt;Start by creating a ConfigMap that contains a VCL template and a Secret object that contains the secret for the Varnish administration port. Then apply the &lt;a href=&quot;https://github.com/mittwald/kube-httpcache#deploy-varnish&quot;&gt;Varnish deployment config&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; configmap.yaml
configmap/vcl-template created

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create secret generic varnish-secret &lt;span class=&quot;nt&quot;&gt;--from-literal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c32&lt;/span&gt; /dev/urandom  | &lt;span class=&quot;nb&quot;&gt;base64&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
secret/varnish-secret created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In our specific mandate, 3 replicas of our web cache application are needed. Each one must be running in a different zone or datacenter. Preferably, if possible, expected to run in a Kubernetes node different from the database since as administrators we would like the database to take advantage of all the possible resources of the high-performing server. Taken into account this prerequisite, the following advanced rules are applied:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;nodeAffinity rule&lt;/strong&gt;. This rule ensures that during scheduling time the application should be placed &lt;em&gt;if possible&lt;/em&gt; on nodes where the key performance does not contain the value high. Note the word &lt;em&gt;if possible&lt;/em&gt;. This means, it will try to run on a not performing server, however, if there none available it will be co-located with the database.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;podAntiAffinity rule&lt;/strong&gt;. This rule ensures that two applications with the key &lt;code class=&quot;highlighter-rouge&quot;&gt;app&lt;/code&gt; equals to &lt;code class=&quot;highlighter-rouge&quot;&gt;cache&lt;/code&gt; must not run in the same zone. Notice that the only application with this key value is the Varnish http-cache itself and more important, notice that this rule applies to the topologyKey &lt;code class=&quot;highlighter-rouge&quot;&gt;topology.kubernetes.io/zone&lt;/code&gt;. This means that only one Varnish http-cache instance can run in each zone, e.g. one http-cache in &lt;em&gt;scu, bcn and sab&lt;/em&gt; respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;varnish-cache&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cache&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cache&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;nodeAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that during scheduling time the application must be placed *if possible* on nodes NOT performance=high&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;preference&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;performance&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NotIn&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;high&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;podAntiAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that the application cannot run in the same zone (app=cache).&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;labelSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cache&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;topologyKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;topology.kubernetes.io/zone&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cache&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;quay.io/spaces/kube-httpcache:stable&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Always&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;In this set of affinity rules, a new scheduling policy has been introduced: &lt;code class=&quot;highlighter-rouge&quot;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;. It can be thought as “soft” scheduling, in the sense that it specifies preferences that the scheduler will try to enforce but will not guarantee.&lt;/p&gt;

&lt;p&gt;The weight field in preferredDuringSchedulingIgnoredDuringExecution must be in the range 1-100 and it is taken into account in the &lt;a href=&quot;#introduction&quot;&gt;scoring step&lt;/a&gt;. Remember that the node(s) with the highest total score is/are the most preferred.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here, the modified deployment is applied:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@eko1 varnish]# kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; deployment.yaml
deployment.apps/varnish-cache created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Pod is scheduled as expected since there is a node available in each zone without the &lt;code class=&quot;highlighter-rouge&quot;&gt;performance=high&lt;/code&gt; label.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
varnish-cache-54489f9fc9-5pbr2       1/1     Running   0          91s   10.244.4.5   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
varnish-cache-54489f9fc9-9s9tm       1/1     Running   0          91s   10.244.3.5   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
varnish-cache-54489f9fc9-dflzs       1/1     Running   0          91s   10.244.6.5   kni-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-launcher-mssql2016p948r-257pn   2/2     Running   0          70m   10.244.1.4   kni-worker4   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          31m   10.244.2.5   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          70m   10.244.5.4   kni-worker6   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, database and http-cache components of our customapp are up and running. Only the appliance created by an external provider needs to be deployed to complete the stack.&lt;/p&gt;

&lt;h3 id=&quot;the-third-party-appliance-virtual-machine&quot;&gt;The third-party appliance virtual machine&lt;/h3&gt;

&lt;p&gt;A third-party provider delivered a black box (appliance) in the form of a virtual machine where the application bought by the finance department is installed. Lucky to us, we have been able to transform it into a container VM ready to be run in our cluster with the help of KubeVirt.&lt;/p&gt;

&lt;p&gt;Following up with our objective, this web application must take advantage of the web cache application running as a Pod. So we require the appliance to be co-located in the same server that Varnish Cache in order to accelerate the delivery of the content provided by the appliance. Also, it is required to run every replica of the appliance in different zones or data centers. Taken into account these prerequisites, the following advanced rules are configured:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;podAffinity rule&lt;/strong&gt;. This rule ensures that during scheduling time the application must be placed on nodes where an application (Pod) with key &lt;code class=&quot;highlighter-rouge&quot;&gt;app' equals to&lt;/code&gt;cache` is running. That is to say where the Varnish Cache is running. Note that this is mandatory, it will only run co-located with the web cache Pod.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;podAntiAffinity rule&lt;/strong&gt;. This rule ensures that two applications with the key &lt;code class=&quot;highlighter-rouge&quot;&gt;kubevirt.io/domain&lt;/code&gt; equals to &lt;code class=&quot;highlighter-rouge&quot;&gt;blackbox&lt;/code&gt; must not run in the same zone. Notice that the only application with this key value is the appliance and more important, notice that this rule applies to the topologyKey &lt;code class=&quot;highlighter-rouge&quot;&gt;topology.kubernetes.io/zone&lt;/code&gt;. This means that only one appliance instance can run in each zone, e.g. one appliance in &lt;em&gt;scu, bcn and sab&lt;/em&gt; respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualMachineInstanceReplicaSet&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;blackbox&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;blackbox&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;blackbox&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;blackbox&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;podAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that during scheduling time the application must be placed on nodes where Varnish Cache is running&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;labelSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cache&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;topologyKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;topology.kubernetes.io/hostname&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;podAntiAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that two applications with the key `kubevirt.io/domain` equals to `blackbox` cannot run in the same zone&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;labelSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kubevirt.io/domain&quot;&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;blackbox&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;topologyKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;topology.kubernetes.io/zone&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the modified deployment is applied. As expected the VMI is scheduled as expected in the same Kubernetes nodes as Varnish Cache and each one in a different datacenter or zone.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods,vmi &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide

NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          172m    10.244.4.5   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          172m    10.244.3.5   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-dflzs	 1/1     Running   0          172m    10.244.6.5   kni-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxtk49x-nw45s    2/2     Running   0          2m31s   10.244.6.6   kni-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          2m31s   10.244.4.9   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          2m31s   10.244.3.6   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h1m    10.244.1.4   kni-worker4   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h22m   10.244.2.5   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h1m    10.244.5.4   kni-worker6   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

NAME                                                AGE     PHASE     IP           NODENAME	 LIVE-MIGRATABLE
virtualmachineinstance.kubevirt.io/blackboxtk49x    2m31s   Running   10.244.6.6   kni-worker    False
virtualmachineinstance.kubevirt.io/blackboxxt829    2m31s   Running   10.244.4.9   kni-worker5   False
virtualmachineinstance.kubevirt.io/blackboxzf9kt    2m31s   Running   10.244.3.6   kni-worker3   False
virtualmachineinstance.kubevirt.io/mssql2016p948r   4h1m    Running   10.244.1.4   kni-worker4   False
virtualmachineinstance.kubevirt.io/mssql2016tpj6n   3h22m   Running   10.244.2.5   kni-worker2   False
virtualmachineinstance.kubevirt.io/mssql2016z2qnw   4h1m    Running   10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, our stack has been successfully deployed and configured accordingly to the requirements agreed. However, it is important before going into production to verify the proper behaviour in case of node failures. That’s what is going to be shown in the next section.&lt;/p&gt;

&lt;h2 id=&quot;verify-the-resiliency-of-our-customapp&quot;&gt;Verify the resiliency of our customapp&lt;/h2&gt;

&lt;p&gt;In this section, several tests must be executed to validate that the scheduling already in place is line up with the expected behaviour of the customapp application.&lt;/p&gt;

&lt;h3 id=&quot;draining-a-regular-node&quot;&gt;Draining a regular node&lt;/h3&gt;

&lt;p&gt;In this test, the node located in &lt;code class=&quot;highlighter-rouge&quot;&gt;scu&lt;/code&gt; zone which is not labelled as high-performance will be upgraded. The proper procedure to maintain a Kubernetes node is as follows: drain the node, upgrade packages and then reboot it.&lt;/p&gt;

&lt;p&gt;As it is depicted, once the &lt;code class=&quot;highlighter-rouge&quot;&gt;kni-worker&lt;/code&gt; is marked as unschedulable and drained, the Varnish Cache pod and the black box appliance VM are automatically moved to the high-performance node in the same zone.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          3h8m    10.244.4.5   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-9s5sr	 1/1     Running   0          2m32s   10.244.2.7   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          3h8m    10.244.3.5   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxxh5tg-g7hns    2/2     Running   0          13m     10.244.2.8   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          18m     10.244.4.9   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          18m     10.244.3.6   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h17m   10.244.1.4   kni-worker4   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h37m   10.244.2.5   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h17m   10.244.5.4   kni-worker6   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

NAME                                                AGE     PHASE     IP           NODENAME      LIVE-MIGRATABLE
virtualmachineinstance.kubevirt.io/blackboxxh5tg    13m     Running   10.244.2.8   kni-worker2   False
virtualmachineinstance.kubevirt.io/blackboxxt829    18m     Running	 10.244.4.9   kni-worker5   False
virtualmachineinstance.kubevirt.io/blackboxzf9kt    18m     Running	 10.244.3.6   kni-worker3   False
virtualmachineinstance.kubevirt.io/mssql2016p948r   4h17m   Running	 10.244.1.4   kni-worker4   False
virtualmachineinstance.kubevirt.io/mssql2016tpj6n   3h37m   Running	 10.244.2.5   kni-worker2   False
virtualmachineinstance.kubevirt.io/mssql2016z2qnw   4h17m   Running	 10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remember that this is happening because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is a &lt;strong&gt;mandatory&lt;/strong&gt; policy that only one replica of each application can run at the same time in the same zone.&lt;/li&gt;
  &lt;li&gt;There is a &lt;strong&gt;soft policy&lt;/strong&gt; (preferred) that both applications should run on a non high-performance node. However, since there are any of these nodes available it has been scheduled in the high-performance server along with the database.&lt;/li&gt;
  &lt;li&gt;Both applications must run in the same node&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;Note that uncordoning the node will not make the blackbox appliance and the Varnish Cache pod to come back to the previous node.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl uncordon kni-worker
node/kni-worker uncordoned

NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          3h10m   10.244.4.5   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-9s5sr	 1/1     Running   0          5m29s   10.244.2.7   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          3h10m   10.244.3.5   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxxh5tg-g7hns    2/2     Running   0          16m     10.244.2.8   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          21m     10.244.4.9   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          21m     10.244.3.6   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h20m   10.244.1.4   kni-worker4   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h40m   10.244.2.5   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h20m   10.244.5.4   kni-worker6   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to return to the most desirable state, the pod and VM from kni-worker2 must be deleted.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;Both applications must be deleted since the &lt;code class=&quot;highlighter-rouge&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; policy is only applied during scheduling time.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete  pod/varnish-cache-54489f9fc9-9s5sr
pod &lt;span class=&quot;s2&quot;&gt;&quot;varnish-cache-54489f9fc9-9s5sr&quot;&lt;/span&gt; deleted

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete virtualmachineinstance.kubevirt.io/blackboxxh5tg
virtualmachineinstance.kubevirt.io &lt;span class=&quot;s2&quot;&gt;&quot;blackboxxh5tg&quot;&lt;/span&gt; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once done, the scheduling process is run again for both applications and the applications are placed in the most desirable node taking into account affinity rules configured.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          3h13m   10.244.4.5   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          3h13m   10.244.3.5   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/varnish-cache-54489f9fc9-fldhc	 1/1     Running   0          2m7s    10.244.6.7   kni-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackbox54l7t-4c6wh    2/2     Running   0          23s     10.244.6.8   kni-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          23m     10.244.4.9   kni-worker5   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          23m     10.244.3.6   kni-worker3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h23m   10.244.1.4   kni-worker4   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h43m   10.244.2.5   kni-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h23m   10.244.5.4   kni-worker6   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

NAME                                                AGE     PHASE     IP           NODENAME	 LIVE-MIGRATABLE
virtualmachineinstance.kubevirt.io/blackbox54l7t    23s     Running   10.244.6.8   kni-worker    False
virtualmachineinstance.kubevirt.io/blackboxxt829    23m     Running   10.244.4.9   kni-worker5   False
virtualmachineinstance.kubevirt.io/blackboxzf9kt    23m     Running   10.244.3.6   kni-worker3   False
virtualmachineinstance.kubevirt.io/mssql2016p948r   4h23m   Running   10.244.1.4   kni-worker4   False
virtualmachineinstance.kubevirt.io/mssql2016tpj6n   3h43m   Running   10.244.2.5   kni-worker2   False
virtualmachineinstance.kubevirt.io/mssql2016z2qnw   4h23m   Running   10.244.5.4   kni-worker6   False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This behaviour can be extrapolated to a failure or shutdown of any odd or non high-performance worker node. In that case, all workloads will be moved to the high performing server &lt;em&gt;in the same zone&lt;/em&gt;. Although this is not ideal, our &lt;code class=&quot;highlighter-rouge&quot;&gt;customapp&lt;/code&gt; will be still available while the node recovery is ongoing.&lt;/p&gt;

&lt;h3 id=&quot;draining-a-high-performance-node&quot;&gt;Draining a high-performance node&lt;/h3&gt;

&lt;p&gt;On the other hand, in case of a high-performance worker node failure, which was shown &lt;a href=&quot;#the-clustered-database&quot;&gt;previously&lt;/a&gt;, the database will not be able to move to another server, since there is only one high performing server per zone. A possible solution is just adding a stand-by high-performance node in each zone.&lt;/p&gt;

&lt;p&gt;However, since the database is configured as a clustered database, the application running in the same zone as the failed database will still be able to establish a connection to any of the other two running databases located in different zones. This configuration is done at the application level. Actually, from the application standpoint, it just connects to a database pool of resources.&lt;/p&gt;

&lt;p&gt;Since this is not ideal either, e.g. establishing a connection to another zone or datacenter takes longer than in the same datacenter, the application will be still available and providing service to the clients.&lt;/p&gt;

&lt;h2 id=&quot;affinity-rules-are-everywhere&quot;&gt;Affinity rules are everywhere&lt;/h2&gt;

&lt;p&gt;As written in the title section, affinity rules are essential to provide high availability and resiliency to Kubernetes applications. Furthermore, KubeVirt’s components also take advantage of these rules to avoid unwanted situations that could compromise the stability of the VMs running in the cluster.&lt;/p&gt;

&lt;p&gt;For instance, below it is partly shown a snippet of the deployment object for virt-api and virt-controller. Notice the following affinity rule created:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;podAntiAffinity rule&lt;/strong&gt;. This rule ensures that two replicas of the same application should not run if possible in the same Kubernetes node (&lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes.io/hostname&lt;/code&gt;). It is used the key &lt;code class=&quot;highlighter-rouge&quot;&gt;kubevirt.io&lt;/code&gt; to match the application &lt;code class=&quot;highlighter-rouge&quot;&gt;virt-api&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;virt-controller&lt;/code&gt;. See that it is a soft requirement, which means that the kube-scheduler will try to match the rule, however, if it is not possible it can place both replicas in the same node.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-api&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-api&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-api&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;scheduler.alpha.kubernetes.io/critical-pod&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;scheduler.alpha.kubernetes.io/tolerations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[{&quot;key&quot;:&quot;CriticalAddonsOnly&quot;,&quot;operator&quot;:&quot;Exists&quot;}]'&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-api&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;prometheus.kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-api&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;podAntiAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that two replicas of the same application should not run if possible in the same Kubernetes node&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;podAffinityTerm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                      &lt;span class=&quot;na&quot;&gt;labelSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;
                          &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                          &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-api&lt;/span&gt;
                      &lt;span class=&quot;na&quot;&gt;topologyKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes.io/hostname&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-controller&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-controller&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-controller&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;scheduler.alpha.kubernetes.io/critical-pod&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;scheduler.alpha.kubernetes.io/tolerations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[{&quot;key&quot;:&quot;CriticalAddonsOnly&quot;,&quot;operator&quot;:&quot;Exists&quot;}]'&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-controller&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;prometheus.kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-controller&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;podAntiAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#This rule ensures that two replicas of the same application should not run if possible in the same Kubernetes node&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;podAffinityTerm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;labelSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;
                      &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                      &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-controller&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;topologyKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes.io/hostname&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;It is worth mentioning that DaemonSets internally also uses advanced scheduling rules. Basically, they are &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeAffinity&lt;/code&gt; rules in order to place each replica in each Kubernetes node of the cluster.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@eko1 varnish]# kubectl get daemonset &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt
NAMESPACE     NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
kubevirt      virt-handler   6         6         6       6            6           &amp;lt;none&amp;gt;                        25h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See the partial snippet of a &lt;code class=&quot;highlighter-rouge&quot;&gt;virt-handler&lt;/code&gt; Pod created by a DaemonSet (see ownerReferences section, kind: DaemonSet) that configures a &lt;code class=&quot;highlighter-rouge&quot;&gt;nodeAffinity&lt;/code&gt; rule that requires the Pod to run in a specific hostname matched by the key &lt;code class=&quot;highlighter-rouge&quot;&gt;metadata.name&lt;/code&gt; and value the name of the node (&lt;code class=&quot;highlighter-rouge&quot;&gt;kni-worker2&lt;/code&gt;). Note that the value of the key changes depending on the nodes that are part of the cluster, this is done by the DaemonSet.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io/install-strategy-identifier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0000ee7f7cd4756bb221037885c3c86816db6de7&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io/install-strategy-registry&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;index.docker.io/kubevirt&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io/install-strategy-version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v0.26.0&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;scheduler.alpha.kubernetes.io/critical-pod&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;scheduler.alpha.kubernetes.io/tolerations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[{&quot;key&quot;:&quot;CriticalAddonsOnly&quot;,&quot;operator&quot;:&quot;Exists&quot;}]'&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;creationTimestamp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2020-02-12T11:11:14Z&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;generateName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-handler-&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;app.kubernetes.io/managed-by&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt-operator&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;controller-revision-hash&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;84d96d4775&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-handler&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pod-template-generation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;prometheus.kubevirt.io&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-handler-ctzcg&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubevirt&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ownerReferences&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;blockOwnerDeletion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;controller&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DaemonSet&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virt-handler&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;6e7faece-a7aa-4ed0-959e-4332b2be4ec3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;resourceVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;28301&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selfLink&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/api/v1/namespaces/kubevirt/pods/virt-handler-ctzcg&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;95d68dad-ad06-489f-b3d3-92413bcae1da&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;nodeAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;nodeSelectorTerms&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;matchFields&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metadata.name&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kni-worker2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this blog post, a real use case has been detailed on how advanced scheduling can be configured in a hybrid scenario where VMs and Pods are part of the same application stack. The reader can realize that Kubernetes itself already provides a lot of functionality out of the box to Pods running on top of it. One of these inherited capabilities is the possibility to create even more complex affinity or/and anti-affinity rules than traditional virtualization products.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/&quot;&gt;Kubernetes labels and annotations official documentation&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubevirt.io/2019/NodeDrain-KubeVirt.html&quot;&gt;Kubevirt node drain blog post&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&quot;&gt;Kubernetes node affinity documentation&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md&quot;&gt;Kubernetes design proposal for Inter-pod topological affinity and anti-affinity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubevirt/kubevirt/pull/2089&quot;&gt;KubeVirt add affinity to virt pods pull request discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alberto Losada Grande</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Advanced VM scheduling" /><category term="affinity" /><category term="scheduling" /><category term="topologyKeys" /><summary type="html">This blog post shows how KubeVirt can take advantage of Kubernetes inner features to provide an advanced scheduling mechanism to virtual machines (VMs). The same or even more complex affinity and anti-affinity rules can be assigned to VMs or Pods in Kubernetes than in traditional virtualization solutions.</summary></entry></feed>